# Factor Analysis

  
```{r, include=FALSE, warning=FALSE, message=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).

# knitr settings to control how R chunks work.
library(knitr)
# opts_knit$set(root.dir = "../../")
opts_chunk$set(
  echo = TRUE,
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  comment = NULL,
  warning = FALSE,
  message = FALSE
)

library(mosaic)
library(pander)
suppressPackageStartupMessages(library(tidyverse))
library(texreg)
library(car)
library(lavaan)
library(semPlot)
library(psych)
library(tables)
library(lavaan)
options(ztable.type = "latex")
options(xtable.comment = FALSE)
# Set ggplot default font size
theme_update(text = element_text(size = 18))
# This changes the default colors in lattice plots.
trellis.par.set(theme=theme.mosaic())  
tsize = 30
panderOptions("missing", "")
```

## Correlation Coefficient


*Pearson product-moment correlation*:

$$
r_{xy} = \frac{\Sigma_{n=1}^n (x_k - \bar{x})(y_i - \bar{y})}{\sqrt{\Sigma_{n=1}^n(x_i - \bar{x})^2} \sqrt{\Sigma_{n=1}^n(y_i - \bar{y})^2}} = \frac{s_{xy}}{s_x s_y}.
$$

The equation looks very daunting, until you see that it is just the covariance of $x$ and $y$ divided by the product of their standard deviations.


```{r}
library("MPsychoR")
data("YouthDep")
item1 <- YouthDep[, 1]
levels(item1) <- c("0", "1", "1")
item2 <- YouthDep[, 14]
levels(item2) <- c("0", "1", "1")
table(item1, item2)
```


```{r}
## ------ correlation coefficients
library("psych")
tetcor <- tetrachoric(cbind(item1, item2))
tetcor
item1 <- YouthDep[, 1]
item2 <- YouthDep[, 14]
polcor <- polychoric(cbind(item1, item2))
polcor

draw.tetra(r = .35, t1 = 1.16, t2 = .36)

DepItems <- YouthDep[,1:26] 
Depnum <- data.matrix(DepItems) - 1  ## convert to numeric   
Rdep <- polychoric(Depnum)
```

## Think about these situations

What do you do when you have a large number of variables you are considering as predictors of a dependent variable?

* Often, subsets of these variables are measuring the same, or very similar things.
* We might like to reduce the variables to a smaller number of predictors.

What if you are developing a measurement scale and have a large number of items you think measure the same construct


* You might want to see how strongly the items are related to the construct.

## Solutions

1. Principal Components Analysis
    
    * transforming the original variables into a new set of linear combinations (pricipal components).
    
2. Factor Analysis

    * setting up a mathematical model to estimate the number or factors



## Principal Components Analysis

* Concerned with explaining variance-covariance structure of a set of variables.
*  PCA attempts to explain as much of the total variance among the observed variables as possible with a smaller number of components.
* Because the variables are standardized prior to analysis, the total amount of variance available is the number of variables.
* The goal is **data reduction** for subsequent analysis.
* Variables *cause* components.
* Components are not representative of any underlying theory.
  
## Factor Analysis

* The goal is understanding underlying constructs.
* Uses a modified correlation matrix (reduced matrix)
* factors *cause* the variables.
* Factors represent theoretical constructs.
* Focuses on the common variance of the variables, and purges the unique variance.
  

## Components

The principal components partition the total variance (the sum of the variances of the original variables) by finding the linear combination of the variables that account for the maximum amount of variance:

$$
PC1 = a_{11}x_1 + a_{12}x_2 ... a_{1p}x_p, 
$$
This is repeated as many time as there are variables.

## PC Extraction

draw pretty pictures on the board

## Eigenvalues

Eigenvalues represent the variance in the variables explained by the success components.

## Determining the Number of Factors

1. Kaiser criterion: Retain only factors with eigenvalues > 1. (generally accurate)
2. Scree plot: plot eigenvalues and drop factors after leveling off. 
3. Parallel analysis: compare observed eigenvalues to parallel set of data from randomly generated data. Retain factors in original if eigenvalue is greater than random eigenvalue.
4. Factor meaningfulness is also very important to consider.

## Example data

```{r}
lower <- "
1.00
0.70 1.00
0.65 0.66 1.00
0.62 0.63 0.60 1.00
"
cormat <- getCov(lower, names = c("d1", "d2", "d3", "d4"))

cormat

```

## Kaiser

Retain factors with eigenvalues greater than 1
```{r}
eigen(cormat)$values
```

## Scree Plot

```{r}
scree(cormat, factors = FALSE)
```


## Horn's Parallel Analysis

```{r}
fa.parallel(cormat, fa = "pc")
```


## Another example

```{r}
fa.parallel(Harman74.cor$cov, fa = "pc")
```

## Rotation

* Principal components are derived to maximize the variance accounted for (data reduction).
* Rotation is done to make the factors more interpretable (i.e. meaningful).
* Two major classes of rotation:
    - Orthogonal - new factors are still uncorrelated, as were the initial factors.
    - Oblique - new factors are allowed to be correlated.
    
Essentially reallocates the loadings. The first factor may not be the one accounting for the most variance.    
    
## Orthogonal Rotation

1. **Quartimax** - idea is to clean up the *variables*. Rotation done so each variable loads mainly on one factor. Problematic if there is a general factor on which most or all variables load on (think IQ).

2. **Varimax** - to clean up *factors*. So each factor has  high correlation with a smaller number of variables, low correlation with the other variables. Generally makes interpretation easier.

## Oblique Rotation

* Often correlated factors are more reasonable. 
* Therefore, oblique rotation is often preferred.
* But interpretation is more complicated.

## Factor Matrices

1. Factor pattern matrix: 
    - includes *pattern coefficients* analogous to standardized partial regression coefficients. 
    - Indicated the unique importance of a factor to a variable, holding other factors constant.
    
2. Factor structure matrix: 
    - includes *structure coefficients* which are simple correlations of the variables with the factors.
    
## Which matrix should we interpret?

* When orthogonal rotation is used interpret *structural coefficients* (but they are the same as pattern coefficients).

* When oblique rotation is used pattern coefficients are preferred because they account for the correlation between the factors and they are parameters of the correlated factor model (which we will discuss next class).

## Which variables should be used to interpret each factor?

* The idea is to use only those variables that have a strong association with the factor.
* Typical thresholds are |.30| or |.40|. 
* Content knowledge is critical.

## Examples

Let's look at some examples


  
```{r, include=FALSE, warning=FALSE, message=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).

# knitr settings to control how R chunks work.
library(knitr)
# opts_knit$set(root.dir = "../../")
opts_chunk$set(
  echo = FALSE,
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  comment = NULL,
  warning = FALSE,
  message = FALSE
)

library(mosaic)
library(pander)
suppressPackageStartupMessages(library(tidyverse))
library(texreg)
library(car)
library(lavaan)
library(semPlot)
library(psych)
library(tables)
library(lavaan)
library(Hmisc)
options(ztable.type = "latex")
options(xtable.comment = FALSE)
# Set ggplot default font size
theme_update(text = element_text(size = 18))
# This changes the default colors in lattice plots.
trellis.par.set(theme=theme.mosaic())  
tsize = 30
#panderOptions("missing", "")
```


## Steps in Factor Analysis

1. Choose extraction method 
    - So far we've focused on PCA
2. Determine the number of components/factors
    - Kaiser method: eigenvalues > 1
    - Scree plot: All components before leveling off
    - Horn's parallel analysis: components/factors greater than simulated values from random numbers
3. Rotate Factors  
    - Orthogonal
    - Oblique
    
4. Interpret Components/Factors    
    
## Tom Swift's Electric Factor Analysis Factory

"Little Jiffy" method of factor analysis

1. Extraction method : PCA
2. Number of factors: eigenvalues > 1
3. Rotation: orthogonal(varimax)
4. Interpretation

## Metal Boxes

```{r, eval=TRUE}
tab <- data.frame(
  Dimension  = c("Thickness", "Width", "Length", "Volume", "Density", "Weight",
                 "Surface area", "Cross-section", "Edge length", 
                 "Diagonal length", "Cost/lb"),
  Derivation = c("x", "y", "z", "xyz", "d", "xyzd", "2(xy + xz + yz)",
                 "yz", "4(x + y + z)", "(x^2)", "c"))
kable(tab, caption = "Functional Definitions of Tom Swift's Original 11 Variables")
```
```{r}
# Import data and look at them.
swift <- read.csv("data/swift.csv", header = TRUE)
str(swift)
```


\begin{table}[]
\begin{tabular}{lllll}
 Dimension &  Derivation  &  \\
 \hline \\
 Thickness &  $x$  \\
 Width & $y$ \\
 Length & $z$ \\
 Volume & $xyz$ \\
 Density & $d$ \\
 Weight & $xyzd$ \\
 Total surface area & $2(xy + xz +_ yz)$ \\
 Cross-sectional area & $yz$ \\
 Total edge length & $4(x +  y + z)$ \\
 Internal diagonal length & $(x^2 + y^2 + z^2)^2$ \\
 Cost per pound & $c$
\end{tabular}
\end{table}


## Correlations

\tiny
```{r}
# load("data/Rdata/swift.Rdata")

# round(cor(swift), 2)

kable(cor(swift), caption = "Correlations between dimensions", digits = 2)
```

## Eigenvalues > 1

```{r}
scree(cor(swift), factors = FALSE)
```

## Orthogonal Rotation

\footnotesize
```{r}
jiffy <- pca(cov(swift), nfactors = 5)
print(jiffy$loadings, cut = .70)
```

## Orthogonal Rotation with Loadings > .70

\footnotesize
```{r}
jiffy <- pca(cor(swift), nfactors = 3, rotate = "varimax", method = "ml")
print(jiffy$loadings, cut = .70)
```


## R Code for Chapter 2

```{r, eval=FALSE, echo=TRUE}
## ----------------------- Chapter 2: Factor Analysis ------------------
library("MPsychoR")
data("YouthDep")
item1 <- YouthDep[, 1]
levels(item1) <- c("0", "1", "1")
item2 <- YouthDep[, 14]
levels(item2) <- c("0", "1", "1")
table(item1, item2)

## ------ correlation coefficients
library("psych")
tetcor <- tetrachoric(cbind(item1, item2))
tetcor
item1 <- YouthDep[, 1]
item2 <- YouthDep[, 14]
polcor <- polychoric(cbind(item1, item2))
polcor

DepItems <- YouthDep[,1:26] 
Depnum <- data.matrix(DepItems) - 1  ## convert to numeric   
Rdep <- polychoric(Depnum)

data("Rmotivation")
vind <- grep("ext|int", colnames(Rmotivation)) 
Rmotivation1 <- Rmotivation[, vind]
Rmot1 <- tetrachoric(Rmotivation1, smooth = FALSE)
tail(round(eigen(Rmot1$rho)$values, 3))
Rmot <- tetrachoric(Rmotivation1)
tail(round(eigen(Rmot$rho)$values, 3))

## ----- exploratory factor analysis 
motFA <- fa(Rmot$rho, nfactors = 2, rotate = "none", fm = "ml")
print(motFA$loadings, cutoff = 0.2)
round(motFA$communality, 2)

motFA2 <- fa(Rmot$rho, nfactors = 2, rotate = "varimax", fm = "ml")
plot(motFA$loadings, asp = 1, xlim = c(-0.2, 0.9), ylim = c(-0.5, 0.9), type = "n", xlab = "Factor 1", ylab = "Factor 2", main = "Loadings Plot")
text(motFA$loadings, labels = rownames(motFA$loadings), cex = 0.8, col = "gray")
abline(h = 0, v = 0, col = "lightgray", lty = 2)
text(motFA2$loadings, labels = rownames(motFA2$loadings), col = 1, cex = 0.8)
legend("bottomleft", legend = c("rotated", "unrotated"), col = c("black", "gray"), pch = 19)

Rmot2 <- tetrachoric(Rmotivation[,1:36])
motFA3 <- fa(Rmot2$rho, nfactors = 3, rotate = "oblimin", fm = "ml")
motFA3$loadings
round(motFA3$Phi, 3)

motFA2 <- fa(Rmotivation1, nfactors = 2, rotate = "varimax", cor = "tet", fm = "ml", scores = "regression",
             missing = TRUE, impute = "median")
dim(motFA2$scores)

Rdep <- polychoric(Depnum)$rho
evals <- eigen(Rdep)$values
scree(Rdep, factors = FALSE)
(evals/sum(evals)*100)[1:2]

set.seed(123)
resPA <- fa.parallel(Depnum, fa = "pc", cor = "poly", fm = "ml")  
resvss <- vss(Rdep, fm = "ml", n.obs = nrow(Depnum), plot = FALSE)
resvss

fadep <- fa(Depnum, 1, cor = "poly", fm = "ml")
summary(fadep)

resnf <- nfactors(Depnum, n = 8, fm = "ml", cor = "poly")
resnf

## ----- Bayesian exploratory factor analysis
library("MPsychoR")
library("corrplot")
library("BayesFM")
data("Privacy")
Privstd <- scale(Privacy)
corrplot(cor(Privstd))

Nid <- 2              ## minimum number of variables per factor
pmax <- trunc(ncol(Privstd)/Nid)   ## maximum number of factors
pmax

set.seed(123)
Rsim <- simul.R.prior(pmax, nu0 = pmax + c(1, 2, 5, 7, 10))
plot(Rsim)

Ksim <- simul.nfac.prior(nvar = ncol(Privstd), Nid = Nid, Kmax = pmax, kappa = c(.1, .2, .5, 1))
plot(Ksim)

set.seed(222)
fitbefa <- befa(Privstd, Nid = 2, Kmax = pmax, nu0 = 10, kappa = 0.2, kappa0 = 0.1, xi0 = 0.1,
                burnin = 5000, iter = 50000)
fitbefa <- post.column.switch(fitbefa)   ## column reordering
fitbefa <- post.sign.switch(fitbefa)     ## sign switching
sumbefa <- summary(fitbefa)

## ----- confirmatory factor analysis
library("MPsychoR")
library("lavaan")
data("Rmotivation")
vind <- grep("ext|int", colnames(Rmotivation)) ## ext/int items
Rmot <- na.omit(Rmotivation[, vind])
mot_model <- ' 
  extrinsic  =~ ext1 + ext2 + ext3 + ext4 + ext5 + ext6 + 
                ext7 + ext8 + ext9 + ext10 + ext11 + ext12      
  intrinsic =~  int1 + int2 + int3 + int4 + int5'
fitMot <- lavaan::cfa(mot_model, data = Rmot, ordered = names(Rmot))

library("semPlot")
semPaths(fitMot, what = "est", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 4.5, asize = 2.5,
         intercepts = FALSE, rotation = 4, thresholdColor = "red", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)

inspect(fitMot, what = "est")$theta
inspect(fitMot, what = "est")$lambda
inspect(fitMot, what = "std")$lambda
inspect(fitMot, what = "est")$psi
inspect(fitMot, what = "std")$psi

parameterEstimates(fitMot, standardized = TRUE)
summary(fitMot, standardized = TRUE, fit.measures = TRUE)
parameterEstimates(fitMot)[5,]

mot_model2 <- '
  extrinsic  =~ ext1 + ext2 + ext3 + ext4 + ext6 + ext7 + 
                ext8 + ext9 + ext10 + ext11 + ext12
  intrinsic =~  int1 + int2 + int3 + int4 + int5'
fitMot2 <- lavaan::cfa(mot_model2, data = Rmot, ordered = names(Rmot)[-5])
vind <- c(1:4, 13:16, 32:35)
Rmot2 <- na.omit(Rmotivation[, vind])

mot_model3 <- '
  extrinsic  =~ ext1 + ext2 + ext3 + ext4 
  hybrid =~ hyb1 + hyb2 + hyb3 + hyb4              
  intrinsic =~  int1 + int2 + int3 + int4 
  motivation =~ extrinsic + hybrid + intrinsic'
fitMot3 <- lavaan::cfa(mot_model3, data = Rmot2, ordered = names(Rmot2))

semPaths(fitMot3, what = "std", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5,
         intercepts = FALSE, rotation = 4, thresholdColor = "red", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)

summary(fitMot3, standardized = TRUE, fit.measures = TRUE)

vind <- c(1:4, 13:16, 32:35, 39:41)
Rmot3 <- na.omit(Rmotivation[, vind])
mot_model4 <- '
  extrinsic  =~ ext1 + ext2 + ext3 + ext4 
  hybrid =~ hyb1 + hyb2 + hyb3 + hyb4              
  intrinsic =~  int1 + int2 + int3 + int4 
  motivation =~ extrinsic + hybrid + intrinsic
  motivation ~ npkgs + phd'
fitMot4 <- lavaan::cfa(mot_model4, data = Rmot3, ordered = names(Rmot3[1:12]))

semPaths(fitMot4, what = "std", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5,
         intercepts = FALSE, rotation = 4, thresholdColor = "red", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)
parameterEstimates(fitMot4)[16:17,]

library("semTools")
data("Bergh")
GP_model <- 'GP =~ EP + HP + DP + SP'
minvfit1 <- measEq.syntax(GP_model, data = Bergh, group = "gender", return.fit = TRUE)
minvfit2 <- measEq.syntax(GP_model, data = Bergh, group = "gender", 
                          group.equal = c("loadings"), return.fit = TRUE)
minvfit3 <- measEq.syntax(GP_model, data = Bergh, group = "gender", 
                          group.equal = c("loadings", "intercepts"), return.fit = TRUE)
minvfit4 <- measEq.syntax(GP_model, data = Bergh, group = "gender", 
                          group.equal = c("loadings", "intercepts", "means"), return.fit = TRUE)
anova(minvfit1, minvfit2, minvfit3, minvfit4)

GP_model <- 'GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + SP'
fitBase <- lavaan::cfa(GP_model, data = Bergh, group = "gender", estimator = "MLR")

GP_model <- 'GP =~ EP + HP + DP + SP'
fitBase <- lavaan::cfa(GP_model, data = Bergh, group = "gender", group.equal = c("loadings"),
                       group.partial = c("GP=~ SP"), estimator = "MLR")

fitBase1 <- lavaan::cfa(GP_model, data = Bergh, group = "gender", group.equal = c("loadings", "intercepts"), 
                        group.partial = c("GP=~SP", "DP~1", "HP~1", "SP~1"), estimator = "MLR")

GP_model2 <- 'GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + c(NA, 0)*SP'
fitIO <- lavaan::cfa(GP_model2, data = Bergh, group = "gender", group.equal = c("intercepts"), 
                     group.partial = c("DP~1", "HP~1", "SP~1"), estimator = "MLR")

fitMarg <- lavaan::cfa(GP_model, data = Bergh, group = "gender", group.equal = c("loadings", "intercepts"),
                       group.partial = c("DP~1", "HP~1", "SP~1"), estimator = "MLR")

anova(fitMarg, fitBase1)

library("MPsychoR")
library("lavaan")
data("SDOwave")
model_sdo1 <- '
  SDO1996 =~ 1*I1.1996 + a2*I2.1996 + a3*I3.1996 + a4*I4.1996
  SDO1998 =~ 1*I1.1998 + a2*I2.1998 + a3*I3.1998 + a4*I4.1998
  SDO1996 ~~ SDO1998

  ## intercepts
  I1.1996 ~ int1*1; I1.1998 ~ int1*1
  I2.1996 ~ int2*1; I2.1998 ~ int2*1 
  I3.1996 ~ int3*1; I3.1998 ~ int3*1
  I4.1996 ~ int4*1; I4.1998 ~ int4*1

  ## residual covariances
  I1.1996 ~~ I1.1998
  I2.1996 ~~ I2.1998
  I3.1996 ~~ I3.1998
  I4.1996 ~~ I4.1998

  ## latent means: 1996 as baseline
  SDO1996 ~ 0*1
  SDO1998 ~ 1'
fitsdo1 <- cfa(model_sdo1, data = SDOwave, estimator = "MLR")
parameterEstimates(fitsdo1)[22:23,]

model_sdo2 <- '
  ## 1st CFA level, constant loadings across time
  SDOD1996 =~ 1*I1.1996 + d1*I2.1996
  SDOD1998 =~ 1*I1.1998 + d1*I2.1998
  SDOD1999 =~ 1*I1.1999 + d1*I2.1999 
  SDOE1996 =~ 1*I3.1996 + a1*I4.1996 
  SDOE1998 =~ 1*I3.1998 + a1*I4.1998
  SDOE1999 =~ 1*I3.1999 + a1*I4.1999

  ## 2nd CFA level, constant loadings across time
  SDO1996 =~ 1*SDOD1996 + sd1*SDOE1996
  SDO1998 =~ 1*SDOD1998 + sd1*SDOE1998
  SDO1999 =~ 1*SDOD1999 + sd1*SDOE1999

  ## Constant 1st level intercepts
  I1.1996 ~ iI1*1; I1.1998 ~ iI1*1; I1.1999 ~ iI1*1
  I2.1996 ~ iI2*1; I2.1998 ~ iI2*1; I2.1999 ~ iI2*1
  I3.1996 ~ iI3*1; I3.1998 ~ iI3*1; I3.1999 ~ iI3*1
  I4.1996 ~ iI4*1; I4.1998 ~ iI4*1; I4.1999 ~ iI4*1

  ## residual covariances:
  I1.1999 ~~ I1.1998; I1.1996 ~~ I1.1998; I1.1999 ~~ I1.1996
  I2.1999 ~~ I2.1998; I2.1996 ~~ I2.1998; I2.1999 ~~ I2.1996
  I3.1999 ~~ I3.1998; I3.1996 ~~ I3.1998; I3.1999 ~~ I3.1996
  I4.1999 ~~ I4.1998; I4.1996 ~~ I4.1998; I4.1999 ~~ I4.1996

  ## latent means
  SDO1996 ~ 0*1    ## 1996 baseline year
  SDO1998 ~ 1      ## 1998 vs. 1996
  SDO1999 ~ 1      ## 1999 vs. 1996
'
fitsdo2 <- cfa(model_sdo2, data = SDOwave, estimator = "MLR")

semPaths(fitsdo2, what = "est", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 6, asize = 2.5,
         intercepts = FALSE, rotation = 4, thresholdColor = "red", mar = c(1, 5, 1.5, 5), fade = FALSE)
parameterEstimates(fitsdo2)[43:45,]

data("FamilyIQ")
modelIQ <- '
 level: 1
  numeric =~ wordlist + cards + matrices
  perception =~ figures + animals + occupation
 level: 2
  general =~ wordlist + cards +  matrices + figures + animals +
             occupation'
fitIQ <- cfa(modelIQ, data = FamilyIQ, cluster = "family", std.lv = TRUE)
fitIQ

## ----- bayesian confirmatory factor analysis
library("blavaan")
dpriors()[c("lambda", "itheta", "ipsi")]

library("MPsychoR")
data("Bergh")
GP_model <- 'GP =~ EP + HP + DP + SP'
set.seed(123)
fitBCFA <- bcfa(GP_model, data = Bergh, burnin = 2000, sample = 10000, n.chains = 2)

plot(fitBCFA, pars = 1:2, plot.type = "trace")
plot(fitBCFA, pars = 1:2, plot.type = "autocorr")
summary(fitBCFA)
```

