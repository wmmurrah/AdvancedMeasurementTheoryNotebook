---
title: Classic Test Theory 
format:  
  html:
    code-copy: true
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
opts_chunk$set(echo = TRUE, comment = NULL)
library(psych)
library(MPsychoR)
```


## Measurement in Science

**Measurement** is the quantification of theoretical constructs by means of assigning labels or numbers to observation, in a systematic way.
This is one way in which we simplify reality as a means to better understand it.
The vast majority, maybe all, of the constructs we want to learn about are not directly measurable.
When we make measurement, we inevitably must leave out some information about what we are observing, hence simplifying it.

A very important issue in measurement is validity. 
**Validity** generally is the extent to which our measures reflect what we are attempting to measure, in a particular context.
A similar concept is **reliability**, which deals with the consistency of our measures in a given context.
If we were to take the same measurement of the same thing in the same context, we would expect to get the same measurement. 
To the extent that this is true, the measure is reliable.
Note that to be valid an instrument must be reliable, but just because an instrument is reliable does not mean it is valid.

The concept of **invariance** relates to the extent to which scores on a measure are independent of examinee characteristics not relevant to the construct attempting to be measured. 
These characteristics can include things like gender, ethnicity, cultural background etc.

### Scales of Measurement

They way we quantify or classify constructs to generate measures can be classified by the scheme in the following figure.

```{mermaid}
flowchart TB
  A[Scales of Measurement] --> B[Qualitative/Categorical]
  A --> C[Quantitative/Numeric]
  B --> D[Nominal]
  B --> E[Ordinal]
  C --> F[Interval]
  C --> G[Ratio]
```


To understand the scales of measurement, let's use a data example.
First, we will import a small data set of questions I ask students in some of my statistics classes.

```{r}
#| eval: false
# File location on github:
file_location <- "https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/data/student_survey.csv"

# Import data from csv file:
student_survey <- read.csv(file = file_location,
                           header = TRUE)
# View raw data
student_survey
```

```{r}
#| echo: false
file_location <- "https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/data/student_survey.csv"
# Import data from csv file:
student_survey <- read.csv(file = file_location,
                           header = TRUE)

rmarkdown::paged_table(student_survey, options = list(cols.print = 5))
```

You can look at the codebook for this data below in section 3.5.1.
We can see that all of the variables are coded as integers (see the `<int>` under the variable name in the data frame), with the exception of gender with is a character string (`<chr>`). 
But if you look over the variables, and read the variable descriptions in the codebook,  you may realize that some of the variables are not best considered numeric.
We will need to recode those variables.
While we do that we will discuss scales of measurement.

For example, we can see that the `sem` variable quantifies the current semester (Spring, Fall, or Summer) for the student taking the survey.
Here we table this variable
```{r}
table(student_survey$sem)
```

Semester is clearly either a nominal or ordinal scale of measurement.
It could be ordinal because, in a given calendar year spring comes before summer which comes before fall.
But for our purposes of this survey the ordering is not important, so we will ignore it for now and create a nominal variable.
In R we do this with the `factor()` function as follows:

```{r}
student_survey$sem <- factor(student_survey$sem,
                             levels = c(1:3),
                             labels = c("Spring", "Fall", "Summer"))
```

This code tells R to create an object in side of `student_survey` called `sem`.
Because this object already exists here, this code will replace the existing object with the new one.
Then the `factor()` function take an object as the first argument (`student_survey$sem`) which is the old object.
So, essentially we are going take the old object turn it into a factor and replace the old object with the newly created factor.
The next argument  `levels = c(1:3)` tells R that the values of the original object are the integers 1, 2, and 3. 
Then, the `labels = c("Spring", "Fall", "Summer")` argument maps the three character strings ("Spring", "Fall", "Summer") onto the integers 1, 2, and 3. 
The ordering of the two vectors (1, 2, and 3 on the one hand and "Spring", "Fall", "Summer" on the other are important.
"Spring" is mapped onto 1, "Fall" onto 2, and "Summer" onto 3.
After doing this we can table this variable again and see what happened.

```{r}
table(student_survey$sem)
```

We can do something similar with the `hand` variable, which the codebook states captures the student's handedness, and also is a nominal variable.
But this time instead of saving the new variable over the old, I will create a new variable I will call `handedness`.

```{r}
student_survey$handedness <- factor(student_survey$hand, 
                                    levels = c(1,2),
                                    labels = c("left", "right"))
```

The major difference here is on the left side of the assignment operator (`<-`).
Instead of using the same name of the original object `hand`, I gave it a new name `handedness`.
Also note that in the `levels` argument, instead of the `1:2` shortcut I used `c(1,2)`, does the same thing.

```{r}
table(student_survey$handedness)
```

We have two more nominal variables gender and course.
Next, let's recode `gender`.
Because this variable contains character strings, which we can use as the labels, the code is simpler, we do not have to pass the `levels` or `labels` arguments.

```{r}
student_survey$gender <- factor(student_survey$gender)
```

```{r}
table(student_survey$gender)
```

Our final nominal variable is  `course`, which measures which course the student taking the survey was enrolled.
Because the labels are a bit more cumbersome, and to keep the code readable, we will first create a vector of the labels called `lbls`.
Then we can use that vector in the `factor()` function.
When we are done with the `lbls` object we will remove it with the `rm()` function.
Finally, we will table the new variable.

```{r}
# Create temporary labels for course factor.
lbls <- c("ERMA 7200 Basic Methods in Education Research",
          "ERMA 7300 Design and Analysis I",
          "ERMA 7310 Design and Analysis II",
          "ERMA 8340 Advanced Psychometrics")
student_survey$course <- factor(student_survey$course, 
                                levels = c(1,2,3,4),
                                labels = lbls)
rm(lbls) # Remove labels object

table(student_survey$course)
```

Ordinal Variables are those that have a natural order but the interval between those variables is not necessary the same across the different values.
It the student survey data an example is `birth` which measured the birth order of students.

```{r}
student_survey$birth <- ordered(student_survey$birth)

table(student_survey$birth)
```

The last 20 variables of the student survey are question that ask about research and statistics.
These are also measured as integers but should be ordinal variables. 
Creating a vector of labels as we did with the `course` variable, is also useful when you need to recode several variables with the same labels, such as in a set of variables that use the same Likert scale, as is the case for the Research and Statistics questions in the student survey.
Below, we again create a object called `lbls` with the Likert labels.
Then we create a vector of the column numbers that contain the Likert items, which are the 15th through the 31st columns, and name it `cols`.
In R the square brackets are indexing functions and it allows us to use only a subset of the columns in the data frame.
Then we use the `lapply` to repeat the `factor()` function for each of the Likert columns.

```{r}
# Likert labels
lbls <- c("strongly disagree", "disagree", "neither agree/disagree", 
          "agree", "strongly agree")

# Column numbers containing Likert variables.
cols <- 12:31

# Use indexing to transform all Likert items to ordered factor.
student_survey[ ,cols] <- lapply(student_survey[ ,cols], 
                               function(x) factor(x, 
                                                  levels = c(1,2,3,4,5),
                                                  labels = lbls, 
                                                  ordered = TRUE))

```

Note that to make a function ordered, which is the way to create ordinal variables in R, you pass the value `TRUE` to the `ordered` function. 
It will use the order of `levels` to order the values.


Here is the new dataframe

```{r}
#| eval: false
student_survey
```

```{r}
#| echo: false
rmarkdown::paged_table(student_survey, options = list(cols.print = 5))
```
## Classical True Score Model

The true score model is:
$$
X = T + E \tag{1}
$$
where $X$ is the **observed score**, $T$ is the **true score**, which is unknown, and $E$ is the **error**

Four assumptions to the model above:

1. $E(X) = T$, the expected value of the observed score $X$ is the true score $T$.

2. $Cov(T,E) = 0$, the true score ane error are independent(not correlated)

3. $Cov(E_1, E)2 = 0$, errors across test forms are independent.

4. $Cov(E_1, T_2) = 0$, error on one form of test is independent of the true score on another form.

Which leads to a re-expression of equation (1) above:

$$
\sigma^2_X = \sigma^2_T + \sigma^2_E
$$

To demonstrate this let's assume we have the following data [^1] , which was generated to meet these assumptions.

```{r}
# Filepath to data on github. 
filepath <- "https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/code/generateToy_CTTdata.R"
source(filepath)
CTTdata
```

where `id` is a variable indicating individual test-takers, `time` indicated which of 3 times each individual was assessed, `x1` - `x10` are the scores on 10 items that comprise the test, and `Tau` is the true value of the individuals ability. 
I use `Tau` here instead of `T`, because `T` is a protected symbol in R which is short-hand for `TRUE`.
Note that we would not know `Tau` in most situations, but because this is simulated data we will pretend we do.

We can create a composite score for the ten items for each individual on each occasion by averaging columns 3 through 12.

```{r}
CTTdata$X <- rowMeans(CTTdata[ ,3:12])
```

And we can also create `E`, the error with:

```{r}
CTTdata$E <- CTTdata$X - CTTdata$Tau
```
Again, in practice we would not be able to directly compute `E` because we would not know `Tau`, but we will use it to build an understanding of what error is.

Now we have:

```{r}
CTTdata
```
Look over the last three columns and make sure you understand their relation. 
For example, in the first row, note that `X` is .2 points above `Tau`, which is exactly the value of `E` we computed ($X_1 - T_1 = E_1 =  4.2 - 4 = .2$).
The 1 subscript in the previous expression indicated row 1 (i.e. i = 1).


```{r}
CTTdata$X_t <- round(ave(CTTdata$X, CTTdata$id, FUN = mean),1)
```

```{r}
CTTdata
```

## Reliability

$$
\text{reliability} = \frac{\sigma^2_T}{\sigma^2_X} = \frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E} = \rho^2_{XT}
$$

The reliability is the proportion of variance of $T$ in $X$, which is also the squared correlation between $X$ and $T$.

```{r}
Tau <- CTTdata$Tau
X <- CTTdata$X
E <- CTTdata$X - CTTdata$Tau
```

```{r}
var(Tau)/var(X)
```
```{r}
var(Tau)/(var(Tau) + var(E))
```

```{r}
cor(Tau, X)^2
```

```{r}
#| warning: false
#| message: false
library(hemp)
split_half(CTTdata, type = "alternate")
```
```{r}
coef_alpha(CTTdata)
```


```{r}
plot(x = CTTdata$Tau, y = CTTdata$id, xlim = c(1,10),
     ylim = c(0,7))
points(x = CTTdata$X, y = jitter(CTTdata$id), pch = 3, col = "red")
points(x = ave(x = CTTdata$X, factor(CTTdata$id), FUN = mean), y = CTTdata$id, 
       col = "blue", pch = 18)

 points(x = CTTdata$X_t, pch = 2, factor(CTTdata$id))
```


## SAPA Example

In this section I wil use data from the `hemp` package.

```{r}
library(hemp)
data("SAPA")
```

Take a few minutes to look at the data description.
```{r}
?SAPA
```

You can explore individual items as follows:
```{r}
prop.table(table(SAPA$reason.4))
barplot(prop.table(table(SAPA$reason.4)))
```

You can look at the proportion correct for all items.

```{r}
# Proportion correct for each item:
cbind(proportion_correct = colMeans(SAPA, na.rm = TRUE))
```
```{r}
num_miss(SAPA)
```

### Reliability

```{r}
split_half(SAPA, type = "alternate")
```
The split-half reliability coefficient is known to be downwardly biased.
The Spearman-Brown formula can adjust for this.
To get the Spearman-Brown reliability estimate use the following.

```{r}
split_half(SAPA, sb = TRUE)
```

We might wish to estimate what length of test is needed to achieve a particular reliability.

```{r}
test_length(SAPA, r = .95, r_type = "split")
```




#### Cronbach's $\alpha$

```{r}
coef_alpha(SAPA)
```

```{r}
psych::alpha(SAPA)
```
To get bootstraped confidence intervals for the `hemp` coefficient alpha.

```{r}
library(boot)

alpha_fun <- function(data, row){
  coef_alpha(data[row, ])
}

alpha_boot <- boot(data = SAPA, statistic = alpha_fun, 
                   R = 1e4)
alpha_boot
```

```{r}
plot(alpha_boot)
```

```{r}
boot.ci(alpha_boot, type = "bca")
```

### Validity

Validity is more complicated to estimate than reliability.
One useful from of validity is *criterion related validity* which is assessed by looking at how closely aligned scores on the scale you are evaluating are with some other established measure of this construct.

```{r}
data("interest")

print(cor(interest[ ,c("vocab", "reading", "sentcomp")]), digits= 2)
```

### Item Analysis

Item difficulty is an estimate of how hard a particular item is.
A fairly straight-forward way to assess item difficulty is looking at the proportion of participants who answered each item correctly.
If our items are score `0` or `1` for incorrect and correct answers respectively, we can calculate the column (item) means to get the proportion correct.

```{r}
item_difficulty <- colMeans(SAPA, na.rm = TRUE)
round(cbind(item_difficulty), digits = 2)
```
Note that a more intuitive name for this estimate would be item easiness, as the higher the number, the easier the item is. 
But we use item difficulty for historical reasons.

We can aslo calculate the *item discrimination*, which is a measure of how well an item discriminates between participants with high ability vs. those with low ability.
The most common way to do this is to calcualte the point-biserial correlation between a participants score on an item and their total score.

```{r}
total_score <- rowSums(SAPA, na.rm = TRUE)
item_discrimination <- cor(SAPA,
                           total_score,
                           use = "pairwise.complete.obs")

item_discrimination
```
higher values (closer to 1.00) mean the item has good discrimination, while values close to zero suggest little or not relation, and high negative numbers, suggest that people who do well on the rest of the instrument tend to do poorly in this item.
This last situation often suggests something unintended is going on with the item or, said differently, the item is not "behaving" well.

Another way to calculate discrimination of items is to calculate the *item discrimination index* which splits the test takers into a high and low group based on their total score and then correlate this grouping variable with each item response.

```{r}
idi(SAPA, SAPA$reason.4, perc_cut = .27)
```

```{r}
iri(SAPA)
```

Finally, for multiple choice tests, you may also want look at the distractors, which are the incorrect answers to such a question. 
This is done by looking at barplots or the proportion of test takers that answer each choice.
If there are wrong choices that many test takers select, you may want to reconsider the distractors. There may be something confusing about the choices. 
If you have a popular distractor and a low discrimination measure, changing the distractor may help.


## R Scripts and Data
### Student Survey Data Codebook

```{r}
#| echo: false
writeLines(readLines("data/student_survey_codebook.txt"))
```

### Simulating CTT data

```{r}
#------------------------------------------------------------------------
# Title: simulate_CTTdata
# Author: William Murrah
# Description: Simulate data to demonstrate CTT and reliability
# Created: Monday, 09 August 2021
# R version: R version 4.1.0 (2021-05-18)
# Project(working) directory: /Users/wmm0017/Projects/Courses/
#   AdvancedMeasurementTheoryNotebook
#------------------------------------------------------------------------

simx <- function(truescore, sigmax = 1) {
  x <- rnorm(18, truescore, sigmax)
  return(round(x))
}
id <- rep(1:6, each = 3)
Tau <- rep(rep(4:6, each = 3),2)
set.seed(20210805)
CTTdata <- data.frame(
  id = id,
  time = rep(1:3, 6),
  x1 = simx(Tau),
  x2 = simx(Tau),
  x3 = simx(Tau),
  x4 = simx(Tau),
  x5 = simx(Tau),
  x6 = simx(Tau),
  x7 = simx(Tau),
  x8 = simx(Tau),
  x9 = simx(Tau),
  x10 = simx(Tau),
  Tau = Tau
)
rm(id, Tau, simx)

```

[^1]: You can use the code below or copy the R script is at end of this chapter and store it on your computer, though you will have to adapt the code to your location
