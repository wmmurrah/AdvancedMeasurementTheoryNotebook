[["index.html", "Advanced Measurement Theory Course Notebook Introduction to the Course Resources for Learning R R Packages How To Use These Notes", " Advanced Measurement Theory Course Notebook William M. Murrah 2022-12-10 Introduction to the Course Welcome! This is a notebook for ERMA 8350 Advanced Measurement Theory. The class will be using the textbook Handbook of Educational Measurement and Psychometrics Using R, which will be the primary source for the course . I will use this notebook to make available additional information to help you learn the material. It may include some examples from the textbook, with some elaborations, additional readings, and some more details about implementing the methods in R. These web-based notes will make it easy for you to use code, by allowing you to copy and paste code found within. Some of you will have experience with R and others not. So I will try to also point you to additional resources that may be helpful. For example, in this preface I will provide links to resources to help you setup R and RStudio. RStudio is a platform to make using R more productive. I will use it extensively in this course (Desjardins and Bulut 2018). There are at least two way you can access the software needed for this course. You can use the virtual labs on campus. I know at least the education virtual labls have R and RStudio installed. IF you go this route you can watch the following video. Note you will need Duo setup for this to work. Using Vlab to acces R/RStudio A better option if you have a laptop, you can install both programs on your computer. They are both absolutely free and available on all major operating systems, so you will not have to worry about transferring information across computers, limited connection speeds, or other hassles inherent with the VLab route. The following links take you to videos instructing you how to install them. Installing R and RStudio Organizing Projects in RStudio Resources for Learning R While such experience is certainly helpful, I do not assume you have prior knowledge of using R. I will demonstrate the use of R and provide (particularly in this notebook) the R code needed to use the methods we will learn. However, even if you have prior experience with R, you should plan to spend time learning to program in R. Some people find this intimidating initially, but most of you will grow to find R programming rewarding, and even fun by the end of the course. But, there will be frustration for sure. Here are some good places to start learning R: CRAN R Packages R is, among other paradigms, a functional programming language, which means is heavily utilizes functions. R’s functions are stored in packages. While base R has a long list of very useful functions, to fully realize the power of R you will have to use additional packages. So, learning how to install packages (downloading from the web to your computer) and loading packages (making the package’s functions accessible to your current R session) are important skills to master. How To Use These Notes Before going further, it may be helpful to watch the following video about how to use the code in this notebook with Rstudio: How to Use RStudio with this Notebook References "],["CCT.html", "Chapter 1 Classic Test Theory 1.1 Overview 1.2 Classical True Score Model 1.3 Reliability 1.4 Generalizability Theory 1.5 Additional Readings 1.6 R Scripts", " Chapter 1 Classic Test Theory 1.1 Overview Three important concepts: We can build a model of instruments similar to the way we model conceptual relationships (measurement model vs. structural model). The concept of reliability Measurement models can account for different sources of error. 1.2 Classical True Score Model The true score model is: \\[ X = T + E \\] where \\(X\\) is the observed score, \\(T\\) is the true score, which is unknown, and \\(E\\) is the error To demonstrate this let’s assume we have the following data (R script is at end of this chapter), source(&quot;code/simulate_CTTdata.R&quot;) CTTdata id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau 1 1 1 3 6 5 3 5 5 4 5 3 3 4 2 1 2 6 3 5 3 4 2 4 4 3 5 4 3 1 3 4 4 2 4 4 3 5 3 4 5 4 4 2 1 3 6 8 6 5 4 5 5 5 5 5 5 2 2 6 4 6 6 4 6 6 5 4 4 5 6 2 3 4 6 6 5 5 5 1 3 6 4 5 7 3 1 6 5 6 6 6 6 9 6 6 5 6 8 3 2 6 6 6 7 5 6 6 6 6 7 6 9 3 3 6 5 8 6 6 6 7 7 5 7 6 10 4 1 4 3 5 4 2 3 3 5 5 2 4 11 4 2 4 5 5 4 5 5 3 5 3 4 4 12 4 3 2 4 4 4 6 4 3 4 5 4 4 13 5 1 5 6 5 4 5 5 5 6 5 6 5 14 5 2 6 6 4 6 4 5 4 5 5 5 5 15 5 3 6 4 5 4 5 5 4 4 5 5 5 16 6 1 6 6 7 8 6 6 7 6 6 4 6 17 6 2 4 5 7 5 5 7 4 5 6 7 6 18 6 3 5 6 6 6 4 5 4 5 7 6 6 where id is a variable indicating individual test-takers, time indicated which of 3 times each individual was assessed, x1 - x10 are the scores on 10 items that comprise the test, and Tau is the true value of the individuals ability. I use Tau here instead of T, because T is a protected symbol in R which is short-hand for TRUE. Note that we would not know Tau in most situations, but because this is simulated data we will pretend we do. We can create a composite score for the ten items for each individual on each occasion by averaging columns 3 through 12. CTTdata$X &lt;- rowMeans(CTTdata[ ,3:12]) And we can also create E, the error with: CTTdata$E &lt;- CTTdata$X - CTTdata$Tau Again, in practice we would not be able to directly compute E because we would not know Tau, but we will use it to build an understanding of what error is. Now we have: CTTdata id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau X E 1 1 1 3 6 5 3 5 5 4 5 3 3 4 4.2 0.2 2 1 2 6 3 5 3 4 2 4 4 3 5 4 3.9 -0.1 3 1 3 4 4 2 4 4 3 5 3 4 5 4 3.8 -0.2 4 2 1 3 6 8 6 5 4 5 5 5 5 5 5.2 0.2 5 2 2 6 4 6 6 4 6 6 5 4 4 5 5.1 0.1 6 2 3 4 6 6 5 5 5 1 3 6 4 5 4.5 -0.5 7 3 1 6 5 6 6 6 6 9 6 6 5 6 6.1 0.1 8 3 2 6 6 6 7 5 6 6 6 6 7 6 6.1 0.1 9 3 3 6 5 8 6 6 6 7 7 5 7 6 6.3 0.3 10 4 1 4 3 5 4 2 3 3 5 5 2 4 3.6 -0.4 11 4 2 4 5 5 4 5 5 3 5 3 4 4 4.3 0.3 12 4 3 2 4 4 4 6 4 3 4 5 4 4 4.0 0.0 13 5 1 5 6 5 4 5 5 5 6 5 6 5 5.2 0.2 14 5 2 6 6 4 6 4 5 4 5 5 5 5 5.0 0.0 15 5 3 6 4 5 4 5 5 4 4 5 5 5 4.7 -0.3 16 6 1 6 6 7 8 6 6 7 6 6 4 6 6.2 0.2 17 6 2 4 5 7 5 5 7 4 5 6 7 6 5.5 -0.5 18 6 3 5 6 6 6 4 5 4 5 7 6 6 5.4 -0.6 Look over the last three columns and make sure you understand their relation. For example, in the first row, note that X is .2 points above Tau, which is exactly the value of E we computed (\\(X_1 - T_1 = E_1 = 4.2 - 4 = .2\\)). The 1 subscript in the previous expression indicated row 1 (i.e. i = 1). CTTdata$X_t &lt;- round(ave(CTTdata$X, CTTdata$id, FUN = mean),1) CTTdata id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau X E X_t 1 1 1 3 6 5 3 5 5 4 5 3 3 4 4.2 0.2 4.0 2 1 2 6 3 5 3 4 2 4 4 3 5 4 3.9 -0.1 4.0 3 1 3 4 4 2 4 4 3 5 3 4 5 4 3.8 -0.2 4.0 4 2 1 3 6 8 6 5 4 5 5 5 5 5 5.2 0.2 4.9 5 2 2 6 4 6 6 4 6 6 5 4 4 5 5.1 0.1 4.9 6 2 3 4 6 6 5 5 5 1 3 6 4 5 4.5 -0.5 4.9 7 3 1 6 5 6 6 6 6 9 6 6 5 6 6.1 0.1 6.2 8 3 2 6 6 6 7 5 6 6 6 6 7 6 6.1 0.1 6.2 9 3 3 6 5 8 6 6 6 7 7 5 7 6 6.3 0.3 6.2 10 4 1 4 3 5 4 2 3 3 5 5 2 4 3.6 -0.4 4.0 11 4 2 4 5 5 4 5 5 3 5 3 4 4 4.3 0.3 4.0 12 4 3 2 4 4 4 6 4 3 4 5 4 4 4.0 0.0 4.0 13 5 1 5 6 5 4 5 5 5 6 5 6 5 5.2 0.2 5.0 14 5 2 6 6 4 6 4 5 4 5 5 5 5 5.0 0.0 5.0 15 5 3 6 4 5 4 5 5 4 4 5 5 5 4.7 -0.3 5.0 16 6 1 6 6 7 8 6 6 7 6 6 4 6 6.2 0.2 5.7 17 6 2 4 5 7 5 5 7 4 5 6 7 6 5.5 -0.5 5.7 18 6 3 5 6 6 6 4 5 4 5 7 6 6 5.4 -0.6 5.7 1.3 Reliability \\[ \\text{reliability} = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E} = \\rho^2_{XT} \\] The reliability is the proportion of variance of \\(T\\) in \\(X\\), which is also the squared correlation between \\(X\\) and \\(T\\). Tau &lt;- CTTdata$Tau X &lt;- CTTdata$X E &lt;- CTTdata$X - CTTdata$Tau var(Tau)/var(X) [1] 0.9170806 var(Tau)/(var(Tau) + var(E)) [1] 0.8898776 cor(Tau, X)^2 [1] 0.886766 plot(x = CTTdata$Tau, y = CTTdata$id, xlim = c(1,10), ylim = c(0,7)) points(x = CTTdata$X, y = jitter(CTTdata$id), pch = 3, col = &quot;red&quot;) points(x = ave(x = CTTdata$X, factor(CTTdata$id), FUN = mean), y = CTTdata$id, col = &quot;blue&quot;, pch = 18) # points(x = CTTdata$X_t, pch = 2, factor(CTTdata$id)) 1.3.1 Cronbach’s \\(\\alpha\\) In the notes for this chapter, I demonstrate aspects of classical test theory, reliability and generalizability theory using data from a study exploring the motivation of R package authors (Mair et al. 2015). This tutorial is based on Chapter 1 of Mair (2018), which can be consulted for a more in depth exposition of the underlying theory. Here I focus on demonstrating some of those concepts in R, as well as describing how to get certain results in R. First, I load the packages used in this tutorial: # Packages used: library(psych) library(MPsychoR) Next, I load the full data set from the MPsychoR package (Mair 2020), then as in the chapter, I subset the data to only include hybrid motivation items, followed by removing rows with missing values. data(&quot;Rmotivation&quot;) # Create data frame with only Hybrid Motivation items. HybMot &lt;- subset(Rmotivation, select = grep(&quot;hyb&quot;, names(Rmotivation))) # Remove rows with any missing data. HybMot &lt;- na.omit(HybMot) This leads to a data set with 777 authors and 19 items. # How many authors(rows) and items(columns)? dim(HybMot) [1] 777 19 # Note they are all dichotomous items. head(HybMot) hyb1 hyb2 hyb3 hyb4 hyb5 hyb6 hyb7 hyb8 hyb9 hyb10 hyb11 hyb12 hyb13 hyb14 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 3 0 0 1 0 1 0 0 0 0 1 0 1 1 0 4 1 1 1 1 1 0 1 0 0 1 0 1 1 0 5 1 0 0 1 1 0 0 0 0 1 1 1 1 0 8 1 1 1 1 1 1 1 1 1 1 0 1 1 1 9 1 0 0 1 1 0 0 0 0 1 0 0 1 0 hyb15 hyb16 hyb17 hyb18 hyb19 1 1 1 1 1 1 3 1 0 0 1 0 4 0 1 1 1 1 5 1 0 1 1 1 8 1 1 1 1 1 9 0 1 1 1 1 # Variance/Covariance Matrix vcmat &lt;- cov(HybMot) scroll_box(kable(vcmat, digits = 2), width = &quot;100%&quot;) hyb1 hyb2 hyb3 hyb4 hyb5 hyb6 hyb7 hyb8 hyb9 hyb10 hyb11 hyb12 hyb13 hyb14 hyb15 hyb16 hyb17 hyb18 hyb19 hyb1 0.18 0.06 0.04 0.03 0.03 0.05 0.01 0.05 0.04 0.04 0.03 0.03 0.03 0.03 0.04 0.05 0.04 0.03 0.06 hyb2 0.06 0.25 0.06 0.05 0.03 0.05 -0.01 0.04 0.05 0.02 0.04 0.04 0.03 0.03 0.03 0.03 0.01 0.00 0.03 hyb3 0.04 0.06 0.23 0.13 0.03 0.05 0.00 0.03 0.05 0.04 0.05 0.06 0.02 0.02 0.06 0.02 0.03 0.02 0.04 hyb4 0.03 0.05 0.13 0.21 0.03 0.04 0.01 0.03 0.04 0.03 0.05 0.05 0.02 0.02 0.04 0.02 0.03 0.02 0.04 hyb5 0.03 0.03 0.03 0.03 0.11 0.02 0.00 0.01 0.01 0.03 0.03 0.03 0.03 0.02 0.04 0.02 0.02 0.01 0.03 hyb6 0.05 0.05 0.05 0.04 0.02 0.24 0.01 0.11 0.15 0.05 0.06 0.06 0.02 0.07 0.06 0.05 0.05 0.02 0.07 hyb7 0.01 -0.01 0.00 0.01 0.00 0.01 0.22 0.04 0.01 0.03 0.00 0.02 0.00 0.00 0.01 0.02 0.02 0.01 0.02 hyb8 0.05 0.04 0.03 0.03 0.01 0.11 0.04 0.25 0.10 0.06 0.05 0.06 0.02 0.04 0.06 0.05 0.05 0.03 0.06 hyb9 0.04 0.05 0.05 0.04 0.01 0.15 0.01 0.10 0.20 0.04 0.04 0.05 0.01 0.05 0.04 0.04 0.03 0.01 0.05 hyb10 0.04 0.02 0.04 0.03 0.03 0.05 0.03 0.06 0.04 0.15 0.03 0.06 0.03 0.02 0.04 0.04 0.04 0.03 0.05 hyb11 0.03 0.04 0.05 0.05 0.03 0.06 0.00 0.05 0.04 0.03 0.23 0.10 0.03 0.04 0.10 0.03 0.03 0.01 0.02 hyb12 0.03 0.04 0.06 0.05 0.03 0.06 0.02 0.06 0.05 0.06 0.10 0.23 0.04 0.03 0.11 0.04 0.03 0.02 0.04 hyb13 0.03 0.03 0.02 0.02 0.03 0.02 0.00 0.02 0.01 0.03 0.03 0.04 0.10 0.01 0.03 0.03 0.02 0.01 0.02 hyb14 0.03 0.03 0.02 0.02 0.02 0.07 0.00 0.04 0.05 0.02 0.04 0.03 0.01 0.20 0.03 0.04 0.03 0.01 0.04 hyb15 0.04 0.03 0.06 0.04 0.04 0.06 0.01 0.06 0.04 0.04 0.10 0.11 0.03 0.03 0.23 0.03 0.03 0.02 0.04 hyb16 0.05 0.03 0.02 0.02 0.02 0.05 0.02 0.05 0.04 0.04 0.03 0.04 0.03 0.04 0.03 0.17 0.06 0.05 0.07 hyb17 0.04 0.01 0.03 0.03 0.02 0.05 0.02 0.05 0.03 0.04 0.03 0.03 0.02 0.03 0.03 0.06 0.14 0.04 0.07 hyb18 0.03 0.00 0.02 0.02 0.01 0.02 0.01 0.03 0.01 0.03 0.01 0.02 0.01 0.01 0.02 0.05 0.04 0.08 0.05 hyb19 0.06 0.03 0.04 0.04 0.03 0.07 0.02 0.06 0.05 0.05 0.02 0.04 0.02 0.04 0.04 0.07 0.07 0.05 0.21 k &lt;- ncol(HybMot) sigma2_Xi &lt;- tr(vcmat) # trace of matrix or sum(diag(vmat)) sigma2_X &lt;- sum(vcmat) 1.3.2 Other Reliability Coefficients 1.4 Generalizability Theory Generalizability theory, or G-theory for short, is an extension of CTT, which decomposes the one error term in CTT into multiple sources of error called facets. These could include sources such as items, raters, or measurement occasions. These were each given a subscript on page 2 of the text. Before looking at these different sources of error, let’s calculate Cronbach’s \\(\\alpha\\) in a different way, that will allow this decomposition going forward. We will first need to reshape the data from wide to long format. A great tutorial on reshaping data with the reshape2 package can be found here: https://seananderson.ca/2013/10/19/reshape/ Basically, we need to transform the data so that instead of each item being in a separate column are reshaped so there is one column with the cell values, and one column that identifies which item the score is from. library(&quot;reshape2&quot;) # Add person variable Hyb1 &lt;- data.frame(HybMot, person = 1:nrow(HybMot)) Hyblong &lt;- melt(Hyb1, id.vars = c(&quot;person&quot;), variable.name = &quot;item&quot;) Hyblong$person &lt;- as.factor(Hyblong$person) 1.4.1 Reliability and Generalizability summary(aov(value ~ person + item, data = Hyblong)) Df Sum Sq Mean Sq F value Pr(&gt;F) person 776 663.0 0.85 5.549 &lt;2e-16 *** item 18 573.8 31.88 207.048 &lt;2e-16 *** Residuals 13968 2150.5 0.15 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From this output we can calculate Cronbach’s \\(\\alpha\\) with the following values: MSp &lt;- 0.85 MSe &lt;- 0.15 alpha &lt;- (MSp - MSe)/MSp print(alpha, digits = 2) [1] 0.82 ICC(HybMot) Call: ICC(x = HybMot) Intraclass correlation coefficients type ICC F df1 df2 p lower bound Single_raters_absolute ICC1 0.15 4.4 776 13986 7.2e-279 0.14 Single_random_raters ICC2 0.16 5.5 776 13968 0.0e+00 0.14 Single_fixed_raters ICC3 0.19 5.5 776 13968 0.0e+00 0.17 Average_raters_absolute ICC1k 0.77 4.4 776 13986 7.2e-279 0.75 Average_random_raters ICC2k 0.78 5.5 776 13968 0.0e+00 0.75 Average_fixed_raters ICC3k 0.82 5.5 776 13968 0.0e+00 0.80 upper bound Single_raters_absolute 0.17 Single_random_raters 0.18 Single_fixed_raters 0.21 Average_raters_absolute 0.79 Average_random_raters 0.81 Average_fixed_raters 0.84 Number of subjects = 777 Number of Judges = 19 See the help file for a discussion of the other 4 McGraw and Wong estimates, icchyb &lt;- ICC(HybMot) sqrt((0.85-0.15)/19) [1] 0.191943 sqrt((31.88-0.15)/777) [1] 0.2020806 library(&quot;lme4&quot;) Loading required package: Matrix VarCorr(lmer(value ~ (1|person) + (1|item), data = Hyblong)) Groups Name Std.Dev. person (Intercept) 0.19200 item (Intercept) 0.20206 Residual 0.39238 library(&quot;gtheory&quot;) gfit &lt;- gstudy(data = Hyblong, formula = value ~ (1|person) + (1|item)) dfit &lt;- dstudy(gfit, colname.objects = &quot;person&quot;, colname.scores = &quot;value&quot;, data = Hyblong) round(dfit$generalizability, 3) [1] 0.82 1.4.2 Multiple Sources of Error Generalizability theory acknowledges that multiple sources of error can impact scores simultaneously, and allow for estimating the effects of each (Raykov and Marcoulides 2011). These various sources of error, or facets (e.g., items, raters, measurement occasions). All measurements of behavior are conceptualized as being sampled from a universe of admissible observations (Raykov and Marcoulides 2011). If the observed score is expected to vary across a facet (e.g. vary across occasions, or vary depending on the items included, or the rater scoring), then that facet is a defining characteristic of the universe. The idea of reliability is replace with the idea of generalizability, which ,instead of asking how accurately observed scores can reflect the true score (CTT), generalizability theory asks how accurately observed scores allow us to generalize about behavior of an individual in a particular universe. Below is the code from the Mair (2018) text. library(gtheory) data(&quot;Lakes&quot;) phydat &lt;- subset(Lakes, subtest == &quot;physical&quot;) phydat$item &lt;- droplevels(phydat$item) head(phydat) personID raterID item score subtest 12611 1 7 phy1 5 physical 12612 1 1 phy1 5 physical 12613 1 3 phy1 5 physical 12614 1 8 phy1 4 physical 12615 1 5 phy1 6 physical 12616 2 3 phy1 5 physical formula &lt;- score ~ (1|personID) + (1|raterID) + (1|item) + (1|personID:raterID) + (1|personID:item) + (1|raterID:item) gfit &lt;- gstudy(formula = formula, data = phydat) gfit $components source var percent n 1 personID:raterID 0.127298630 10.7 1 2 personID:item 0.151229568 12.7 1 3 personID 0.458766113 38.5 1 4 raterID:item 0.008802081 0.7 1 5 raterID 0.139048763 11.7 1 6 item 0.033388424 2.8 1 7 Residual 0.272703419 22.9 1 attr(,&quot;class&quot;) [1] &quot;gstudy&quot; &quot;list&quot; dfit &lt;- dstudy(gfit, colname.objects = &quot;personID&quot;, colname.scores = &quot;score&quot;, data = phydat) dfit$components source var percent n 1 personID:raterID 0.0254597259 4.3 5 2 personID:item 0.0504098560 8.5 3 3 personID 0.4587661131 77.4 1 4 raterID:item 0.0005868054 0.1 15 5 raterID 0.0278097526 4.7 5 6 item 0.0111294746 1.9 3 7 Residual 0.0181802279 3.1 15 dfit$var.error.abs [1] 0.1335758 dfit$sem.abs [1] 0.3654803 dfit$var.error.rel [1] 0.09404981 dfit$sem.rel [1] 0.3066754 dfit$dependability [1] 0.7744954 dfit$generalizability [1] 0.8298714 1.5 Additional Readings For more information of G theory, see Raykov and Marcoulides (2011). For an example using the R package lavaan with G theory, see Jorgensen (2021). 1.6 R Scripts 1.6.1 Simulating CTT data #------------------------------------------------------------------------ # Title: simulate_CTTdata # Author: William Murrah # Description: Simulate data to demonstrate CTT and reliability # Created: Monday, 09 August 2021 # R version: R version 4.1.0 (2021-05-18) # Project(working) directory: /Users/wmm0017/Projects/Courses/AdvancedMeasurementTheoryNotebook #------------------------------------------------------------------------ simx &lt;- function(truescore, sigmax = 1) { x &lt;- rnorm(18, truescore, sigmax) return(round(x)) } id &lt;- rep(1:6, each = 3) Tau &lt;- rep(rep(4:6, each = 3),2) set.seed(20210805) CTTdata &lt;- data.frame( id = id, time = rep(1:3, 6), x1 = simx(Tau), x2 = simx(Tau), x3 = simx(Tau), x4 = simx(Tau), x5 = simx(Tau), x6 = simx(Tau), x7 = simx(Tau), x8 = simx(Tau), x9 = simx(Tau), x10 = simx(Tau), Tau = Tau ) rm(id, Tau, simx) References "],["generalizability-theory-1.html", "Chapter 2 Generalizability Theory", " Chapter 2 Generalizability Theory We describe our methods in this chapter. "],["factor-analysis.html", "Chapter 3 Factor Analysis 3.1 Correlation Coefficient 3.2 Think about these situations 3.3 Solutions 3.4 Principal Components Analysis 3.5 Factor Analysis 3.6 Components 3.7 PC Extraction 3.8 Eigenvalues 3.9 Determining the Number of Factors 3.10 Example data 3.11 Kaiser 3.12 Scree Plot 3.13 Horn’s Parallel Analysis 3.14 Another example 3.15 Rotation 3.16 Orthogonal Rotation 3.17 Oblique Rotation 3.18 Factor Matrices 3.19 Which matrix should we interpret? 3.20 Which variables should be used to interpret each factor? 3.21 Examples 3.22 Steps in Factor Analysis 3.23 Tom Swift’s Electric Factor Analysis Factory 3.24 Metal Boxes 3.25 Correlations 3.26 Eigenvalues &gt; 1 3.27 Orthogonal Rotation 3.28 Orthogonal Rotation with Loadings &gt; .70 3.29 R Code for Chapter 2", " Chapter 3 Factor Analysis 3.1 Correlation Coefficient Pearson product-moment correlation: \\[ r_{xy} = \\frac{\\Sigma_{n=1}^n (x_k - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{n=1}^n(x_i - \\bar{x})^2} \\sqrt{\\Sigma_{n=1}^n(y_i - \\bar{y})^2}} = \\frac{s_{xy}}{s_x s_y}. \\] The equation looks very daunting, until you see that it is just the covariance of \\(x\\) and \\(y\\) divided by the product of their standard deviations. library(&quot;MPsychoR&quot;) data(&quot;YouthDep&quot;) item1 &lt;- YouthDep[, 1] levels(item1) &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;1&quot;) item2 &lt;- YouthDep[, 14] levels(item2) &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;1&quot;) table(item1, item2) item2 item1 0 1 0 1353 656 1 115 166 ## ------ correlation coefficients library(&quot;psych&quot;) tetcor &lt;- tetrachoric(cbind(item1, item2)) tetcor Call: tetrachoric(x = cbind(item1, item2)) tetrachoric correlation item1 item2 item1 1.00 item2 0.35 1.00 with tau of item1 item2 1.16 0.36 item1 &lt;- YouthDep[, 1] item2 &lt;- YouthDep[, 14] polcor &lt;- polychoric(cbind(item1, item2)) polcor Call: polychoric(x = cbind(item1, item2)) Polychoric correlations item1 item2 item1 1.00 item2 0.33 1.00 with tau of 1 2 item1 1.16 2.3 item2 0.36 1.2 draw.tetra(r = .35, t1 = 1.16, t2 = .36) DepItems &lt;- YouthDep[,1:26] Depnum &lt;- data.matrix(DepItems) - 1 ## convert to numeric Rdep &lt;- polychoric(Depnum) 3.2 Think about these situations What do you do when you have a large number of variables you are considering as predictors of a dependent variable? Often, subsets of these variables are measuring the same, or very similar things. We might like to reduce the variables to a smaller number of predictors. What if you are developing a measurement scale and have a large number of items you think measure the same construct You might want to see how strongly the items are related to the construct. 3.3 Solutions Principal Components Analysis transforming the original variables into a new set of linear combinations (pricipal components). Factor Analysis setting up a mathematical model to estimate the number or factors 3.4 Principal Components Analysis Concerned with explaining variance-covariance structure of a set of variables. PCA attempts to explain as much of the total variance among the observed variables as possible with a smaller number of components. Because the variables are standardized prior to analysis, the total amount of variance available is the number of variables. The goal is data reduction for subsequent analysis. Variables cause components. Components are not representative of any underlying theory. 3.5 Factor Analysis The goal is understanding underlying constructs. Uses a modified correlation matrix (reduced matrix) factors cause the variables. Factors represent theoretical constructs. Focuses on the common variance of the variables, and purges the unique variance. 3.6 Components The principal components partition the total variance (the sum of the variances of the original variables) by finding the linear combination of the variables that account for the maximum amount of variance: \\[ PC1 = a_{11}x_1 + a_{12}x_2 ... a_{1p}x_p, \\] This is repeated as many time as there are variables. 3.7 PC Extraction draw pretty pictures on the board 3.8 Eigenvalues Eigenvalues represent the variance in the variables explained by the success components. 3.9 Determining the Number of Factors Kaiser criterion: Retain only factors with eigenvalues &gt; 1. (generally accurate) Scree plot: plot eigenvalues and drop factors after leveling off. Parallel analysis: compare observed eigenvalues to parallel set of data from randomly generated data. Retain factors in original if eigenvalue is greater than random eigenvalue. Factor meaningfulness is also very important to consider. 3.10 Example data lower &lt;- &quot; 1.00 0.70 1.00 0.65 0.66 1.00 0.62 0.63 0.60 1.00 &quot; cormat &lt;- getCov(lower, names = c(&quot;d1&quot;, &quot;d2&quot;, &quot;d3&quot;, &quot;d4&quot;)) cormat d1 d2 d3 d4 d1 1.00 0.70 0.65 0.62 d2 0.70 1.00 0.66 0.63 d3 0.65 0.66 1.00 0.60 d4 0.62 0.63 0.60 1.00 3.11 Kaiser Retain factors with eigenvalues greater than 1 eigen(cormat)$values [1] 2.9311792 0.4103921 0.3592372 0.2991916 3.12 Scree Plot scree(cormat, factors = FALSE) 3.13 Horn’s Parallel Analysis fa.parallel(cormat, fa = &quot;pc&quot;) Parallel analysis suggests that the number of factors = NA and the number of components = 1 3.14 Another example fa.parallel(Harman74.cor$cov, fa = &quot;pc&quot;) Parallel analysis suggests that the number of factors = NA and the number of components = 2 3.15 Rotation Principal components are derived to maximize the variance accounted for (data reduction). Rotation is done to make the factors more interpretable (i.e. meaningful). Two major classes of rotation: Orthogonal - new factors are still uncorrelated, as were the initial factors. Oblique - new factors are allowed to be correlated. Essentially reallocates the loadings. The first factor may not be the one accounting for the most variance. 3.16 Orthogonal Rotation Quartimax - idea is to clean up the variables. Rotation done so each variable loads mainly on one factor. Problematic if there is a general factor on which most or all variables load on (think IQ). Varimax - to clean up factors. So each factor has high correlation with a smaller number of variables, low correlation with the other variables. Generally makes interpretation easier. 3.17 Oblique Rotation Often correlated factors are more reasonable. Therefore, oblique rotation is often preferred. But interpretation is more complicated. 3.18 Factor Matrices Factor pattern matrix: includes pattern coefficients analogous to standardized partial regression coefficients. Indicated the unique importance of a factor to a variable, holding other factors constant. Factor structure matrix: includes structure coefficients which are simple correlations of the variables with the factors. 3.19 Which matrix should we interpret? When orthogonal rotation is used interpret structural coefficients (but they are the same as pattern coefficients). When oblique rotation is used pattern coefficients are preferred because they account for the correlation between the factors and they are parameters of the correlated factor model (which we will discuss next class). 3.20 Which variables should be used to interpret each factor? The idea is to use only those variables that have a strong association with the factor. Typical thresholds are |.30| or |.40|. Content knowledge is critical. 3.21 Examples Let’s look at some examples 3.22 Steps in Factor Analysis Choose extraction method So far we’ve focused on PCA Determine the number of components/factors Kaiser method: eigenvalues &gt; 1 Scree plot: All components before leveling off Horn’s parallel analysis: components/factors greater than simulated values from random numbers Rotate Factors Orthogonal Oblique Interpret Components/Factors 3.23 Tom Swift’s Electric Factor Analysis Factory “Little Jiffy” method of factor analysis Extraction method : PCA Number of factors: eigenvalues &gt; 1 Rotation: orthogonal(varimax) Interpretation 3.24 Metal Boxes Table 3.1: Functional Definitions of Tom Swift’s Original 11 Variables Dimension Derivation Thickness x Width y Length z Volume xyz Density d Weight xyzd Surface area 2(xy + xz + yz) Cross-section yz Edge length 4(x + y + z) Diagonal length (x^2) Cost/lb c &#39;data.frame&#39;: 63 obs. of 11 variables: $ thick : num 1.362 2.385 3.101 0.934 0.845 ... $ width : num 1.71 2.83 4.32 3.2 3.84 ... $ length : num 2.93 5.01 5.99 4.15 4.09 ... $ volume : num 6.02 30.2 72.01 11.78 16.1 ... $ density : int 10 7 16 22 11 16 11 21 6 13 ... $ weight : num 60 210 1152 264 176 ... $ surface : num 22 62.1 108.2 38 48 ... $ crosssec: num 5.87 15.06 23.53 12.02 16.13 ... $ edge : num 23.9 39.9 51.5 31.9 36.1 ... $ diagonal: num 196 1444 3721 676 1089 ... $ cost : num 4.48 2.37 9.77 22.21 15.86 ... 3.25 Correlations Table 3.2: Correlations between dimensions thick width length volume density weight surface crosssec edge diagonal cost thick 1.00 0.49 0.24 0.84 -0.13 0.59 0.74 0.46 0.61 0.51 -0.02 width 0.49 1.00 0.61 0.77 -0.15 0.55 0.87 0.92 0.88 0.78 0.03 length 0.24 0.61 1.00 0.58 -0.02 0.45 0.72 0.83 0.84 0.86 -0.02 volume 0.84 0.77 0.58 1.00 -0.22 0.65 0.97 0.81 0.87 0.85 -0.11 density -0.13 -0.15 -0.02 -0.22 1.00 0.44 -0.20 -0.15 -0.15 -0.18 0.62 weight 0.59 0.55 0.45 0.65 0.44 1.00 0.65 0.56 0.61 0.57 0.24 surface 0.74 0.87 0.72 0.97 -0.20 0.65 1.00 0.92 0.97 0.91 -0.07 crosssec 0.46 0.92 0.83 0.81 -0.15 0.56 0.92 1.00 0.96 0.93 -0.03 edge 0.61 0.88 0.84 0.87 -0.15 0.61 0.97 0.96 1.00 0.92 -0.04 diagonal 0.51 0.78 0.86 0.85 -0.18 0.57 0.91 0.93 0.92 1.00 -0.12 cost -0.02 0.03 -0.02 -0.11 0.62 0.24 -0.07 -0.03 -0.04 -0.12 1.00 3.26 Eigenvalues &gt; 1 3.27 Orthogonal Rotation Loadings: RC1 RC3 RC2 RC5 RC4 thick 0.968 width 0.734 length 0.986 volume 0.754 density 0.864 weight surface 0.703 crosssec 0.829 edge 0.819 diagonal 0.875 cost 0.955 RC1 RC3 RC2 RC5 RC4 SS loadings 4.425 2.662 1.318 1.225 1.106 Proportion Var 0.402 0.242 0.120 0.111 0.101 Cumulative Var 0.402 0.644 0.764 0.876 0.976 3.28 Orthogonal Rotation with Loadings &gt; .70 Loadings: RC1 RC3 RC2 thick 0.947 width 0.801 length 0.936 volume 0.744 density 0.930 weight surface 0.792 crosssec 0.942 edge 0.892 diagonal 0.905 cost 0.841 RC1 RC3 RC2 SS loadings 5.298 2.699 1.868 Proportion Var 0.482 0.245 0.170 Cumulative Var 0.482 0.727 0.897 3.29 R Code for Chapter 2 ## ----------------------- Chapter 2: Factor Analysis ------------------ library(&quot;MPsychoR&quot;) data(&quot;YouthDep&quot;) item1 &lt;- YouthDep[, 1] levels(item1) &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;1&quot;) item2 &lt;- YouthDep[, 14] levels(item2) &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;1&quot;) table(item1, item2) ## ------ correlation coefficients library(&quot;psych&quot;) tetcor &lt;- tetrachoric(cbind(item1, item2)) tetcor item1 &lt;- YouthDep[, 1] item2 &lt;- YouthDep[, 14] polcor &lt;- polychoric(cbind(item1, item2)) polcor DepItems &lt;- YouthDep[,1:26] Depnum &lt;- data.matrix(DepItems) - 1 ## convert to numeric Rdep &lt;- polychoric(Depnum) data(&quot;Rmotivation&quot;) vind &lt;- grep(&quot;ext|int&quot;, colnames(Rmotivation)) Rmotivation1 &lt;- Rmotivation[, vind] Rmot1 &lt;- tetrachoric(Rmotivation1, smooth = FALSE) tail(round(eigen(Rmot1$rho)$values, 3)) Rmot &lt;- tetrachoric(Rmotivation1) tail(round(eigen(Rmot$rho)$values, 3)) ## ----- exploratory factor analysis motFA &lt;- fa(Rmot$rho, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;ml&quot;) print(motFA$loadings, cutoff = 0.2) round(motFA$communality, 2) motFA2 &lt;- fa(Rmot$rho, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) plot(motFA$loadings, asp = 1, xlim = c(-0.2, 0.9), ylim = c(-0.5, 0.9), type = &quot;n&quot;, xlab = &quot;Factor 1&quot;, ylab = &quot;Factor 2&quot;, main = &quot;Loadings Plot&quot;) text(motFA$loadings, labels = rownames(motFA$loadings), cex = 0.8, col = &quot;gray&quot;) abline(h = 0, v = 0, col = &quot;lightgray&quot;, lty = 2) text(motFA2$loadings, labels = rownames(motFA2$loadings), col = 1, cex = 0.8) legend(&quot;bottomleft&quot;, legend = c(&quot;rotated&quot;, &quot;unrotated&quot;), col = c(&quot;black&quot;, &quot;gray&quot;), pch = 19) Rmot2 &lt;- tetrachoric(Rmotivation[,1:36]) motFA3 &lt;- fa(Rmot2$rho, nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) motFA3$loadings round(motFA3$Phi, 3) motFA2 &lt;- fa(Rmotivation1, nfactors = 2, rotate = &quot;varimax&quot;, cor = &quot;tet&quot;, fm = &quot;ml&quot;, scores = &quot;regression&quot;, missing = TRUE, impute = &quot;median&quot;) dim(motFA2$scores) Rdep &lt;- polychoric(Depnum)$rho evals &lt;- eigen(Rdep)$values scree(Rdep, factors = FALSE) (evals/sum(evals)*100)[1:2] set.seed(123) resPA &lt;- fa.parallel(Depnum, fa = &quot;pc&quot;, cor = &quot;poly&quot;, fm = &quot;ml&quot;) resvss &lt;- vss(Rdep, fm = &quot;ml&quot;, n.obs = nrow(Depnum), plot = FALSE) resvss fadep &lt;- fa(Depnum, 1, cor = &quot;poly&quot;, fm = &quot;ml&quot;) summary(fadep) resnf &lt;- nfactors(Depnum, n = 8, fm = &quot;ml&quot;, cor = &quot;poly&quot;) resnf ## ----- Bayesian exploratory factor analysis library(&quot;MPsychoR&quot;) library(&quot;corrplot&quot;) library(&quot;BayesFM&quot;) data(&quot;Privacy&quot;) Privstd &lt;- scale(Privacy) corrplot(cor(Privstd)) Nid &lt;- 2 ## minimum number of variables per factor pmax &lt;- trunc(ncol(Privstd)/Nid) ## maximum number of factors pmax set.seed(123) Rsim &lt;- simul.R.prior(pmax, nu0 = pmax + c(1, 2, 5, 7, 10)) plot(Rsim) Ksim &lt;- simul.nfac.prior(nvar = ncol(Privstd), Nid = Nid, Kmax = pmax, kappa = c(.1, .2, .5, 1)) plot(Ksim) set.seed(222) fitbefa &lt;- befa(Privstd, Nid = 2, Kmax = pmax, nu0 = 10, kappa = 0.2, kappa0 = 0.1, xi0 = 0.1, burnin = 5000, iter = 50000) fitbefa &lt;- post.column.switch(fitbefa) ## column reordering fitbefa &lt;- post.sign.switch(fitbefa) ## sign switching sumbefa &lt;- summary(fitbefa) ## ----- confirmatory factor analysis library(&quot;MPsychoR&quot;) library(&quot;lavaan&quot;) data(&quot;Rmotivation&quot;) vind &lt;- grep(&quot;ext|int&quot;, colnames(Rmotivation)) ## ext/int items Rmot &lt;- na.omit(Rmotivation[, vind]) mot_model &lt;- &#39; extrinsic =~ ext1 + ext2 + ext3 + ext4 + ext5 + ext6 + ext7 + ext8 + ext9 + ext10 + ext11 + ext12 intrinsic =~ int1 + int2 + int3 + int4 + int5&#39; fitMot &lt;- lavaan::cfa(mot_model, data = Rmot, ordered = names(Rmot)) library(&quot;semPlot&quot;) semPaths(fitMot, what = &quot;est&quot;, edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 4.5, asize = 2.5, intercepts = FALSE, rotation = 4, thresholdColor = &quot;red&quot;, mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4) inspect(fitMot, what = &quot;est&quot;)$theta inspect(fitMot, what = &quot;est&quot;)$lambda inspect(fitMot, what = &quot;std&quot;)$lambda inspect(fitMot, what = &quot;est&quot;)$psi inspect(fitMot, what = &quot;std&quot;)$psi parameterEstimates(fitMot, standardized = TRUE) summary(fitMot, standardized = TRUE, fit.measures = TRUE) parameterEstimates(fitMot)[5,] mot_model2 &lt;- &#39; extrinsic =~ ext1 + ext2 + ext3 + ext4 + ext6 + ext7 + ext8 + ext9 + ext10 + ext11 + ext12 intrinsic =~ int1 + int2 + int3 + int4 + int5&#39; fitMot2 &lt;- lavaan::cfa(mot_model2, data = Rmot, ordered = names(Rmot)[-5]) vind &lt;- c(1:4, 13:16, 32:35) Rmot2 &lt;- na.omit(Rmotivation[, vind]) mot_model3 &lt;- &#39; extrinsic =~ ext1 + ext2 + ext3 + ext4 hybrid =~ hyb1 + hyb2 + hyb3 + hyb4 intrinsic =~ int1 + int2 + int3 + int4 motivation =~ extrinsic + hybrid + intrinsic&#39; fitMot3 &lt;- lavaan::cfa(mot_model3, data = Rmot2, ordered = names(Rmot2)) semPaths(fitMot3, what = &quot;std&quot;, edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5, intercepts = FALSE, rotation = 4, thresholdColor = &quot;red&quot;, mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4) summary(fitMot3, standardized = TRUE, fit.measures = TRUE) vind &lt;- c(1:4, 13:16, 32:35, 39:41) Rmot3 &lt;- na.omit(Rmotivation[, vind]) mot_model4 &lt;- &#39; extrinsic =~ ext1 + ext2 + ext3 + ext4 hybrid =~ hyb1 + hyb2 + hyb3 + hyb4 intrinsic =~ int1 + int2 + int3 + int4 motivation =~ extrinsic + hybrid + intrinsic motivation ~ npkgs + phd&#39; fitMot4 &lt;- lavaan::cfa(mot_model4, data = Rmot3, ordered = names(Rmot3[1:12])) semPaths(fitMot4, what = &quot;std&quot;, edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5, intercepts = FALSE, rotation = 4, thresholdColor = &quot;red&quot;, mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4) parameterEstimates(fitMot4)[16:17,] library(&quot;semTools&quot;) data(&quot;Bergh&quot;) GP_model &lt;- &#39;GP =~ EP + HP + DP + SP&#39; minvfit1 &lt;- measEq.syntax(GP_model, data = Bergh, group = &quot;gender&quot;, return.fit = TRUE) minvfit2 &lt;- measEq.syntax(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;), return.fit = TRUE) minvfit3 &lt;- measEq.syntax(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;), return.fit = TRUE) minvfit4 &lt;- measEq.syntax(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;means&quot;), return.fit = TRUE) anova(minvfit1, minvfit2, minvfit3, minvfit4) GP_model &lt;- &#39;GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + SP&#39; fitBase &lt;- lavaan::cfa(GP_model, data = Bergh, group = &quot;gender&quot;, estimator = &quot;MLR&quot;) GP_model &lt;- &#39;GP =~ EP + HP + DP + SP&#39; fitBase &lt;- lavaan::cfa(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;), group.partial = c(&quot;GP=~ SP&quot;), estimator = &quot;MLR&quot;) fitBase1 &lt;- lavaan::cfa(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;), group.partial = c(&quot;GP=~SP&quot;, &quot;DP~1&quot;, &quot;HP~1&quot;, &quot;SP~1&quot;), estimator = &quot;MLR&quot;) GP_model2 &lt;- &#39;GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + c(NA, 0)*SP&#39; fitIO &lt;- lavaan::cfa(GP_model2, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;intercepts&quot;), group.partial = c(&quot;DP~1&quot;, &quot;HP~1&quot;, &quot;SP~1&quot;), estimator = &quot;MLR&quot;) fitMarg &lt;- lavaan::cfa(GP_model, data = Bergh, group = &quot;gender&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;), group.partial = c(&quot;DP~1&quot;, &quot;HP~1&quot;, &quot;SP~1&quot;), estimator = &quot;MLR&quot;) anova(fitMarg, fitBase1) library(&quot;MPsychoR&quot;) library(&quot;lavaan&quot;) data(&quot;SDOwave&quot;) model_sdo1 &lt;- &#39; SDO1996 =~ 1*I1.1996 + a2*I2.1996 + a3*I3.1996 + a4*I4.1996 SDO1998 =~ 1*I1.1998 + a2*I2.1998 + a3*I3.1998 + a4*I4.1998 SDO1996 ~~ SDO1998 ## intercepts I1.1996 ~ int1*1; I1.1998 ~ int1*1 I2.1996 ~ int2*1; I2.1998 ~ int2*1 I3.1996 ~ int3*1; I3.1998 ~ int3*1 I4.1996 ~ int4*1; I4.1998 ~ int4*1 ## residual covariances I1.1996 ~~ I1.1998 I2.1996 ~~ I2.1998 I3.1996 ~~ I3.1998 I4.1996 ~~ I4.1998 ## latent means: 1996 as baseline SDO1996 ~ 0*1 SDO1998 ~ 1&#39; fitsdo1 &lt;- cfa(model_sdo1, data = SDOwave, estimator = &quot;MLR&quot;) parameterEstimates(fitsdo1)[22:23,] model_sdo2 &lt;- &#39; ## 1st CFA level, constant loadings across time SDOD1996 =~ 1*I1.1996 + d1*I2.1996 SDOD1998 =~ 1*I1.1998 + d1*I2.1998 SDOD1999 =~ 1*I1.1999 + d1*I2.1999 SDOE1996 =~ 1*I3.1996 + a1*I4.1996 SDOE1998 =~ 1*I3.1998 + a1*I4.1998 SDOE1999 =~ 1*I3.1999 + a1*I4.1999 ## 2nd CFA level, constant loadings across time SDO1996 =~ 1*SDOD1996 + sd1*SDOE1996 SDO1998 =~ 1*SDOD1998 + sd1*SDOE1998 SDO1999 =~ 1*SDOD1999 + sd1*SDOE1999 ## Constant 1st level intercepts I1.1996 ~ iI1*1; I1.1998 ~ iI1*1; I1.1999 ~ iI1*1 I2.1996 ~ iI2*1; I2.1998 ~ iI2*1; I2.1999 ~ iI2*1 I3.1996 ~ iI3*1; I3.1998 ~ iI3*1; I3.1999 ~ iI3*1 I4.1996 ~ iI4*1; I4.1998 ~ iI4*1; I4.1999 ~ iI4*1 ## residual covariances: I1.1999 ~~ I1.1998; I1.1996 ~~ I1.1998; I1.1999 ~~ I1.1996 I2.1999 ~~ I2.1998; I2.1996 ~~ I2.1998; I2.1999 ~~ I2.1996 I3.1999 ~~ I3.1998; I3.1996 ~~ I3.1998; I3.1999 ~~ I3.1996 I4.1999 ~~ I4.1998; I4.1996 ~~ I4.1998; I4.1999 ~~ I4.1996 ## latent means SDO1996 ~ 0*1 ## 1996 baseline year SDO1998 ~ 1 ## 1998 vs. 1996 SDO1999 ~ 1 ## 1999 vs. 1996 &#39; fitsdo2 &lt;- cfa(model_sdo2, data = SDOwave, estimator = &quot;MLR&quot;) semPaths(fitsdo2, what = &quot;est&quot;, edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 6, asize = 2.5, intercepts = FALSE, rotation = 4, thresholdColor = &quot;red&quot;, mar = c(1, 5, 1.5, 5), fade = FALSE) parameterEstimates(fitsdo2)[43:45,] data(&quot;FamilyIQ&quot;) modelIQ &lt;- &#39; level: 1 numeric =~ wordlist + cards + matrices perception =~ figures + animals + occupation level: 2 general =~ wordlist + cards + matrices + figures + animals + occupation&#39; fitIQ &lt;- cfa(modelIQ, data = FamilyIQ, cluster = &quot;family&quot;, std.lv = TRUE) fitIQ ## ----- bayesian confirmatory factor analysis library(&quot;blavaan&quot;) dpriors()[c(&quot;lambda&quot;, &quot;itheta&quot;, &quot;ipsi&quot;)] library(&quot;MPsychoR&quot;) data(&quot;Bergh&quot;) GP_model &lt;- &#39;GP =~ EP + HP + DP + SP&#39; set.seed(123) fitBCFA &lt;- bcfa(GP_model, data = Bergh, burnin = 2000, sample = 10000, n.chains = 2) plot(fitBCFA, pars = 1:2, plot.type = &quot;trace&quot;) plot(fitBCFA, pars = 1:2, plot.type = &quot;autocorr&quot;) summary(fitBCFA) "],["item-response-theory.html", "Chapter 4 Item Response Theory 4.1 Example one 4.2 Example two", " Chapter 4 Item Response Theory Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],["principal-components-analysis-1.html", "Chapter 5 Principal Components Analysis", " Chapter 5 Principal Components Analysis We have finished a nice book. "],["correspondence-analysis.html", "Chapter 6 Correspondence Analysis", " Chapter 6 Correspondence Analysis "],["gifi-methods.html", "Chapter 7 Gifi Methods", " Chapter 7 Gifi Methods "],["multidimensional-scaling.html", "Chapter 8 Multidimensional Scaling", " Chapter 8 Multidimensional Scaling "],["graphing-multidimensional-data.html", "Chapter 9 Graphing Multidimensional Data", " Chapter 9 Graphing Multidimensional Data "],["networks.html", "Chapter 10 Networks", " Chapter 10 Networks "],["modeling-trajectories-and-time-series.html", "Chapter 11 Modeling Trajectories and Time Series", " Chapter 11 Modeling Trajectories and Time Series "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
