[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "",
    "text": "1 Introduction to the Course\nWelcome! This is a notebook for ERMA 8350 Advanced Measurement Theory. The class will be using the textbook Handbook of Educational Measurement and Psychometrics Using R (Desjardins and Bulut 2018), which will be the primary source for learning to use R for the methods covered in this course. I will use this notebook to make available additional readings to help you learn the theory behind these methods and to provide published examples of their use. It may include some examples from the textbook, with some elaborations, additional readings, and some more details about implementing the methods in R. These web-based notes will make it easy for you to use code, by allowing you to copy and paste code found within. Some of you will have experience with R and others not. So I will try to also point you to additional resources that may be helpful. For example, in this preface I will provide links to resources to help you setup R and RStudio. RStudio is a platform to make using R more productive. I will use it extensively in this course.\nThere are at least two way you can access the software needed for this course. You can use the virtual labs on campus. I know at least the education virtual labs have R and RStudio installed. IF you go this route you can watch the following video. Note you will need Duo setup for this to work.\nUsing Vlab to acces R/RStudio\nA better option if you have a laptop, you can install both programs on your computer. They are both absolutely free and available on all major operating systems, so you will not have to worry about transferring information across computers, limited connection speeds, or other hassles inherent with the VLab route.\nThe following links take you to videos instructing you how to install them.\nInstalling R and RStudio\nOrganizing Projects in RStudio"
  },
  {
    "objectID": "index.html#resources-for-learning-r",
    "href": "index.html#resources-for-learning-r",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "Resources for Learning R",
    "text": "Resources for Learning R\nWhile such experience is certainly helpful, I do not assume you have prior knowledge of using R. I will demonstrate the use of R and provide (particularly in this notebook) the R code needed to use the methods we will learn. However, even if you have prior experience with R, you should plan to spend time learning to program in R. Some people find this intimidating initially, but most of you will grow to find R programming rewarding, and even fun by the end of the course. But, there will be frustration for sure.\nHere are some good places to start learning R:\nCRAN"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "R Packages",
    "text": "R Packages\nR is, among other paradigms, a functional programming language, which means is heavily utilizes functions. R’s functions are stored in packages. While base R has a long list of very useful functions, to fully realize the power of R you will have to use additional packages. So, learning how to install packages (downloading from the web to your computer) and loading packages (making the package’s functions accessible to your current R session) are important skills to master.\n\n\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press."
  },
  {
    "objectID": "intro_R_programming.html",
    "href": "intro_R_programming.html",
    "title": "2  Introduction to R Programming",
    "section": "",
    "text": "Before we can start learning about advanced measurement theory, we need a basic understanding of the software we will use.\nTo understand why I use R, learn how to install it, and for an introduction to its use, go to https://www.statistical-thinking.com and follow the link in the left-hand column called “Introduction to R and RStudio” under the “Statistical Software” section. Read through the materials and watch the linked videos.\n\n\n\n\n\n\nUnder Contruction\n\n\n\nMany sections on the website referenced above are in draft form.\n\n\nWhen you have worked through that section of the website read through chapter 1 of the textbook “Introduction to the R Programming Language”. Make sure you try many of the coding examples within these resources. Don’t worry if you are having trouble with some tasks. The important thing is to come to the next class with good questions."
  },
  {
    "objectID": "ctt.html#measurement-in-science",
    "href": "ctt.html#measurement-in-science",
    "title": "3  Classic Test Theory",
    "section": "3.1 Measurement in Science",
    "text": "3.1 Measurement in Science\nMeasurement is the quantification of theoretical constructs by means of assigning labels or numbers to observation, in a systematic way. This is one way in which we simplify reality as a means to better understand it. The vast majority, maybe all, of the constructs we want to learn about are not directly measurable. When we make measurement, we inevitably must leave out some information about what we are observing, hence simplifying it.\nA very important issue in measurement is validity. Validity generally is the extent to which our measures reflect what we are attempting to measure, in a particular context. A similar concept is reliability, which deals with the consistency of our measures in a given context. If we were to take the same measurement of the same thing in the same context, we would expect to get the same measurement. To the extent that this is true, the measure is reliable. Note that to be valid an instrument must be reliable, but just because an instrument is reliable does not mean it is valid.\nThe concept of invariance relates to the extent to which scores on a measure are independent of examinee characteristics not relevant to the construct attempting to be measured. These characteristics can include things like gender, ethnicity, cultural background etc.\n\n3.1.1 Scales of Measurement\nThey way we quantify or classify constructs to generate measures can be classified by the scheme in the following figure.\n\n\n\n\nflowchart TB\n  A[Scales of Measurement] --> B[Qualitative/Categorical]\n  A --> C[Quantitative/Numeric]\n  B --> D[Nominal]\n  B --> E[Ordinal]\n  C --> F[Interval]\n  C --> G[Ratio]\n\n\n\n\n\n\n\n\nTo understand the scales of measurement, let’s use a data example. First, we will import a small data set of questions I ask students in some of my statistics classes.\n\n# File location on github:\nfile_location <- \"https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/data/student_survey.csv\"\n\n# Import data from csv file:\nstudent_survey <- read.csv(file = file_location,\n                           header = TRUE)\n# View raw data\nstudent_survey\n\n\n\n\n\n  \n\n\n\nYou can look at the codebook for this data below in section 3.5.1. We can see that all of the variables are coded as integers (see the <int> under the variable name in the data frame), with the exception of gender with is a character string (<chr>). But if you look over the variables, and read the variable descriptions in the codebook, you may realize that some of the variables are not best considered numeric. We will need to recode those variables. While we do that we will discuss scales of measurement.\nFor example, we can see that the sem variable quantifies the current semester (Spring, Fall, or Summer) for the student taking the survey. Here we table this variable\n\ntable(student_survey$sem)\n\n\n 1  2 \n 1 54 \n\n\nSemester is clearly either a nominal or ordinal scale of measurement. It could be ordinal because, in a given calendar year spring comes before summer which comes before fall. But for our purposes of this survey the ordering is not important, so we will ignore it for now and create a nominal variable. In R we do this with the factor() function as follows:\n\nstudent_survey$sem <- factor(student_survey$sem,\n                             levels = c(1:3),\n                             labels = c(\"Spring\", \"Fall\", \"Summer\"))\n\nThis code tells R to create an object in side of student_survey called sem. Because this object already exists here, this code will replace the existing object with the new one. Then the factor() function take an object as the first argument (student_survey$sem) which is the old object. So, essentially we are going take the old object turn it into a factor and replace the old object with the newly created factor. The next argument levels = c(1:3) tells R that the values of the original object are the integers 1, 2, and 3. Then, the labels = c(\"Spring\", \"Fall\", \"Summer\") argument maps the three character strings (“Spring”, “Fall”, “Summer”) onto the integers 1, 2, and 3. The ordering of the two vectors (1, 2, and 3 on the one hand and “Spring”, “Fall”, “Summer” on the other are important. “Spring” is mapped onto 1, “Fall” onto 2, and “Summer” onto 3. After doing this we can table this variable again and see what happened.\n\ntable(student_survey$sem)\n\n\nSpring   Fall Summer \n     1     54      0 \n\n\nWe can do something similar with the hand variable, which the codebook states captures the student’s handedness, and also is a nominal variable. But this time instead of saving the new variable over the old, I will create a new variable I will call handedness.\n\nstudent_survey$handedness <- factor(student_survey$hand, \n                                    levels = c(1,2),\n                                    labels = c(\"left\", \"right\"))\n\nThe major difference here is on the left side of the assignment operator (<-). Instead of using the same name of the original object hand, I gave it a new name handedness. Also note that in the levels argument, instead of the 1:2 shortcut I used c(1,2), does the same thing.\n\ntable(student_survey$handedness)\n\n\n left right \n    4    51 \n\n\nWe have two more nominal variables gender and course. Next, let’s recode gender. Because this variable contains character strings, which we can use as the labels, the code is simpler, we do not have to pass the levels or labels arguments.\n\nstudent_survey$gender <- factor(student_survey$gender)\n\n\ntable(student_survey$gender)\n\n\nfemale   male \n    16     39 \n\n\nOur final nominal variable is course, which measures which course the student taking the survey was enrolled. Because the labels are a bit more cumbersome, and to keep the code readable, we will first create a vector of the labels called lbls. Then we can use that vector in the factor() function. When we are done with the lbls object we will remove it with the rm() function. Finally, we will table the new variable.\n\n# Create temporary labels for course factor.\nlbls <- c(\"ERMA 7200 Basic Methods in Education Research\",\n          \"ERMA 7300 Design and Analysis I\",\n          \"ERMA 7310 Design and Analysis II\",\n          \"ERMA 8340 Advanced Psychometrics\")\nstudent_survey$course <- factor(student_survey$course, \n                                levels = c(1,2,3,4),\n                                labels = lbls)\nrm(lbls) # Remove labels object\n\ntable(student_survey$course)\n\n\nERMA 7200 Basic Methods in Education Research \n                                            0 \n              ERMA 7300 Design and Analysis I \n                                           55 \n             ERMA 7310 Design and Analysis II \n                                            0 \n             ERMA 8340 Advanced Psychometrics \n                                            0 \n\n\nOrdinal Variables are those that have a natural order but the interval between those variables is not necessary the same across the different values. It the student survey data an example is birth which measured the birth order of students.\n\nstudent_survey$birth <- ordered(student_survey$birth)\n\ntable(student_survey$birth)\n\n\n 1  2  3  4  5  6 \n25 10  8  5  1  1 \n\n\nThe last 20 variables of the student survey are question that ask about research and statistics. These are also measured as integers but should be ordinal variables. Creating a vector of labels as we did with the course variable, is also useful when you need to recode several variables with the same labels, such as in a set of variables that use the same Likert scale, as is the case for the Research and Statistics questions in the student survey. Below, we again create a object called lbls with the Likert labels. Then we create a vector of the column numbers that contain the Likert items, which are the 15th through the 31st columns, and name it cols. In R the square brackets are indexing functions and it allows us to use only a subset of the columns in the data frame. Then we use the lapply to repeat the factor() function for each of the Likert columns.\n\n# Likert labels\nlbls <- c(\"strongly disagree\", \"disagree\", \"neither agree/disagree\", \n          \"agree\", \"strongly agree\")\n\n# Column numbers containing Likert variables.\ncols <- 12:31\n\n# Use indexing to transform all Likert items to ordered factor.\nstudent_survey[ ,cols] <- lapply(student_survey[ ,cols], \n                               function(x) factor(x, \n                                                  levels = c(1,2,3,4,5),\n                                                  labels = lbls, \n                                                  ordered = TRUE))\n\nNote that to make a function ordered, which is the way to create ordinal variables in R, you pass the value TRUE to the ordered function. It will use the order of levels to order the values.\nHere is the new dataframe\n\nstudent_survey"
  },
  {
    "objectID": "ctt.html#classical-true-score-model",
    "href": "ctt.html#classical-true-score-model",
    "title": "3  Classic Test Theory",
    "section": "3.2 Classical True Score Model",
    "text": "3.2 Classical True Score Model\nThe true score model is: \\[\nX = T + E \\tag{1}\n\\] where \\(X\\) is the observed score, \\(T\\) is the true score, which is unknown, and \\(E\\) is the error\nFour assumptions to the model above:\n\n\\(E(X) = T\\), the expected value of the observed score \\(X\\) is the true score \\(T\\).\n\\(Cov(T,E) = 0\\), the true score ane error are independent(not correlated)\n\\(Cov(E_1, E)2 = 0\\), errors across test forms are independent.\n\\(Cov(E_1, T_2) = 0\\), error on one form of test is independent of the true score on another form.\n\nWhich leads to a re-expression of equation (1) above:\n\\[\n\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\n\\]\nTo demonstrate this let’s assume we have the following data 1 , which was generated to meet these assumptions.\n\n# Filepath to data on github. \nfilepath <- \"https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/code/generateToy_CTTdata.R\"\nsource(filepath)\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau\n1   1    1  3  6  5  3  5  5  4  5  3   3   4\n2   1    2  6  3  5  3  4  2  4  4  3   5   4\n3   1    3  4  4  2  4  4  3  5  3  4   5   4\n4   2    1  3  6  8  6  5  4  5  5  5   5   5\n5   2    2  6  4  6  6  4  6  6  5  4   4   5\n6   2    3  4  6  6  5  5  5  1  3  6   4   5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6\n8   3    2  6  6  6  7  5  6  6  6  6   7   6\n9   3    3  6  5  8  6  6  6  7  7  5   7   6\n10  4    1  4  3  5  4  2  3  3  5  5   2   4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4\n12  4    3  2  4  4  4  6  4  3  4  5   4   4\n13  5    1  5  6  5  4  5  5  5  6  5   6   5\n14  5    2  6  6  4  6  4  5  4  5  5   5   5\n15  5    3  6  4  5  4  5  5  4  4  5   5   5\n16  6    1  6  6  7  8  6  6  7  6  6   4   6\n17  6    2  4  5  7  5  5  7  4  5  6   7   6\n18  6    3  5  6  6  6  4  5  4  5  7   6   6\n\n\nwhere id is a variable indicating individual test-takers, time indicated which of 3 times each individual was assessed, x1 - x10 are the scores on 10 items that comprise the test, and Tau is the true value of the individuals ability. I use Tau here instead of T, because T is a protected symbol in R which is short-hand for TRUE. Note that we would not know Tau in most situations, but because this is simulated data we will pretend we do.\nWe can create a composite score for the ten items for each individual on each occasion by averaging columns 3 through 12.\n\nCTTdata$X <- rowMeans(CTTdata[ ,3:12])\n\nAnd we can also create E, the error with:\n\nCTTdata$E <- CTTdata$X - CTTdata$Tau\n\nAgain, in practice we would not be able to directly compute E because we would not know Tau, but we will use it to build an understanding of what error is.\nNow we have:\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6\n\n\nLook over the last three columns and make sure you understand their relation. For example, in the first row, note that X is .2 points above Tau, which is exactly the value of E we computed (\\(X_1 - T_1 = E_1 = 4.2 - 4 = .2\\)). The 1 subscript in the previous expression indicated row 1 (i.e. i = 1).\n\nCTTdata$X_t <- round(ave(CTTdata$X, CTTdata$id, FUN = mean),1)\n\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E X_t\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2 4.0\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1 4.0\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2 4.0\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2 4.9\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1 4.9\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5 4.9\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1 6.2\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1 6.2\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3 6.2\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4 4.0\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3 4.0\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0 4.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2 5.0\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0 5.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3 5.0\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2 5.7\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5 5.7\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6 5.7"
  },
  {
    "objectID": "ctt.html#reliability",
    "href": "ctt.html#reliability",
    "title": "3  Classic Test Theory",
    "section": "3.3 Reliability",
    "text": "3.3 Reliability\n\\[\n\\text{reliability} = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E} = \\rho^2_{XT}\n\\]\nThe reliability is the proportion of variance of \\(T\\) in \\(X\\), which is also the squared correlation between \\(X\\) and \\(T\\).\n\nTau <- CTTdata$Tau\nX <- CTTdata$X\nE <- CTTdata$X - CTTdata$Tau\n\n\nvar(Tau)/var(X)\n\n[1] 0.9170806\n\n\n\nvar(Tau)/(var(Tau) + var(E))\n\n[1] 0.8898776\n\n\n\ncor(Tau, X)^2\n\n[1] 0.886766\n\n\n\nlibrary(hemp)\nsplit_half(CTTdata, type = \"alternate\")\n\n[1] 0.887\n\n\n\ncoef_alpha(CTTdata)\n\n[1] 0.894\n\n\n\nplot(x = CTTdata$Tau, y = CTTdata$id, xlim = c(1,10),\n     ylim = c(0,7))\npoints(x = CTTdata$X, y = jitter(CTTdata$id), pch = 3, col = \"red\")\npoints(x = ave(x = CTTdata$X, factor(CTTdata$id), FUN = mean), y = CTTdata$id, \n       col = \"blue\", pch = 18)\n\n points(x = CTTdata$X_t, pch = 2, factor(CTTdata$id))"
  },
  {
    "objectID": "ctt.html#sapa-example",
    "href": "ctt.html#sapa-example",
    "title": "3  Classic Test Theory",
    "section": "3.4 SAPA Example",
    "text": "3.4 SAPA Example\nIn this section I wil use data from the hemp package.\n\nlibrary(hemp)\ndata(\"SAPA\")\n\nTake a few minutes to look at the data description.\n\n?SAPA\n\nYou can explore individual items as follows:\n\nprop.table(table(SAPA$reason.4))\n\n\n        0         1 \n0.3598162 0.6401838 \n\nbarplot(prop.table(table(SAPA$reason.4)))\n\n\n\n\nYou can look at the proportion correct for all items.\n\n# Proportion correct for each item:\ncbind(proportion_correct = colMeans(SAPA, na.rm = TRUE))\n\n          proportion_correct\nreason.4           0.6401838\nreason.16          0.6981627\nreason.17          0.6973079\nreason.19          0.6152331\nletter.7           0.5997375\nletter.33          0.5712410\nletter.34          0.6132633\nletter.58          0.4439344\nmatrix.45          0.5259357\nmatrix.46          0.5498688\nmatrix.47          0.6139199\nmatrix.55          0.3740157\nrotate.3           0.1936967\nrotate.4           0.2127380\nrotate.6           0.2994091\nrotate.8           0.1850394\n\n\n\nnum_miss(SAPA)\n\n          num_miss perc_miss\nreason.4         2      0.13\nreason.16        1      0.07\nreason.17        2      0.13\nreason.19        2      0.13\nletter.7         1      0.07\nletter.33        2      0.13\nletter.34        2      0.13\nletter.58        0      0.00\nmatrix.45        2      0.13\nmatrix.46        1      0.07\nmatrix.47        2      0.13\nmatrix.55        1      0.07\nrotate.3         2      0.13\nrotate.4         2      0.13\nrotate.6         2      0.13\nrotate.8         1      0.07\n\n\n\n3.4.1 Reliability\n\nsplit_half(SAPA, type = \"alternate\")\n\n[1] 0.758\n\n\nThe split-half reliability coefficient is known to be downwardly biased. The Spearman-Brown formula can adjust for this. To get the Spearman-Brown reliability estimate use the following.\n\nsplit_half(SAPA, sb = TRUE)\n\n[1] 0.8623436\n\n\nWe might wish to estimate what length of test is needed to achieve a particular reliability.\n\ntest_length(SAPA, r = .95, r_type = \"split\")\n\n[1] 49\n\n\n\n3.4.1.1 Cronbach’s \\(\\alpha\\)\n\ncoef_alpha(SAPA)\n\n[1] 0.841\n\n\n\npsych::alpha(SAPA)\n\n\nReliability analysis   \nCall: psych::alpha(x = SAPA)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.84      0.84    0.85      0.25 5.3 0.006 0.49 0.25     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.83  0.84  0.85\nDuhachek  0.83  0.84  0.85\n\n Reliability if an item is dropped:\n          raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nreason.4       0.83      0.83    0.83      0.25 4.9   0.0064 0.0056  0.22\nreason.16      0.83      0.83    0.84      0.25 5.0   0.0063 0.0057  0.23\nreason.17      0.83      0.83    0.83      0.25 4.9   0.0064 0.0054  0.22\nreason.19      0.83      0.83    0.84      0.25 5.0   0.0063 0.0057  0.23\nletter.7       0.83      0.83    0.83      0.25 4.9   0.0064 0.0053  0.22\nletter.33      0.83      0.83    0.84      0.25 5.0   0.0063 0.0055  0.23\nletter.34      0.83      0.83    0.83      0.25 4.9   0.0064 0.0052  0.22\nletter.58      0.83      0.83    0.84      0.25 4.9   0.0064 0.0058  0.22\nmatrix.45      0.83      0.84    0.84      0.25 5.1   0.0062 0.0057  0.24\nmatrix.46      0.83      0.84    0.84      0.25 5.1   0.0062 0.0056  0.23\nmatrix.47      0.83      0.83    0.84      0.25 5.0   0.0063 0.0059  0.22\nmatrix.55      0.84      0.84    0.84      0.26 5.2   0.0061 0.0054  0.25\nrotate.3       0.83      0.83    0.83      0.25 5.0   0.0062 0.0042  0.24\nrotate.4       0.83      0.83    0.83      0.25 4.9   0.0063 0.0044  0.23\nrotate.6       0.83      0.83    0.83      0.25 4.9   0.0063 0.0049  0.23\nrotate.8       0.83      0.84    0.84      0.25 5.1   0.0062 0.0044  0.24\n\n Item statistics \n             n raw.r std.r r.cor r.drop mean   sd\nreason.4  1523  0.59  0.58  0.54   0.50 0.64 0.48\nreason.16 1524  0.53  0.53  0.48   0.45 0.70 0.46\nreason.17 1523  0.59  0.58  0.55   0.50 0.70 0.46\nreason.19 1523  0.56  0.55  0.51   0.47 0.62 0.49\nletter.7  1524  0.58  0.58  0.54   0.50 0.60 0.49\nletter.33 1523  0.56  0.55  0.50   0.46 0.57 0.50\nletter.34 1523  0.59  0.59  0.55   0.51 0.61 0.49\nletter.58 1525  0.58  0.57  0.53   0.49 0.44 0.50\nmatrix.45 1523  0.51  0.50  0.44   0.41 0.53 0.50\nmatrix.46 1524  0.52  0.50  0.45   0.42 0.55 0.50\nmatrix.47 1523  0.55  0.54  0.49   0.46 0.61 0.49\nmatrix.55 1524  0.45  0.44  0.37   0.34 0.37 0.48\nrotate.3  1523  0.51  0.53  0.50   0.43 0.19 0.40\nrotate.4  1523  0.56  0.58  0.55   0.48 0.21 0.41\nrotate.6  1523  0.55  0.57  0.53   0.47 0.30 0.46\nrotate.8  1524  0.48  0.51  0.46   0.40 0.19 0.39\n\nNon missing response frequency for each item\n             0    1 miss\nreason.4  0.36 0.64    0\nreason.16 0.30 0.70    0\nreason.17 0.30 0.70    0\nreason.19 0.38 0.62    0\nletter.7  0.40 0.60    0\nletter.33 0.43 0.57    0\nletter.34 0.39 0.61    0\nletter.58 0.56 0.44    0\nmatrix.45 0.47 0.53    0\nmatrix.46 0.45 0.55    0\nmatrix.47 0.39 0.61    0\nmatrix.55 0.63 0.37    0\nrotate.3  0.81 0.19    0\nrotate.4  0.79 0.21    0\nrotate.6  0.70 0.30    0\nrotate.8  0.81 0.19    0\n\n\nTo get bootstraped confidence intervals for the hemp coefficient alpha.\n\nlibrary(boot)\n\nalpha_fun <- function(data, row){\n  coef_alpha(data[row, ])\n}\n\nalpha_boot <- boot(data = SAPA, statistic = alpha_fun, \n                   R = 1e4)\nalpha_boot\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = SAPA, statistic = alpha_fun, R = 10000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1*    0.841 -0.0004028 0.005489842\n\n\n\nplot(alpha_boot)\n\n\n\n\n\nboot.ci(alpha_boot, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = alpha_boot, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.829,  0.851 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n3.4.2 Validity\nValidity is more complicated to estimate than reliability. One useful from of validity is criterion related validity which is assessed by looking at how closely aligned scores on the scale you are evaluating are with some other established measure of this construct.\n\ndata(\"interest\")\n\nprint(cor(interest[ ,c(\"vocab\", \"reading\", \"sentcomp\")]), digits= 2)\n\n         vocab reading sentcomp\nvocab     1.00    0.80     0.81\nreading   0.80    1.00     0.73\nsentcomp  0.81    0.73     1.00\n\n\n\n\n3.4.3 Item Analysis\nItem difficulty is an estimate of how hard a particular item is. A fairly straight-forward way to assess item difficulty is looking at the proportion of participants who answered each item correctly. If our items are score 0 or 1 for incorrect and correct answers respectively, we can calculate the column (item) means to get the proportion correct.\n\nitem_difficulty <- colMeans(SAPA, na.rm = TRUE)\nround(cbind(item_difficulty), digits = 2)\n\n          item_difficulty\nreason.4             0.64\nreason.16            0.70\nreason.17            0.70\nreason.19            0.62\nletter.7             0.60\nletter.33            0.57\nletter.34            0.61\nletter.58            0.44\nmatrix.45            0.53\nmatrix.46            0.55\nmatrix.47            0.61\nmatrix.55            0.37\nrotate.3             0.19\nrotate.4             0.21\nrotate.6             0.30\nrotate.8             0.19\n\n\nNote that a more intuitive name for this estimate would be item easiness, as the higher the number, the easier the item is. But we use item difficulty for historical reasons.\nWe can aslo calculate the item discrimination, which is a measure of how well an item discriminates between participants with high ability vs. those with low ability. The most common way to do this is to calcualte the point-biserial correlation between a participants score on an item and their total score.\n\ntotal_score <- rowSums(SAPA, na.rm = TRUE)\nitem_discrimination <- cor(SAPA,\n                           total_score,\n                           use = \"pairwise.complete.obs\")\n\nitem_discrimination\n\n               [,1]\nreason.4  0.5875787\nreason.16 0.5326660\nreason.17 0.5859068\nreason.19 0.5582773\nletter.7  0.5835910\nletter.33 0.5569431\nletter.34 0.5946924\nletter.58 0.5750172\nmatrix.45 0.5095047\nmatrix.46 0.5138256\nmatrix.47 0.5478686\nmatrix.55 0.4468619\nrotate.3  0.5100778\nrotate.4  0.5559848\nrotate.6  0.5542336\nrotate.8  0.4807175\n\n\nhigher values (closer to 1.00) mean the item has good discrimination, while values close to zero suggest little or not relation, and high negative numbers, suggest that people who do well on the rest of the instrument tend to do poorly in this item. This last situation often suggests something unintended is going on with the item or, said differently, the item is not “behaving” well.\nAnother way to calculate discrimination of items is to calculate the item discrimination index which splits the test takers into a high and low group based on their total score and then correlate this grouping variable with each item response.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .27)\n\nUpper 27% Lower 27% \n 0.805136  0.194864 \n\n\n\niri(SAPA)\n\n               [,1]\nreason.4  0.2820989\nreason.16 0.2451971\nreason.17 0.2692675\nreason.19 0.2717135\nletter.7  0.2865325\nletter.33 0.2757209\nletter.34 0.2897118\nletter.58 0.2863221\nmatrix.45 0.2544930\nmatrix.46 0.2562540\nmatrix.47 0.2668171\nmatrix.55 0.2161230\nrotate.3  0.2016459\nrotate.4  0.2276081\nrotate.6  0.2539219\nrotate.8  0.1867207\n\n\nFinally, for multiple choice tests, you may also want look at the distractors, which are the incorrect answers to such a question. This is done by looking at barplots or the proportion of test takers that answer each choice. If there are wrong choices that many test takers select, you may want to reconsider the distractors. There may be something confusing about the choices. If you have a popular distractor and a low discrimination measure, changing the distractor may help."
  },
  {
    "objectID": "ctt.html#r-scripts-and-data",
    "href": "ctt.html#r-scripts-and-data",
    "title": "3  Classic Test Theory",
    "section": "3.5 R Scripts and Data",
    "text": "3.5 R Scripts and Data\n\n3.5.1 Student Survey Data Codebook\n\n\nDocumentation for Student Survey data\n\nThis is a real data set from a survey given to graduate students in a statistics class.\n\nVariables\n\nDuration.inseconds. - length in seconds to complete survey.\n\ncourse  - Which course are you completing this survey for?\n          1 = ERMA 7200 Basic Methods in Education Research\n          2 = ERMA 7300 Design and Analysis I\n          3 = ERMA 7300 Design and Analysis II\n          4 = ERMA 8340 Advanced Psychometrics\n\nsem - Which is the current semester? 1 = Spring, 2 = Fall, 3 = Summer\n\nyear - What is the current year (enter 4 numerals. For example 2019)?\n\nexer - How many minutes do you exercise in a typical week?\n\nsoda - How much soda (in ounces) have you consumed in the last 24 hours?\n\ntvmin - How many minutes of television do you watch in a typical day?\n\nsiblings - How many siblings do you have?\n \nbirth - What is your birth order among you and your siblings?\n\ngender - what is your gender? 1 = male, 2 = female\n\nhand - Which is your dominant hand? 1 = left, 2 = right, 3 neither/both\n\nnumTVS - How many TVs do you own?\n\nResearch and Statistics Questions\n          1 = Strongly agree, 2 = Disagree, 3 = Neither agree/disagree, \n          4 = Agree, 5 = Strongly agree\n          \nstats_1 - Involvement in research will enhance my job/career opportunities.\n\nstats_2 - People I respect would approve of my involvement in research.       \n\nstats_3 - Research involvement will allow me to contribute to practitioners’ knowledge base.\n\nstats_4 - Doing research will increase my sense of self-worth.\n\nstats_5 - Becoming involved research will lead to the kind of career I most want.\n\nstats_6 - Research involvement is valued by significant people in my life.\n\nstats_7 - My peers think highly of me if I become involved in research.\n\nstats_8 - Research involvement will enable me to associate with the kind of people I value most.\n\nstats_9 - Involvement on a research team leads to close personal connections.\n\nstats_10 - Research involvement leads to a sense of satisfaction.\n\nstats_11 - Being involved in research contributes to my development as a professional.\n\nstats_12 - I believe research skills will be fruitful for my career.\n\nstats_13 - My involvement in research will lead to meaningful contributions to the field.\n\nstats _14 - Involvement in research will take time away from my significant relationships.\n\nstats_15 - Involvement in research takes time from leisure activities.\n\nstats_16 - Involvement in research helps me to understand the current issues in my profession.\n\nstats _17 - My analytical skills will become more developed because of my involvement in research activities.\n\nstats_18 - I believe that research involvement will lead to becoming well-known and respected in the field.\n\nstats_19 - Research involvement will lead to increased financial opportunities.\n\nstats_20 - Involvement in research will positively influence my applied skills.\n\n\n\n\n3.5.2 Simulating CTT data\n\n#------------------------------------------------------------------------\n# Title: simulate_CTTdata\n# Author: William Murrah\n# Description: Simulate data to demonstrate CTT and reliability\n# Created: Monday, 09 August 2021\n# R version: R version 4.1.0 (2021-05-18)\n# Project(working) directory: /Users/wmm0017/Projects/Courses/\n#   AdvancedMeasurementTheoryNotebook\n#------------------------------------------------------------------------\n\nsimx <- function(truescore, sigmax = 1) {\n  x <- rnorm(18, truescore, sigmax)\n  return(round(x))\n}\nid <- rep(1:6, each = 3)\nTau <- rep(rep(4:6, each = 3),2)\nset.seed(20210805)\nCTTdata <- data.frame(\n  id = id,\n  time = rep(1:3, 6),\n  x1 = simx(Tau),\n  x2 = simx(Tau),\n  x3 = simx(Tau),\n  x4 = simx(Tau),\n  x5 = simx(Tau),\n  x6 = simx(Tau),\n  x7 = simx(Tau),\n  x8 = simx(Tau),\n  x9 = simx(Tau),\n  x10 = simx(Tau),\n  Tau = Tau\n)\nrm(id, Tau, simx)"
  },
  {
    "objectID": "generalizability.html#one-facet-design",
    "href": "generalizability.html#one-facet-design",
    "title": "4  Generalizability Theory",
    "section": "4.1 One-Facet Design",
    "text": "4.1 One-Facet Design\nTo start with we will consider a simple design taken from chapter 3 of Desjardins and Bulut (2018), in which 30 participants were administered an executive functioning (EF) instrument, consisting of 10 dichotomously scored items (scored 0, or 1). In addition to the variation across participants, this design has one facet, which is item. In all there are three sources of variance in EF scores, one source due to participant (\\(\\sigma^2_p\\)), one source due to items (\\(\\sigma^2_i\\)) and one source the interaction of the two, which is also confounded with the residual of the model (\\(\\sigma^2_{pi,e}\\)). These are represented in ?eq-efvar, and graphically displayed in Figure 4.2.\n\\[\n\\sigma^2(X_{pi})= \\sigma^2_p + \\sigma^2_i + \\sigma^2_{pi,e}\n\\tag{4.5}\\]\n\n\n\n\n\nFigure 4.2: Venn diagram of one-facet G theory model.\n\n\n\n\nFirst, load the hypothetical data for this example from the hemp package.\n\nlibrary(hemp)\nlibrary(psych) # for descriptives and the headTail function\ndata(\"efData\") # from hemp package\n\nThee we can look at the data as follows:\n\nstr(efData)\n\n'data.frame':   350 obs. of  3 variables:\n $ Items       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Score       : int  0 1 1 1 1 1 1 1 1 1 ...\n $ Participants: int  1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"reshapeLong\")=List of 4\n  ..$ varying:List of 1\n  .. ..$ score: chr [1:10] \"item.1\" \"item.2\" \"item.3\" \"item.4\" ...\n  .. ..- attr(*, \"v.names\")= chr \"score\"\n  .. ..- attr(*, \"times\")= int [1:10] 1 2 3 4 5 6 7 8 9 10\n  ..$ v.names: chr \"score\"\n  ..$ idvar  : chr \"participant\"\n  ..$ timevar: chr \"item\"\n\nheadTail(efData)\n\n      Items Score Participants\n1.1       1     0            1\n2.1       1     1            2\n3.1       1     1            3\n4.1       1     1            4\n...     ...   ...          ...\n32.10    10     0           32\n33.10    10     1           33\n34.10    10     0           34\n35.10    10     0           35\n\n\nWe can see that this is a balanced design by tallying the number of items per participant.\n\naggregate(Items ~ Participants , data = efData, length)\n\n   Participants Items\n1             1    10\n2             2    10\n3             3    10\n4             4    10\n5             5    10\n6             6    10\n7             7    10\n8             8    10\n9             9    10\n10           10    10\n11           11    10\n12           12    10\n13           13    10\n14           14    10\n15           15    10\n16           16    10\n17           17    10\n18           18    10\n19           19    10\n20           20    10\n21           21    10\n22           22    10\n23           23    10\n24           24    10\n25           25    10\n26           26    10\n27           27    10\n28           28    10\n29           29    10\n30           30    10\n31           31    10\n32           32    10\n33           33    10\n34           34    10\n35           35    10\n\n\n\n4.1.1 G Study\nA G study can be conducted to estimate the three variance components. This is done using the lme4 package in R, which is automatically loaded when you load the hemp package (but could be loaded otherwise with library(lme4)). This package was developed to estimate linear mixed effects models. You can learn more about this package type ?lme4 in the R console and to learn more about the function used below type ?lmer. Briefly, the lmer function allows inclusion of fixed effects, like those included in standard regression models and most ANOVA models, along with random effects as done here. The random effects are each contained in a set of parentheses in the formula argument (e.g. (1 | Participant) and (1 | Items) below). We estimate a model and name it one_facet_model which estimates the scores on the EF instrument with two random effects, one for participant and one for items. Then we perform a G study with the gstudy() function from the hemp package and loo at the estimates of the three variance components.\n\none_facet_model <- lmer(Score ~ (1 | Participants) + (1 | Items), \n                        data = efData)\n\none_facet_gstudy <- gstudy(one_facet_model)\none_facet_gstudy\n\n        Source Est.Variance Percent.Variance\n1 Participants       0.0258             9.9%\n2        Items       0.0959            36.8%\n3     Residual       0.1387            53.3%\n\n\nA nice feature of the gstudy() function is the proportion of variances given in the output. We see that about 37% of the variance in EF scores are estimated to be due to the items, while only about 10% is due to differences across participants. These variances represent the magnitude of the error in generalizing from a participants score on a specific item of the EF instrument to that participants universe score. These components do not tell us the variance in generalizing based on an instrument with 10 items. To get that estimate we can do a D study and set the test length to 10 (see below). Note that the large residual error of 53% represents both the participant by item interaction and random error. We are unable to distinguish between these two sources of error.\n\nparticipant_means <- aggregate(Score ~ Participants, data = efData, mean)\ncolnames(participant_means) <- c(\"Participant\", \"Mean\")\nhist(participant_means$Mean, xlab = \"Proportion of Items Correct\",\n     main = \"\", breaks= 7)\n\n\n\nlattice::histogram(participant_means$Mean, type = \"count\", \n                   xlab = \"Proportion of Items Correct\")\n\n\n\n\n\nitem_means <- aggregate(Score ~ Items, efData, mean)\ncolnames(item_means) <- c(\"Item\", \"Mean\")\nitem_means\n\n   Item       Mean\n1     1 0.94285714\n2     2 0.68571429\n3     3 0.68571429\n4     4 0.08571429\n5     5 0.74285714\n6     6 0.77142857\n7     7 0.08571429\n8     8 0.60000000\n9     9 0.45714286\n10   10 0.11428571\n\n\n\n\n4.1.2 D Study\nOne purpose of a D study is to explore how manipulating the facets might impact the reliability of the instrument as a whole in the relevant context. For example, we mentioned above that the G study does not give us information about our instrument as a whole. If we want to estimate the reliability of this instrument with 10 items, which is what was done in this hypothetical study we could do the following:\n\none_facet_dstudy <- dstudy(one_facet_gstudy, unit = \"Participants\", n = c(\"Items\" = 10))\none_facet_dstudy\n\n        Source Est.Variance   N Ratio of Var:N\n1 Participants       0.0258 350        0.02580\n2        Items       0.0959  10        0.00959\n3     Residual       0.1387  10        0.01387\n\nThe generalizability coefficient is: 0.6503655.\nThe dependability coefficient is: 0.5237515.\n\n\nThe output of the D study contains three types of information. First, the estimated variances of each source of variance (e.g. Participant, Item, and Residual) are given. Second, the generalizability coefficient, which is analogous to the reliability coefficient in CTT is given. Here is is estimated at .65, which is fairly low.\nWe might also want to determine what impact the number of items has on the reliability of the instrument. That way we can estimate the impact of adding more items.\n\ndstudy_plot(one_facet_gstudy, unit = \"Participants\", \n            facets = list(Items = c(10, 20, 30, 40, 50, 60)),\n            g_coef = FALSE)"
  },
  {
    "objectID": "generalizability.html#two-facet-crossed-design",
    "href": "generalizability.html#two-facet-crossed-design",
    "title": "4  Generalizability Theory",
    "section": "4.2 Two-Facet Crossed Design",
    "text": "4.2 Two-Facet Crossed Design"
  },
  {
    "objectID": "generalizability.html#additional-readings",
    "href": "generalizability.html#additional-readings",
    "title": "4  Generalizability Theory",
    "section": "4.3 Additional Readings",
    "text": "4.3 Additional Readings\nFor more information of G theory, see Raykov and Marcoulides (2011). For an example using the R package lavaan with G theory, see Jorgensen (2021).\n\n\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical Test Theory.” Applied Measurement in Education 24 (1): 1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press.\n\n\nJorgensen, Terrence D. 2021. “How to Estimate Absolute-Error Components in Structural Equation Models of Generalizability Theory.” Psych 3 (2): 113–33. https://doi.org/10.3390/psych3020011.\n\n\nRaykov, Tenko, and George A Marcoulides. 2011. Introduction to Psychometric Theory. Routledge."
  },
  {
    "objectID": "efa.html#the-common-factor-model",
    "href": "efa.html#the-common-factor-model",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.1 The Common Factor Model",
    "text": "5.1 The Common Factor Model\nIn the common factor model, variances can be partitioned into common variances and unique variances. Common variances are those due to the common factor, while unique variances include all sources of variance not attributable to the common factor. Unique variance can be decomposed into specific and error variances. Specific variance is due to systematic sources of variability that are unique to the observed indicator. Error variance is due to random measurement variance.\n\\[\nx = \\tau + \\varepsilon\n\\tag{5.1}\\]\n\\[\n\\sigma^2_x = \\sigma^2_T + \\sigma^2_{\\varepsilon}\n\\tag{5.2}\\]\nwhich we can directly compare to the common factor model we will learn about in this chapter:\n\\[\nx = \\mu + \\Lambda f + \\varepsilon\n\\tag{5.3}\\]\n\\[\nVar(x) = \\Lambda \\Phi \\Lambda^T + \\psi = \\Sigma(\\theta)\n\\tag{5.4}\\]\n\n5.1.1 Think about these situations\nWhat do you do when you have a large number of variables you are considering as predictors of a dependent variable?\n\nOften, subsets of these variables are measuring the same, or very similar things.\nWe might like to reduce the variables to a smaller number of predictors.\n\nWhat if you are developing a measurement scale and have a large number of items you think measure the same construct\n\nYou might want to see how strongly the items are related to the construct.\n\nTo make sense of these abstract goals, and to help explain the procedures of EFA in this chapter, I will use an example from chapter 4 of Desjardins and Bulut (2018).\nThis example uses as subset of the interest data which fictitious survey data on cognitive, personality and interest items. A subset of items related to cognition will be used here to follow the description in that text.\nFirst, load the hemp package and create a data frame called cognition.\n\nlibrary(hemp)\ncognition <- subset(interest, select = vocab:analyrea)\n\nThen look at a summary of the data.\n\nsummary(cognition)\n\n     vocab             reading           sentcomp           mathmtcs      \n Min.   :-2.62000   Min.   :-2.4700   Min.   :-2.47000   Min.   :-3.7100  \n 1st Qu.:-0.60500   1st Qu.:-0.5175   1st Qu.:-0.55000   1st Qu.:-0.4925  \n Median : 0.04000   Median : 0.1850   Median : 0.10500   Median : 0.1000  \n Mean   : 0.09016   Mean   : 0.1350   Mean   : 0.07356   Mean   : 0.1055  \n 3rd Qu.: 0.86000   3rd Qu.: 0.7975   3rd Qu.: 0.77500   3rd Qu.: 0.9200  \n Max.   : 2.63000   Max.   : 2.7000   Max.   : 2.73000   Max.   : 3.0600  \n    geometry          analyrea      \n Min.   :-3.3200   Min.   :-2.8300  \n 1st Qu.:-0.5600   1st Qu.:-0.4825  \n Median : 0.0900   Median : 0.2000  \n Mean   : 0.1125   Mean   : 0.1750  \n 3rd Qu.: 0.7675   3rd Qu.: 0.8375  \n Max.   : 3.8600   Max.   : 3.5000  \n\n\nLooking at the output, we see that the means of these variables are close to zero, with the spread of the data being roughly symmetrical around the mean, suggesting that these variables may have been standardized, which entails transforming them into z-scores. If this is the case, we would expect the variances (and standard deviations) to be about 1. We can look at the variances of each of the variables with the combination of the apply() and var() functions. The apply() function allows us to repeat another function over either the rows or columns of a two dimensional data object, such as a data frame. The MARGIN = 2, argument tells the function to appply var() over the columns (MARGIN = 1 would use rows). And the FUN = var argument identifies the function to apply over columns. Note the absence of parentheses after the function var in the call.\n\napply(cognition, MARGIN = 2, FUN = var)\n\n    vocab   reading  sentcomp  mathmtcs  geometry  analyrea \n0.9966514 0.9811568 0.9834142 1.1117325 1.0686631 1.1170926 \n\n\nThis supports our suspicion of standardized variables. It is often good to have items on the same or similar scales for factor analysis.\nThe cognition data are organized so that each individual is represented by one row, and their score on each item is contained in a separate column. This format is often called the wide format. To conduct factor analysis (and many other types of analysis) the long format is needed. Instead of separate columns containing the scores for each item, the long format would have all scores in one column and a categorical variable that captures which item each score represents. Therefore, each individual would be represented by multiple rows, one for each item, hence the long in long format.\nBelow, the reshape() function from base R is used to reshape the data from wide to long format, and store this new data frame in an object named cognition_l (note that the lowercase letter L is appended to the name, not the number 1). This function takes the cognition data (data = cognition), indicated that the first through the sixth columns are the ones to convert (varying = 1:6), names the variable that will contain the cell values from the original data as “score” (v.names = \"score\"), names the new character variable that describes which column the scores came from as “indicator” (timevar = \"indicator\"), uses the column names of those columns as the what will become the labels in the new categorical variable, which is a factor in R (times = names(cognition)), and finally tells the function to tranform the data into the “long” format (direction = \"long\"). The new To learn more about this function you can type ?reshape in to the R console. This transformation create the indicator variable as a charater variable, so we also convert it into a factor.\n\ncognition_l <- reshape(data = cognition,\n                       varying = 1:6,\n                       v.names = \"score\",\n                       timevar = \"indicator\",\n                       times = names(cognition),\n                       direction = \"long\")\n\n# Convert \"indicator\" into factor using the values as labels\ncognition_l$indicator <- factor(cognition_l$indicator)\n\nLooking at the first few rows, we can see that this data indeed has one column for the numeric values in the cells of the original data, and a factor variable containing the indicator type.\n\nhead(cognition_l[order(cognition_l$id), ])\n\n           indicator score id\n1.vocab        vocab  1.67  1\n1.reading    reading  1.67  1\n1.sentcomp  sentcomp  1.46  1\n1.mathmtcs  mathmtcs  0.90  1\n1.geometry  geometry  0.49  1\n1.analyrea  analyrea  1.65  1\n\n\nWith this long data frame, we can look at the univariate distribution of the 6 indicators. Below I show how to do that with the lattice package and the ggplot2 package.\n\nlibrary(lattice)\nhistogram(~ score | indicator, data = cognition_l)\n\n\n\n\n\nlibrary(ggplot2)\nggplot(cognition_l, aes(x = score)) + geom_histogram() +\n  facet_wrap(~ indicator )\n\n\n\n\nFrom these plots it seems reasonable to assume these variables are normally distributed, and we also see that these indicators have similar variances and are centered around zero, consistent with them being standardized.\nNext we can explore bivariate relations between the indicators. Before calculating correlation coefficients, it is helpful to plot the bivariate relations to evalute the assumptions of correlation coefficients as well as for the factor analysis. The pairs() function is useful for this.\n\npairs(cognition)\n\n\n\n\n\n\n5.1.2 Correlation Coefficient\nPearson product-moment correlation:\n\\[\nr_{xy} = \\frac{\\Sigma_{n=1}^n (x_k - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{n=1}^n(x_i - \\bar{x})^2} \\sqrt{\\Sigma_{n=1}^n(y_i - \\bar{y})^2}} = \\frac{s_{xy}}{s_x s_y}.\n\\]\nThe equation looks very daunting, until you see that it is just the covariance of \\(x\\) and \\(y\\) divided by the product of their standard deviations.\n\ncorrelations <- cor(cognition)\nround(correlations, 3)\n\n         vocab reading sentcomp mathmtcs geometry analyrea\nvocab    1.000   0.803    0.813    0.708    0.633    0.673\nreading  0.803   1.000    0.725    0.660    0.526    0.636\nsentcomp 0.813   0.725    1.000    0.618    0.575    0.618\nmathmtcs 0.708   0.660    0.618    1.000    0.774    0.817\ngeometry 0.633   0.526    0.575    0.774    1.000    0.715\nanalyrea 0.673   0.636    0.618    0.817    0.715    1.000\n\n\n\ncor_diff <- correlations - cor(cognition[-c(202, 53, 111), ])\nround(cor_diff, 3)\n\n         vocab reading sentcomp mathmtcs geometry analyrea\nvocab    0.000   0.005    0.009    0.000    0.021    0.009\nreading  0.005   0.000    0.010    0.016    0.025    0.021\nsentcomp 0.009   0.010    0.000    0.009    0.027    0.014\nmathmtcs 0.000   0.016    0.009    0.000   -0.004    0.005\ngeometry 0.021   0.025    0.027   -0.004    0.000    0.015\nanalyrea 0.009   0.021    0.014    0.005    0.015    0.000\n\n\n\nbollen_plot(cognition, crit.value = 0.06)\n\n\n\n\n\ncognition[c(202, 53, 111), ]\n\n    vocab reading sentcomp mathmtcs geometry analyrea\n202  2.63    2.23     2.55     1.38     3.86     3.50\n53  -0.38    0.99    -0.50     1.79    -0.19     2.13\n111 -2.01   -2.47    -2.47    -3.71    -2.59    -2.67\n\n\n\napply(cognition, 2, min)\n\n   vocab  reading sentcomp mathmtcs geometry analyrea \n   -2.62    -2.47    -2.47    -3.71    -3.32    -2.83 \n\napply(cognition, 2, max)\n\n   vocab  reading sentcomp mathmtcs geometry analyrea \n    2.63     2.70     2.73     3.06     3.86     3.50"
  },
  {
    "objectID": "efa.html#solutions",
    "href": "efa.html#solutions",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.2 Solutions",
    "text": "5.2 Solutions\n\nPrincipal Components Analysis\n\ntransforming the original variables into a new set of linear combinations (pricipal components).\n\nFactor Analysis\n\nsetting up a mathematical model to estimate the number or factors\n\n\n\n5.2.1 Principal Components Analysis\n\nConcerned with explaining variance-covariance structure of a set of variables.\nPCA attempts to explain as much of the total variance among the observed variables as possible with a smaller number of components.\nBecause the variables are standardized prior to analysis, the total amount of variance available is the number of variables.\nThe goal is data reduction for subsequent analysis.\nVariables cause components.\nComponents are not representative of any underlying theory.\n\n\n\n5.2.2 Factor Analysis\n\nThe goal is understanding underlying constructs.\nUses a modified correlation matrix (reduced matrix)\nfactors cause the variables.\nFactors represent theoretical constructs.\nFocuses on the common variance of the variables, and purges the unique variance.\n\n\n\n5.2.3 Steps in Factor Analysis\n\nChoose extraction method\n\nSo far we’ve focused on PCA\nEFA is often preferred if you are developing theory\n\nDetermine the number of components/factors\n\nKaiser method: eigenvalues > 1\nScree plot: All components before leveling off\nHorn’s parallel analysis: components/factors greater than simulated values from random numbers\n\nRotate Factors\n\nOrthogonal\nOblique\n\nInterpret Components/Factors\n\n\n\n5.2.4 “Little Jiffy” method of factor analysis\n\nExtraction method : PCA\nNumber of factors: eigenvalues > 1\nRotation: orthogonal(varimax)\nInterpretation\n\nFollowing these steps without thought can lead to many problems (see Preacher and MacCallum 2003)\n\n\n5.2.5 Eigenvalues\nEigenvalues represent the variance in the variables explained by the success components.\n\n\n5.2.6 Determining the Number of Factors\n\nKaiser criterion: Retain only factors with eigenvalues > 1. (generally accurate)\nScree plot: plot eigenvalues and drop factors after leveling off.\nParallel analysis: compare observed eigenvalues to parallel set of data from randomly generated data. Retain factors in original if eigenvalue is greater than random eigenvalue.\nFactor meaningfulness is also very important to consider.\n\n\n5.2.6.1 Kaiser\nRetain factors with eigenvalues greater than 1\n\neigen_decomp <- eigen(correlations)\nround(eigen_decomp$values, 3)\n\n[1] 4.436 0.676 0.322 0.245 0.168 0.152\n\n\n\n\n5.2.6.2 Scree Plot\n\nscree(cognition, pc = FALSE)\n\n\n\n\n\n\n5.2.6.3 Horn’s Parallel Analysis\n\nfa.parallel(cognition, fm = \"ml\")\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  1 \n\n\nUsing these conventions we can rewrite the classic test score model as:\n\nprincipal(correlations)\n\nPrincipal Components Analysis\nCall: principal(r = correlations)\nStandardized loadings (pattern matrix) based upon correlation matrix\n          PC1   h2   u2 com\nvocab    0.90 0.81 0.19   1\nreading  0.84 0.71 0.29   1\nsentcomp 0.84 0.71 0.29   1\nmathmtcs 0.89 0.79 0.21   1\ngeometry 0.82 0.67 0.33   1\nanalyrea 0.86 0.75 0.25   1\n\n                PC1\nSS loadings    4.44\nProportion Var 0.74\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.98\n\n\n\none_factor <- fa(r = cognition, nfactors = 1, rotate = \"oblimin\")\none_factor\n\nFactor Analysis using method =  minres\nCall: fa(r = cognition, nfactors = 1, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n          MR1   h2   u2 com\nvocab    0.89 0.79 0.21   1\nreading  0.81 0.65 0.35   1\nsentcomp 0.80 0.65 0.35   1\nmathmtcs 0.87 0.76 0.24   1\ngeometry 0.77 0.59 0.41   1\nanalyrea 0.84 0.70 0.30   1\n\n                MR1\nSS loadings    4.13\nProportion Var 0.69\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\nThe degrees of freedom for the null model are  15  and the objective function was  5.11 with Chi Square of  1257.24\nThe degrees of freedom for the model are 9  and the objective function was  0.71 \n\nThe root mean square of the residuals (RMSR) is  0.07 \nThe df corrected root mean square of the residuals is  0.1 \n\nThe harmonic number of observations is  250 with the empirical chi square  41.32  with prob <  4.4e-06 \nThe total number of observations was  250  with Likelihood Chi Square =  173.3  with prob <  1.3e-32 \n\nTucker Lewis Index of factoring reliability =  0.779\nRMSEA index =  0.27  and the 90 % confidence intervals are  0.236 0.307\nBIC =  123.61\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1\nCorrelation of (regression) scores with factors   0.97\nMultiple R square of scores with factors          0.94\nMinimum correlation of possible factor scores     0.87\n\n\n\ntwo_factor <- fa(r = cognition, nfactors = 2, rotate = \"oblimin\")\ntwo_factor\n\nFactor Analysis using method =  minres\nCall: fa(r = cognition, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n           MR1   MR2   h2   u2 com\nvocab     0.95  0.00 0.90 0.10   1\nreading   0.83  0.02 0.72 0.28   1\nsentcomp  0.87 -0.02 0.74 0.26   1\nmathmtcs -0.03  0.96 0.89 0.11   1\ngeometry -0.02  0.84 0.68 0.32   1\nanalyrea  0.07  0.81 0.76 0.24   1\n\n                       MR1  MR2\nSS loadings           2.36 2.31\nProportion Var        0.39 0.38\nCumulative Var        0.39 0.78\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n With factor correlations of \n    MR1 MR2\nMR1 1.0 0.8\nMR2 0.8 1.0\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  15  and the objective function was  5.11 with Chi Square of  1257.24\nThe degrees of freedom for the model are 4  and the objective function was  0.05 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  250 with the empirical chi square  1.42  with prob <  0.84 \nThe total number of observations was  250  with Likelihood Chi Square =  11.64  with prob <  0.02 \n\nTucker Lewis Index of factoring reliability =  0.977\nRMSEA index =  0.087  and the 90 % confidence intervals are  0.031 0.148\nBIC =  -10.44\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.97\nMultiple R square of scores with factors          0.94 0.94\nMinimum correlation of possible factor scores     0.88 0.87\n\n\n\n\n\n5.2.7 EFA with Categorical Data\n\nSAPA_subset <- subset(SAPA, select = c(letter.7:letter.58,\n                                       rotate.3:rotate.8))\n\nfa.parallel(SAPA_subset, cor = \"poly\")\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\nEFA_SAPA <- fa(r = SAPA_subset, nfactors = 2, rotate = \"oblimin\",\n               cor = \"poly\")\nEFA_SAPA\n\nFactor Analysis using method =  minres\nCall: fa(r = SAPA_subset, nfactors = 2, rotate = \"oblimin\", cor = \"poly\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            MR1   MR2   h2   u2 com\nletter.7  -0.02  0.79 0.60 0.40 1.0\nletter.33  0.01  0.70 0.50 0.50 1.0\nletter.34 -0.02  0.80 0.63 0.37 1.0\nletter.58  0.21  0.54 0.46 0.54 1.3\nrotate.3   0.86 -0.02 0.72 0.28 1.0\nrotate.4   0.82  0.10 0.77 0.23 1.0\nrotate.6   0.77  0.05 0.64 0.36 1.0\nrotate.8   0.86 -0.09 0.65 0.35 1.0\n\n                       MR1  MR2\nSS loadings           2.84 2.13\nProportion Var        0.36 0.27\nCumulative Var        0.36 0.62\nProportion Explained  0.57 0.43\nCumulative Proportion 0.57 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.56\nMR2 0.56 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  28  and the objective function was  4.26 with Chi Square of  6477.4\nThe degrees of freedom for the model are 13  and the objective function was  0.06 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic number of observations is  1523 with the empirical chi square  22.69  with prob <  0.046 \nThe total number of observations was  1525  with Likelihood Chi Square =  97.39  with prob <  5.3e-15 \n\nTucker Lewis Index of factoring reliability =  0.972\nRMSEA index =  0.065  and the 90 % confidence intervals are  0.053 0.078\nBIC =  2.1\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.95 0.92\nMultiple R square of scores with factors          0.91 0.84\nMinimum correlation of possible factor scores     0.81 0.68"
  },
  {
    "objectID": "efa.html#another-example",
    "href": "efa.html#another-example",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.3 Another Example",
    "text": "5.3 Another Example\n\nlibrary(\"MPsychoR\")\ndata(\"YouthDep\")\nitem1 <- YouthDep[, 1]\nlevels(item1) <- c(\"0\", \"1\", \"1\")\nitem2 <- YouthDep[, 14]\nlevels(item2) <- c(\"0\", \"1\", \"1\")\ntable(item1, item2)\n\n     item2\nitem1    0    1\n    0 1353  656\n    1  115  166\n\n\n\n## ------ correlation coefficients\nlibrary(\"psych\")\ntetcor <- tetrachoric(cbind(item1, item2))\ntetcor\n\nCall: tetrachoric(x = cbind(item1, item2))\ntetrachoric correlation \n      item1 item2\nitem1 1.00       \nitem2 0.35  1.00 \n\n with tau of \nitem1 item2 \n 1.16  0.36 \n\nitem1 <- YouthDep[, 1]\nitem2 <- YouthDep[, 14]\npolcor <- polychoric(cbind(item1, item2))\npolcor\n\nCall: polychoric(x = cbind(item1, item2))\nPolychoric correlations \n      item1 item2\nitem1 1.00       \nitem2 0.33  1.00 \n\n with tau of \n         1   2\nitem1 1.16 2.3\nitem2 0.36 1.2\n\ndraw.tetra(r = .35, t1 = 1.16, t2 = .36)\n\n\n\nDepItems <- YouthDep[,1:26] \nDepnum <- data.matrix(DepItems) - 1  ## convert to numeric   \nRdep <- polychoric(Depnum)\n\n\n5.3.1 Example data\n\nlower <- \"\n1.00\n0.70 1.00\n0.65 0.66 1.00\n0.62 0.63 0.60 1.00\n\"\ncormat <- getCov(lower, names = c(\"d1\", \"d2\", \"d3\", \"d4\"))\n\ncormat\n\n     d1   d2   d3   d4\nd1 1.00 0.70 0.65 0.62\nd2 0.70 1.00 0.66 0.63\nd3 0.65 0.66 1.00 0.60\nd4 0.62 0.63 0.60 1.00\n\n\n\n\n5.3.2 Kaiser\nRetain factors with eigenvalues greater than 1\n\neigen(cormat)$values\n\n[1] 2.9311792 0.4103921 0.3592372 0.2991916\n\n\n\n\n5.3.3 Scree Plot\n\nscree(cormat, factors = FALSE)\n\n\n\n\n\n\n5.3.4 Horn’s Parallel Analysis\n\nfa.parallel(cormat, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1"
  },
  {
    "objectID": "efa.html#another-example-1",
    "href": "efa.html#another-example-1",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.4 Another example",
    "text": "5.4 Another example\n\nfa.parallel(Harman74.cor$cov, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n\n\n5.4.1 Rotation\n\nPrincipal components are derived to maximize the variance accounted for (data reduction).\nRotation is done to make the factors more interpretable (i.e. meaningful).\nTwo major classes of rotation:\n\nOrthogonal - new factors are still uncorrelated, as were the initial factors.\nOblique - new factors are allowed to be correlated.\n\n\nEssentially reallocates the loadings. The first factor may not be the one accounting for the most variance.\n\n\n5.4.2 Orthogonal Rotation\n\nQuartimax - idea is to clean up the variables. Rotation done so each variable loads mainly on one factor. Problematic if there is a general factor on which most or all variables load on (think IQ).\nVarimax - to clean up factors. So each factor has high correlation with a smaller number of variables, low correlation with the other variables. Generally makes interpretation easier.\n\n\n\n5.4.3 Oblique Rotation\n\nOften correlated factors are more reasonable.\nTherefore, oblique rotation is often preferred.\nBut interpretation is more complicated.\n\n\n\n5.4.4 Factor Matrices\n\nFactor pattern matrix:\n\nincludes pattern coefficients analogous to standardized partial regression coefficients.\nIndicated the unique importance of a factor to a variable, holding other factors constant.\n\nFactor structure matrix:\n\nincludes structure coefficients which are simple correlations of the variables with the factors.\n\n\n\n\n5.4.5 Which matrix should we interpret?\n\nWhen orthogonal rotation is used interpret structural coefficients (but they are the same as pattern coefficients).\nWhen oblique rotation is used pattern coefficients are preferred because they account for the correlation between the factors and they are parameters of the correlated factor model (which we will discuss next class).\n\n\n\n5.4.6 Which variables should be used to interpret each factor?\n\nThe idea is to use only those variables that have a strong association with the factor.\nTypical thresholds are |.30| or |.40|.\nContent knowledge is critical."
  },
  {
    "objectID": "efa.html#tom-swifts-electric-factor-analysis-factory",
    "href": "efa.html#tom-swifts-electric-factor-analysis-factory",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.5 Tom Swift’s Electric Factor Analysis Factory",
    "text": "5.5 Tom Swift’s Electric Factor Analysis Factory\n\n5.5.1 Steps in Factor Analysis\n\nChoose extraction method\n\nSo far we’ve focused on PCA\n\nDetermine the number of components/factors\n\nKaiser method: eigenvalues > 1\nScree plot: All components before leveling off\nHorn’s parallel analysis: components/factors greater than simulated values from random numbers\n\nRotate Factors\n\nOrthogonal\nOblique\n\nInterpret Components/Factors\n\n\n\n5.5.2 “Little Jiffy” method of factor analysis\n\nExtraction method : PCA\nNumber of factors: eigenvalues > 1\nRotation: orthogonal(varimax)\nInterpretation\n\n\n\n5.5.3 Metal Boxes\n\n\n\nFunctional Definitions of Tom Swift’s Original 11 Variables\n\n\nDimension\nDerivation\n\n\n\n\nThickness\nx\n\n\nWidth\ny\n\n\nLength\nz\n\n\nVolume\nxyz\n\n\nDensity\nd\n\n\nWeight\nxyzd\n\n\nSurface area\n2(xy + xz + yz)\n\n\nCross-section\nyz\n\n\nEdge length\n4(x + y + z)\n\n\nDiagonal length\n(x^2)\n\n\nCost/lb\nc\n\n\n\n\n\n\n\n'data.frame':   63 obs. of  11 variables:\n $ thick   : num  1.362 1.83 0.567 1.962 1.762 ...\n $ width   : num  1.71 4.01 1.86 1.71 1.95 ...\n $ length  : num  4.93 5.2 4.31 3.91 4.1 ...\n $ volume  : num  10.02 39.59 8.02 16.02 15.92 ...\n $ density : int  17 6 4 14 7 4 1 1 14 8 ...\n $ weight  : num  170 240.1 32.1 224 111.6 ...\n $ surface : num  34 76.1 28.1 39.5 40.4 ...\n $ crosssec: num  9.87 19.94 7.94 8.22 8.24 ...\n $ edge    : num  31.9 43.9 27.9 31.5 31.7 ...\n $ diagonal: num  900 2025 441 576 576 ...\n $ cost    : num  9.73 14.13 -2.92 6.78 7.62 ...\n\n\n\n\n5.5.4 Correlations\n\n\n\nCorrelations between dimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthick\nwidth\nlength\nvolume\ndensity\nweight\nsurface\ncrosssec\nedge\ndiagonal\ncost\n\n\n\n\nthick\n1.00\n0.43\n0.28\n0.78\n-0.16\n0.53\n0.69\n0.40\n0.60\n0.48\n-0.01\n\n\nwidth\n0.43\n1.00\n0.50\n0.78\n-0.10\n0.54\n0.86\n0.88\n0.84\n0.74\n0.04\n\n\nlength\n0.28\n0.50\n1.00\n0.60\n0.13\n0.54\n0.73\n0.82\n0.82\n0.87\n0.22\n\n\nvolume\n0.78\n0.78\n0.60\n1.00\n-0.09\n0.69\n0.97\n0.81\n0.90\n0.86\n0.07\n\n\ndensity\n-0.16\n-0.10\n0.13\n-0.09\n1.00\n0.53\n-0.05\n0.01\n-0.02\n0.04\n0.83\n\n\nweight\n0.53\n0.54\n0.54\n0.69\n0.53\n1.00\n0.71\n0.64\n0.69\n0.68\n0.55\n\n\nsurface\n0.69\n0.86\n0.73\n0.97\n-0.05\n0.71\n1.00\n0.93\n0.97\n0.92\n0.11\n\n\ncrosssec\n0.40\n0.88\n0.82\n0.81\n0.01\n0.64\n0.93\n1.00\n0.95\n0.94\n0.15\n\n\nedge\n0.60\n0.84\n0.82\n0.90\n-0.02\n0.69\n0.97\n0.95\n1.00\n0.92\n0.14\n\n\ndiagonal\n0.48\n0.74\n0.87\n0.86\n0.04\n0.68\n0.92\n0.94\n0.92\n1.00\n0.17\n\n\ncost\n-0.01\n0.04\n0.22\n0.07\n0.83\n0.55\n0.11\n0.15\n0.14\n0.17\n1.00\n\n\n\n\n\n\n\n5.5.5 Eigenvalues > 1\n\n\n\n\n\n\n\n5.5.6 Orthogonal Rotation\n\n\n\nLoadings:\n         RC1    RC4    RC3    RC2    RC5   \nthick                   0.962              \nwidth            0.926                     \nlength    0.955                            \nvolume                                     \ndensity                        0.942       \nweight                                     \nsurface                                    \ncrosssec         0.705                     \nedge                                       \ndiagonal  0.775                            \ncost                           0.960       \n\n                 RC1   RC4   RC3   RC2   RC5\nSS loadings    3.165 2.913 2.226 2.193 0.263\nProportion Var 0.288 0.265 0.202 0.199 0.024\nCumulative Var 0.288 0.552 0.755 0.954 0.978\n\n\n\n\n5.5.7 Orthogonal Rotation with Loadings > .70\n\n\n\nLoadings:\n         RC1    RC3    RC2   \nthick            0.937       \nwidth     0.795              \nlength    0.885              \nvolume    0.709              \ndensity                 0.962\nweight                       \nsurface   0.843              \ncrosssec  0.967              \nedge      0.903              \ndiagonal  0.923              \ncost                    0.933\n\n                 RC1   RC3   RC2\nSS loadings    5.564 2.265 2.233\nProportion Var 0.506 0.206 0.203\nCumulative Var 0.506 0.712 0.915\n\n\n\n\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press.\n\n\nPreacher, Kristopher J., and Robert C. MacCallum. 2003. “Repairing Tom Swifts electric Factor Analysis Machine.” Understanding Statistics 2 (1): 13–43. https://doi.org/10.1207/s15328031us0201_02."
  }
]