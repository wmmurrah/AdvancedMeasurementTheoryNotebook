[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Measurement Theory: a Computational Project-Based Approach",
    "section": "",
    "text": "1 Introduction to the Course\nWelcome! This is a notebook for ERMA 8350 Advanced Measurement Theory. The class will be using the textbook Handbook of Educational Measurement and Psychometrics Using R (Desjardins and Bulut 2018), which will be the primary source for learning to use R for the methods covered in this course. I will use this notebook to make available additional readings to help you learn the theory behind these methods and to provide published examples of their use. It may include some examples from the textbook, with some elaborations, additional readings, and some more details about implementing the methods in R. These web-based notes will make it easy for you to use code, by allowing you to copy and paste code found within. Some of you will have experience with R and others not. So I will try to also point you to additional resources that may be helpful. For example, in this preface I will provide links to resources to help you setup R and RStudio. RStudio is a platform to make using R more productive. I will use it extensively in this course.\nThere are at least two way you can access the software needed for this course. You can use the virtual labs on campus. I know at least the education virtual labs have R and RStudio installed. IF you go this route you can watch the following video. Note you will need Duo setup for this to work.\nUsing Vlab to acces R/RStudio\nA better option if you have a laptop, you can install both programs on your computer. They are both absolutely free and available on all major operating systems, so you will not have to worry about transferring information across computers, limited connection speeds, or other hassles inherent with the VLab route.\nThe following links take you to videos instructing you how to install them.\nInstalling R and RStudio\nOrganizing Projects in RStudio"
  },
  {
    "objectID": "index.html#resources-for-learning-r",
    "href": "index.html#resources-for-learning-r",
    "title": "Advanced Measurement Theory: a Computational Project-Based Approach",
    "section": "Resources for Learning R",
    "text": "Resources for Learning R\nWhile such experience is certainly helpful, I do not assume you have prior knowledge of using R. I will demonstrate the use of R and provide (particularly in this notebook) the R code needed to use the methods we will learn. However, even if you have prior experience with R, you should plan to spend time learning to program in R. Some people find this intimidating initially, but most of you will grow to find R programming rewarding, and even fun by the end of the course. But, there will be frustration for sure.\nHere are some good places to start learning R:\nCRAN"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Advanced Measurement Theory: a Computational Project-Based Approach",
    "section": "R Packages",
    "text": "R Packages\nR is, among other paradigms, a functional programming language, which means is heavily utilizes functions. R’s functions are stored in packages. While base R has a long list of very useful functions, to fully realize the power of R you will have to use additional packages. So, learning how to install packages (downloading from the web to your computer) and loading packages (making the package’s functions accessible to your current R session) are important skills to master."
  },
  {
    "objectID": "index.html#how-to-use-these-notes",
    "href": "index.html#how-to-use-these-notes",
    "title": "Advanced Measurement Theory: a Computational Project-Based Approach",
    "section": "How To Use These Notes",
    "text": "How To Use These Notes\nBefore going further, it may be helpful to watch the following video about how to use the code in this notebook with Rstudio:\nHow to Use RStudio with this Notebook\n\n\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press."
  },
  {
    "objectID": "ctt.html#overview",
    "href": "ctt.html#overview",
    "title": "3  Classic Test Theory",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nThree important concepts:\n\nWe can build a model of instruments similar to the way we model conceptual relationships (measurement model vs. structural model).\nThe concept of reliability\nMeasurement models can account for different sources of error."
  },
  {
    "objectID": "ctt.html#classical-true-score-model",
    "href": "ctt.html#classical-true-score-model",
    "title": "3  Classic Test Theory",
    "section": "3.2 Classical True Score Model",
    "text": "3.2 Classical True Score Model\nThe true score model is: \\[\nX = T + E\n\\] where \\(X\\) is the observed score, \\(T\\) is the true score, which is unknown, and \\(E\\) is the error\nTo demonstrate this let’s assume we have the following data (R script is at end of this chapter),\n\nsource(\"code/simulate_CTTdata.R\")\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau\n1   1    1  3  6  5  3  5  5  4  5  3   3   4\n2   1    2  6  3  5  3  4  2  4  4  3   5   4\n3   1    3  4  4  2  4  4  3  5  3  4   5   4\n4   2    1  3  6  8  6  5  4  5  5  5   5   5\n5   2    2  6  4  6  6  4  6  6  5  4   4   5\n6   2    3  4  6  6  5  5  5  1  3  6   4   5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6\n8   3    2  6  6  6  7  5  6  6  6  6   7   6\n9   3    3  6  5  8  6  6  6  7  7  5   7   6\n10  4    1  4  3  5  4  2  3  3  5  5   2   4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4\n12  4    3  2  4  4  4  6  4  3  4  5   4   4\n13  5    1  5  6  5  4  5  5  5  6  5   6   5\n14  5    2  6  6  4  6  4  5  4  5  5   5   5\n15  5    3  6  4  5  4  5  5  4  4  5   5   5\n16  6    1  6  6  7  8  6  6  7  6  6   4   6\n17  6    2  4  5  7  5  5  7  4  5  6   7   6\n18  6    3  5  6  6  6  4  5  4  5  7   6   6\n\n\nwhere id is a variable indicating individual test-takers, time indicated which of 3 times each individual was assessed, x1 - x10 are the scores on 10 items that comprise the test, and Tau is the true value of the individuals ability. I use Tau here instead of T, because T is a protected symbol in R which is short-hand for TRUE. Note that we would not know Tau in most situations, but because this is simulated data we will pretend we do.\nWe can create a composite score for the ten items for each individual on each occasion by averaging columns 3 through 12.\n\nCTTdata$X <- rowMeans(CTTdata[ ,3:12])\n\nAnd we can also create E, the error with:\n\nCTTdata$E <- CTTdata$X - CTTdata$Tau\n\nAgain, in practice we would not be able to directly compute E because we would not know Tau, but we will use it to build an understanding of what error is.\nNow we have:\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6\n\n\nLook over the last three columns and make sure you understand their relation. For example, in the first row, note that X is .2 points above Tau, which is exactly the value of E we computed (\\(X_1 - T_1 = E_1 = 4.2 - 4 = .2\\)). The 1 subscript in the previous expression indicated row 1 (i.e. i = 1).\n\nCTTdata$X_t <- round(ave(CTTdata$X, CTTdata$id, FUN = mean),1)\n\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E X_t\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2 4.0\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1 4.0\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2 4.0\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2 4.9\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1 4.9\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5 4.9\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1 6.2\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1 6.2\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3 6.2\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4 4.0\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3 4.0\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0 4.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2 5.0\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0 5.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3 5.0\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2 5.7\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5 5.7\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6 5.7"
  },
  {
    "objectID": "ctt.html#reliability",
    "href": "ctt.html#reliability",
    "title": "3  Classic Test Theory",
    "section": "3.3 Reliability",
    "text": "3.3 Reliability\n\\[\n\\text{reliability} = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E} = \\rho^2_{XT}\n\\]\nThe reliability is the proportion of variance of \\(T\\) in \\(X\\), which is also the squared correlation between \\(X\\) and \\(T\\).\n\nTau <- CTTdata$Tau\nX <- CTTdata$X\nE <- CTTdata$X - CTTdata$Tau\n\n\nvar(Tau)/var(X)\n\n[1] 0.9170806\n\n\n\nvar(Tau)/(var(Tau) + var(E))\n\n[1] 0.8898776\n\n\n\ncor(Tau, X)^2\n\n[1] 0.886766\n\n\n\nplot(x = CTTdata$Tau, y = CTTdata$id, xlim = c(1,10),\n     ylim = c(0,7))\npoints(x = CTTdata$X, y = jitter(CTTdata$id), pch = 3, col = \"red\")\npoints(x = ave(x = CTTdata$X, factor(CTTdata$id), FUN = mean), y = CTTdata$id, \n       col = \"blue\", pch = 18)\n\n\n\n# points(x = CTTdata$X_t, pch = 2, factor(CTTdata$id))\n\n\n3.3.1 Cronbach’s \\(\\alpha\\)\nIn the notes for this chapter, I demonstrate aspects of classical test theory, reliability and generalizability theory using data from a study exploring the motivation of R package authors (Mair et al. 2015). This tutorial is based on Chapter 1 of Mair (2018), which can be consulted for a more in depth exposition of the underlying theory. Here I focus on demonstrating some of those concepts in R, as well as describing how to get certain results in R.\nFirst, I load the packages used in this tutorial:\n\n# Packages used:\nlibrary(psych)\nlibrary(MPsychoR)\n\nNext, I load the full data set from the MPsychoR package (Mair 2020), then as in the chapter, I subset the data to only include hybrid motivation items, followed by removing rows with missing values.\n\ndata(\"Rmotivation\")\n\n# Create data frame with only Hybrid Motivation items.\nHybMot <- subset(Rmotivation, \n                        select = grep(\"hyb\", names(Rmotivation)))\n# Remove rows  with any missing data.\nHybMot <- na.omit(HybMot)\n\nThis leads to a data set with 777 authors and 19 items.\n\n# How many authors(rows) and items(columns)?\ndim(HybMot)\n\n[1] 777  19\n\n# Note they are all dichotomous items.\nhead(HybMot)\n\n  hyb1 hyb2 hyb3 hyb4 hyb5 hyb6 hyb7 hyb8 hyb9 hyb10 hyb11 hyb12 hyb13 hyb14\n1    1    0    1    0    1    1    0    0    0     1     1     1     1     0\n3    0    0    1    0    1    0    0    0    0     1     0     1     1     0\n4    1    1    1    1    1    0    1    0    0     1     0     1     1     0\n5    1    0    0    1    1    0    0    0    0     1     1     1     1     0\n8    1    1    1    1    1    1    1    1    1     1     0     1     1     1\n9    1    0    0    1    1    0    0    0    0     1     0     0     1     0\n  hyb15 hyb16 hyb17 hyb18 hyb19\n1     1     1     1     1     1\n3     1     0     0     1     0\n4     0     1     1     1     1\n5     1     0     1     1     1\n8     1     1     1     1     1\n9     0     1     1     1     1\n\n\n\n# Variance/Covariance Matrix\nvcmat <- cov(HybMot)\nscroll_box(kable(vcmat, digits = 2), width = \"100%\")\n\n\n\n \n  \n      \n    hyb1 \n    hyb2 \n    hyb3 \n    hyb4 \n    hyb5 \n    hyb6 \n    hyb7 \n    hyb8 \n    hyb9 \n    hyb10 \n    hyb11 \n    hyb12 \n    hyb13 \n    hyb14 \n    hyb15 \n    hyb16 \n    hyb17 \n    hyb18 \n    hyb19 \n  \n \n\n  \n    hyb1 \n    0.18 \n    0.06 \n    0.04 \n    0.03 \n    0.03 \n    0.05 \n    0.01 \n    0.05 \n    0.04 \n    0.04 \n    0.03 \n    0.03 \n    0.03 \n    0.03 \n    0.04 \n    0.05 \n    0.04 \n    0.03 \n    0.06 \n  \n  \n    hyb2 \n    0.06 \n    0.25 \n    0.06 \n    0.05 \n    0.03 \n    0.05 \n    -0.01 \n    0.04 \n    0.05 \n    0.02 \n    0.04 \n    0.04 \n    0.03 \n    0.03 \n    0.03 \n    0.03 \n    0.01 \n    0.00 \n    0.03 \n  \n  \n    hyb3 \n    0.04 \n    0.06 \n    0.23 \n    0.13 \n    0.03 \n    0.05 \n    0.00 \n    0.03 \n    0.05 \n    0.04 \n    0.05 \n    0.06 \n    0.02 \n    0.02 \n    0.06 \n    0.02 \n    0.03 \n    0.02 \n    0.04 \n  \n  \n    hyb4 \n    0.03 \n    0.05 \n    0.13 \n    0.21 \n    0.03 \n    0.04 \n    0.01 \n    0.03 \n    0.04 \n    0.03 \n    0.05 \n    0.05 \n    0.02 \n    0.02 \n    0.04 \n    0.02 \n    0.03 \n    0.02 \n    0.04 \n  \n  \n    hyb5 \n    0.03 \n    0.03 \n    0.03 \n    0.03 \n    0.11 \n    0.02 \n    0.00 \n    0.01 \n    0.01 \n    0.03 \n    0.03 \n    0.03 \n    0.03 \n    0.02 \n    0.04 \n    0.02 \n    0.02 \n    0.01 \n    0.03 \n  \n  \n    hyb6 \n    0.05 \n    0.05 \n    0.05 \n    0.04 \n    0.02 \n    0.24 \n    0.01 \n    0.11 \n    0.15 \n    0.05 \n    0.06 \n    0.06 \n    0.02 \n    0.07 \n    0.06 \n    0.05 \n    0.05 \n    0.02 \n    0.07 \n  \n  \n    hyb7 \n    0.01 \n    -0.01 \n    0.00 \n    0.01 \n    0.00 \n    0.01 \n    0.22 \n    0.04 \n    0.01 \n    0.03 \n    0.00 \n    0.02 \n    0.00 \n    0.00 \n    0.01 \n    0.02 \n    0.02 \n    0.01 \n    0.02 \n  \n  \n    hyb8 \n    0.05 \n    0.04 \n    0.03 \n    0.03 \n    0.01 \n    0.11 \n    0.04 \n    0.25 \n    0.10 \n    0.06 \n    0.05 \n    0.06 \n    0.02 \n    0.04 \n    0.06 \n    0.05 \n    0.05 \n    0.03 \n    0.06 \n  \n  \n    hyb9 \n    0.04 \n    0.05 \n    0.05 \n    0.04 \n    0.01 \n    0.15 \n    0.01 \n    0.10 \n    0.20 \n    0.04 \n    0.04 \n    0.05 \n    0.01 \n    0.05 \n    0.04 \n    0.04 \n    0.03 \n    0.01 \n    0.05 \n  \n  \n    hyb10 \n    0.04 \n    0.02 \n    0.04 \n    0.03 \n    0.03 \n    0.05 \n    0.03 \n    0.06 \n    0.04 \n    0.15 \n    0.03 \n    0.06 \n    0.03 \n    0.02 \n    0.04 \n    0.04 \n    0.04 \n    0.03 \n    0.05 \n  \n  \n    hyb11 \n    0.03 \n    0.04 \n    0.05 \n    0.05 \n    0.03 \n    0.06 \n    0.00 \n    0.05 \n    0.04 \n    0.03 \n    0.23 \n    0.10 \n    0.03 \n    0.04 \n    0.10 \n    0.03 \n    0.03 \n    0.01 \n    0.02 \n  \n  \n    hyb12 \n    0.03 \n    0.04 \n    0.06 \n    0.05 \n    0.03 \n    0.06 \n    0.02 \n    0.06 \n    0.05 \n    0.06 \n    0.10 \n    0.23 \n    0.04 \n    0.03 \n    0.11 \n    0.04 \n    0.03 \n    0.02 \n    0.04 \n  \n  \n    hyb13 \n    0.03 \n    0.03 \n    0.02 \n    0.02 \n    0.03 \n    0.02 \n    0.00 \n    0.02 \n    0.01 \n    0.03 \n    0.03 \n    0.04 \n    0.10 \n    0.01 \n    0.03 \n    0.03 \n    0.02 \n    0.01 \n    0.02 \n  \n  \n    hyb14 \n    0.03 \n    0.03 \n    0.02 \n    0.02 \n    0.02 \n    0.07 \n    0.00 \n    0.04 \n    0.05 \n    0.02 \n    0.04 \n    0.03 \n    0.01 \n    0.20 \n    0.03 \n    0.04 \n    0.03 \n    0.01 \n    0.04 \n  \n  \n    hyb15 \n    0.04 \n    0.03 \n    0.06 \n    0.04 \n    0.04 \n    0.06 \n    0.01 \n    0.06 \n    0.04 \n    0.04 \n    0.10 \n    0.11 \n    0.03 \n    0.03 \n    0.23 \n    0.03 \n    0.03 \n    0.02 \n    0.04 \n  \n  \n    hyb16 \n    0.05 \n    0.03 \n    0.02 \n    0.02 \n    0.02 \n    0.05 \n    0.02 \n    0.05 \n    0.04 \n    0.04 \n    0.03 \n    0.04 \n    0.03 \n    0.04 \n    0.03 \n    0.17 \n    0.06 \n    0.05 \n    0.07 \n  \n  \n    hyb17 \n    0.04 \n    0.01 \n    0.03 \n    0.03 \n    0.02 \n    0.05 \n    0.02 \n    0.05 \n    0.03 \n    0.04 \n    0.03 \n    0.03 \n    0.02 \n    0.03 \n    0.03 \n    0.06 \n    0.14 \n    0.04 \n    0.07 \n  \n  \n    hyb18 \n    0.03 \n    0.00 \n    0.02 \n    0.02 \n    0.01 \n    0.02 \n    0.01 \n    0.03 \n    0.01 \n    0.03 \n    0.01 \n    0.02 \n    0.01 \n    0.01 \n    0.02 \n    0.05 \n    0.04 \n    0.08 \n    0.05 \n  \n  \n    hyb19 \n    0.06 \n    0.03 \n    0.04 \n    0.04 \n    0.03 \n    0.07 \n    0.02 \n    0.06 \n    0.05 \n    0.05 \n    0.02 \n    0.04 \n    0.02 \n    0.04 \n    0.04 \n    0.07 \n    0.07 \n    0.05 \n    0.21 \n  \n\n\n\n\n\n\nk <- ncol(HybMot)\nsigma2_Xi <- tr(vcmat) # trace of matrix or sum(diag(vmat))\nsigma2_X <- sum(vcmat)\n\n\n\n3.3.2 Other Reliability Coefficients"
  },
  {
    "objectID": "ctt.html#generalizability-theory",
    "href": "ctt.html#generalizability-theory",
    "title": "3  Classic Test Theory",
    "section": "3.4 Generalizability Theory",
    "text": "3.4 Generalizability Theory\nGeneralizability theory, or G-theory for short, is an extension of CTT, which decomposes the one error term in CTT into multiple sources of error called facets. These could include sources such as items, raters, or measurement occasions. These were each given a subscript on page 2 of the text.\nBefore looking at these different sources of error, let’s calculate Cronbach’s \\(\\alpha\\) in a different way, that will allow this decomposition going forward.\nWe will first need to reshape the data from wide to long format. A great tutorial on reshaping data with the reshape2 package can be found here:\nhttps://seananderson.ca/2013/10/19/reshape/\nBasically, we need to transform the data so that instead of each item being in a separate column are reshaped so there is one column with the cell values, and one column that identifies which item the score is from.\n\nlibrary(\"reshape2\")\n# Add person variable\nHyb1 <- data.frame(HybMot, person = 1:nrow(HybMot))\nHyblong <- melt(Hyb1, id.vars = c(\"person\"), variable.name = \"item\")\nHyblong$person <- as.factor(Hyblong$person)"
  },
  {
    "objectID": "ctt.html#r-scripts",
    "href": "ctt.html#r-scripts",
    "title": "3  Classic Test Theory",
    "section": "3.5 R Scripts",
    "text": "3.5 R Scripts\n\n3.5.1 Simulating CTT data\n\n#------------------------------------------------------------------------\n# Title: simulate_CTTdata\n# Author: William Murrah\n# Description: Simulate data to demonstrate CTT and reliability\n# Created: Monday, 09 August 2021\n# R version: R version 4.1.0 (2021-05-18)\n# Project(working) directory: /Users/wmm0017/Projects/Courses/\n#   AdvancedMeasurementTheoryNotebook\n#------------------------------------------------------------------------\n\nsimx <- function(truescore, sigmax = 1) {\n  x <- rnorm(18, truescore, sigmax)\n  return(round(x))\n}\nid <- rep(1:6, each = 3)\nTau <- rep(rep(4:6, each = 3),2)\nset.seed(20210805)\nCTTdata <- data.frame(\n  id = id,\n  time = rep(1:3, 6),\n  x1 = simx(Tau),\n  x2 = simx(Tau),\n  x3 = simx(Tau),\n  x4 = simx(Tau),\n  x5 = simx(Tau),\n  x6 = simx(Tau),\n  x7 = simx(Tau),\n  x8 = simx(Tau),\n  x9 = simx(Tau),\n  x10 = simx(Tau),\n  Tau = Tau\n)\nrm(id, Tau, simx)\n\n\n\n\n\nMair, Patrick. 2018. Modern Psychometrics with R. Springer.\n\n\n———. 2020. MPsychoR: Modern Psychometrics with R. https://CRAN.R-project.org/package=MPsychoR.\n\n\nMair, Patrick, Eva Hofmann, Kathrin Gruber, Reinhold Hatzinger, Achim Zeileis, and Kurt Hornik. 2015. “Motivation, Values, and Work Design as Drivers of Participation in the r Open Source Project for Statistical Computing.” Proceedings of the National Academy of Sciences 112 (48): 14788–92."
  },
  {
    "objectID": "generalizability.html#additional-readings",
    "href": "generalizability.html#additional-readings",
    "title": "4  Generalizability Theory",
    "section": "4.1 Additional Readings",
    "text": "4.1 Additional Readings\nFor more information of G theory, see Raykov and Marcoulides (2011). For an example using the R package lavaan with G theory, see Jorgensen (2021).\n\n\n\n\nJorgensen, Terrence D. 2021. “How to Estimate Absolute-Error Components in Structural Equation Models of Generalizability Theory.” Psych 3 (2): 113–33. https://doi.org/10.3390/psych3020011.\n\n\nMair, Patrick. 2018. Modern Psychometrics with R. Springer.\n\n\nRaykov, Tenko, and George A Marcoulides. 2011. Introduction to Psychometric Theory. Routledge."
  },
  {
    "objectID": "factor_analysis.html#correlation-coefficient",
    "href": "factor_analysis.html#correlation-coefficient",
    "title": "5  Factor Analysis",
    "section": "5.1 Correlation Coefficient",
    "text": "5.1 Correlation Coefficient\nPearson product-moment correlation:\n\\[\nr_{xy} = \\frac{\\Sigma_{n=1}^n (x_k - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{n=1}^n(x_i - \\bar{x})^2} \\sqrt{\\Sigma_{n=1}^n(y_i - \\bar{y})^2}} = \\frac{s_{xy}}{s_x s_y}.\n\\]\nThe equation looks very daunting, until you see that it is just the covariance of \\(x\\) and \\(y\\) divided by the product of their standard deviations.\n\nlibrary(\"MPsychoR\")\ndata(\"YouthDep\")\nitem1 <- YouthDep[, 1]\nlevels(item1) <- c(\"0\", \"1\", \"1\")\nitem2 <- YouthDep[, 14]\nlevels(item2) <- c(\"0\", \"1\", \"1\")\ntable(item1, item2)\n\n     item2\nitem1    0    1\n    0 1353  656\n    1  115  166\n\n\n\n## ------ correlation coefficients\nlibrary(\"psych\")\ntetcor <- tetrachoric(cbind(item1, item2))\ntetcor\n\nCall: tetrachoric(x = cbind(item1, item2))\ntetrachoric correlation \n      item1 item2\nitem1 1.00       \nitem2 0.35  1.00 \n\n with tau of \nitem1 item2 \n 1.16  0.36 \n\nitem1 <- YouthDep[, 1]\nitem2 <- YouthDep[, 14]\npolcor <- polychoric(cbind(item1, item2))\npolcor\n\nCall: polychoric(x = cbind(item1, item2))\nPolychoric correlations \n      item1 item2\nitem1 1.00       \nitem2 0.33  1.00 \n\n with tau of \n         1   2\nitem1 1.16 2.3\nitem2 0.36 1.2\n\ndraw.tetra(r = .35, t1 = 1.16, t2 = .36)\n\n\n\nDepItems <- YouthDep[,1:26] \nDepnum <- data.matrix(DepItems) - 1  ## convert to numeric   \nRdep <- polychoric(Depnum)"
  },
  {
    "objectID": "factor_analysis.html#think-about-these-situations",
    "href": "factor_analysis.html#think-about-these-situations",
    "title": "5  Factor Analysis",
    "section": "5.2 Think about these situations",
    "text": "5.2 Think about these situations\nWhat do you do when you have a large number of variables you are considering as predictors of a dependent variable?\n\nOften, subsets of these variables are measuring the same, or very similar things.\nWe might like to reduce the variables to a smaller number of predictors.\n\nWhat if you are developing a measurement scale and have a large number of items you think measure the same construct\n\nYou might want to see how strongly the items are related to the construct."
  },
  {
    "objectID": "factor_analysis.html#solutions",
    "href": "factor_analysis.html#solutions",
    "title": "5  Factor Analysis",
    "section": "5.3 Solutions",
    "text": "5.3 Solutions\n\nPrincipal Components Analysis\n\ntransforming the original variables into a new set of linear combinations (pricipal components).\n\nFactor Analysis\n\nsetting up a mathematical model to estimate the number or factors"
  },
  {
    "objectID": "factor_analysis.html#principal-components-analysis",
    "href": "factor_analysis.html#principal-components-analysis",
    "title": "5  Factor Analysis",
    "section": "5.4 Principal Components Analysis",
    "text": "5.4 Principal Components Analysis\n\nConcerned with explaining variance-covariance structure of a set of variables.\nPCA attempts to explain as much of the total variance among the observed variables as possible with a smaller number of components.\nBecause the variables are standardized prior to analysis, the total amount of variance available is the number of variables.\nThe goal is data reduction for subsequent analysis.\nVariables cause components.\nComponents are not representative of any underlying theory."
  },
  {
    "objectID": "factor_analysis.html#factor-analysis",
    "href": "factor_analysis.html#factor-analysis",
    "title": "5  Factor Analysis",
    "section": "5.5 Factor Analysis",
    "text": "5.5 Factor Analysis\n\nThe goal is understanding underlying constructs.\nUses a modified correlation matrix (reduced matrix)\nfactors cause the variables.\nFactors represent theoretical constructs.\nFocuses on the common variance of the variables, and purges the unique variance."
  },
  {
    "objectID": "factor_analysis.html#components",
    "href": "factor_analysis.html#components",
    "title": "5  Factor Analysis",
    "section": "5.6 Components",
    "text": "5.6 Components\nThe principal components partition the total variance (the sum of the variances of the original variables) by finding the linear combination of the variables that account for the maximum amount of variance:\n\\[\nPC1 = a_{11}x_1 + a_{12}x_2 ... a_{1p}x_p,\n\\] This is repeated as many time as there are variables."
  },
  {
    "objectID": "factor_analysis.html#pc-extraction",
    "href": "factor_analysis.html#pc-extraction",
    "title": "5  Factor Analysis",
    "section": "5.7 PC Extraction",
    "text": "5.7 PC Extraction\ndraw pretty pictures on the board"
  },
  {
    "objectID": "factor_analysis.html#eigenvalues",
    "href": "factor_analysis.html#eigenvalues",
    "title": "5  Factor Analysis",
    "section": "5.8 Eigenvalues",
    "text": "5.8 Eigenvalues\nEigenvalues represent the variance in the variables explained by the success components."
  },
  {
    "objectID": "factor_analysis.html#determining-the-number-of-factors",
    "href": "factor_analysis.html#determining-the-number-of-factors",
    "title": "5  Factor Analysis",
    "section": "5.9 Determining the Number of Factors",
    "text": "5.9 Determining the Number of Factors\n\nKaiser criterion: Retain only factors with eigenvalues > 1. (generally accurate)\nScree plot: plot eigenvalues and drop factors after leveling off.\nParallel analysis: compare observed eigenvalues to parallel set of data from randomly generated data. Retain factors in original if eigenvalue is greater than random eigenvalue.\nFactor meaningfulness is also very important to consider."
  },
  {
    "objectID": "factor_analysis.html#example-data",
    "href": "factor_analysis.html#example-data",
    "title": "5  Factor Analysis",
    "section": "5.10 Example data",
    "text": "5.10 Example data\n\nlower <- \"\n1.00\n0.70 1.00\n0.65 0.66 1.00\n0.62 0.63 0.60 1.00\n\"\ncormat <- getCov(lower, names = c(\"d1\", \"d2\", \"d3\", \"d4\"))\n\ncormat\n\n     d1   d2   d3   d4\nd1 1.00 0.70 0.65 0.62\nd2 0.70 1.00 0.66 0.63\nd3 0.65 0.66 1.00 0.60\nd4 0.62 0.63 0.60 1.00"
  },
  {
    "objectID": "factor_analysis.html#kaiser",
    "href": "factor_analysis.html#kaiser",
    "title": "5  Factor Analysis",
    "section": "5.11 Kaiser",
    "text": "5.11 Kaiser\nRetain factors with eigenvalues greater than 1\n\neigen(cormat)$values\n\n[1] 2.9311792 0.4103921 0.3592372 0.2991916"
  },
  {
    "objectID": "factor_analysis.html#scree-plot",
    "href": "factor_analysis.html#scree-plot",
    "title": "5  Factor Analysis",
    "section": "5.12 Scree Plot",
    "text": "5.12 Scree Plot\n\nscree(cormat, factors = FALSE)"
  },
  {
    "objectID": "factor_analysis.html#horns-parallel-analysis",
    "href": "factor_analysis.html#horns-parallel-analysis",
    "title": "5  Factor Analysis",
    "section": "5.13 Horn’s Parallel Analysis",
    "text": "5.13 Horn’s Parallel Analysis\n\nfa.parallel(cormat, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1"
  },
  {
    "objectID": "factor_analysis.html#another-example",
    "href": "factor_analysis.html#another-example",
    "title": "5  Factor Analysis",
    "section": "5.14 Another example",
    "text": "5.14 Another example\n\nfa.parallel(Harman74.cor$cov, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2"
  },
  {
    "objectID": "factor_analysis.html#rotation",
    "href": "factor_analysis.html#rotation",
    "title": "5  Factor Analysis",
    "section": "5.15 Rotation",
    "text": "5.15 Rotation\n\nPrincipal components are derived to maximize the variance accounted for (data reduction).\nRotation is done to make the factors more interpretable (i.e. meaningful).\nTwo major classes of rotation:\n\nOrthogonal - new factors are still uncorrelated, as were the initial factors.\nOblique - new factors are allowed to be correlated.\n\n\nEssentially reallocates the loadings. The first factor may not be the one accounting for the most variance."
  },
  {
    "objectID": "factor_analysis.html#orthogonal-rotation",
    "href": "factor_analysis.html#orthogonal-rotation",
    "title": "5  Factor Analysis",
    "section": "5.16 Orthogonal Rotation",
    "text": "5.16 Orthogonal Rotation\n\nQuartimax - idea is to clean up the variables. Rotation done so each variable loads mainly on one factor. Problematic if there is a general factor on which most or all variables load on (think IQ).\nVarimax - to clean up factors. So each factor has high correlation with a smaller number of variables, low correlation with the other variables. Generally makes interpretation easier."
  },
  {
    "objectID": "factor_analysis.html#oblique-rotation",
    "href": "factor_analysis.html#oblique-rotation",
    "title": "5  Factor Analysis",
    "section": "5.17 Oblique Rotation",
    "text": "5.17 Oblique Rotation\n\nOften correlated factors are more reasonable.\nTherefore, oblique rotation is often preferred.\nBut interpretation is more complicated."
  },
  {
    "objectID": "factor_analysis.html#factor-matrices",
    "href": "factor_analysis.html#factor-matrices",
    "title": "5  Factor Analysis",
    "section": "5.18 Factor Matrices",
    "text": "5.18 Factor Matrices\n\nFactor pattern matrix:\n\nincludes pattern coefficients analogous to standardized partial regression coefficients.\nIndicated the unique importance of a factor to a variable, holding other factors constant.\n\nFactor structure matrix:\n\nincludes structure coefficients which are simple correlations of the variables with the factors."
  },
  {
    "objectID": "factor_analysis.html#which-matrix-should-we-interpret",
    "href": "factor_analysis.html#which-matrix-should-we-interpret",
    "title": "5  Factor Analysis",
    "section": "5.19 Which matrix should we interpret?",
    "text": "5.19 Which matrix should we interpret?\n\nWhen orthogonal rotation is used interpret structural coefficients (but they are the same as pattern coefficients).\nWhen oblique rotation is used pattern coefficients are preferred because they account for the correlation between the factors and they are parameters of the correlated factor model (which we will discuss next class)."
  },
  {
    "objectID": "factor_analysis.html#which-variables-should-be-used-to-interpret-each-factor",
    "href": "factor_analysis.html#which-variables-should-be-used-to-interpret-each-factor",
    "title": "5  Factor Analysis",
    "section": "5.20 Which variables should be used to interpret each factor?",
    "text": "5.20 Which variables should be used to interpret each factor?\n\nThe idea is to use only those variables that have a strong association with the factor.\nTypical thresholds are |.30| or |.40|.\nContent knowledge is critical."
  },
  {
    "objectID": "factor_analysis.html#examples",
    "href": "factor_analysis.html#examples",
    "title": "5  Factor Analysis",
    "section": "5.21 Examples",
    "text": "5.21 Examples\nLet’s look at some examples"
  },
  {
    "objectID": "factor_analysis.html#steps-in-factor-analysis",
    "href": "factor_analysis.html#steps-in-factor-analysis",
    "title": "5  Factor Analysis",
    "section": "5.22 Steps in Factor Analysis",
    "text": "5.22 Steps in Factor Analysis\n\nChoose extraction method\n\nSo far we’ve focused on PCA\n\nDetermine the number of components/factors\n\nKaiser method: eigenvalues > 1\nScree plot: All components before leveling off\nHorn’s parallel analysis: components/factors greater than simulated values from random numbers\n\nRotate Factors\n\nOrthogonal\nOblique\n\nInterpret Components/Factors"
  },
  {
    "objectID": "factor_analysis.html#tom-swifts-electric-factor-analysis-factory",
    "href": "factor_analysis.html#tom-swifts-electric-factor-analysis-factory",
    "title": "5  Factor Analysis",
    "section": "5.23 Tom Swift’s Electric Factor Analysis Factory",
    "text": "5.23 Tom Swift’s Electric Factor Analysis Factory\n“Little Jiffy” method of factor analysis\n\nExtraction method : PCA\nNumber of factors: eigenvalues > 1\nRotation: orthogonal(varimax)\nInterpretation"
  },
  {
    "objectID": "factor_analysis.html#metal-boxes",
    "href": "factor_analysis.html#metal-boxes",
    "title": "5  Factor Analysis",
    "section": "5.24 Metal Boxes",
    "text": "5.24 Metal Boxes\n\n\n\nFunctional Definitions of Tom Swift’s Original 11 Variables\n\n\nDimension\nDerivation\n\n\n\n\nThickness\nx\n\n\nWidth\ny\n\n\nLength\nz\n\n\nVolume\nxyz\n\n\nDensity\nd\n\n\nWeight\nxyzd\n\n\nSurface area\n2(xy + xz + yz)\n\n\nCross-section\nyz\n\n\nEdge length\n4(x + y + z)\n\n\nDiagonal length\n(x^2)\n\n\nCost/lb\nc\n\n\n\n\n\n\n\n'data.frame':   63 obs. of  11 variables:\n $ thick   : num  1.362 2.385 3.101 0.934 0.845 ...\n $ width   : num  1.71 2.83 4.32 3.2 3.84 ...\n $ length  : num  2.93 5.01 5.99 4.15 4.09 ...\n $ volume  : num  6.02 30.2 72.01 11.78 16.1 ...\n $ density : int  10 7 16 22 11 16 11 21 6 13 ...\n $ weight  : num  60 210 1152 264 176 ...\n $ surface : num  22 62.1 108.2 38 48 ...\n $ crosssec: num  5.87 15.06 23.53 12.02 16.13 ...\n $ edge    : num  23.9 39.9 51.5 31.9 36.1 ...\n $ diagonal: num  196 1444 3721 676 1089 ...\n $ cost    : num  4.48 2.37 9.77 22.21 15.86 ..."
  },
  {
    "objectID": "factor_analysis.html#correlations",
    "href": "factor_analysis.html#correlations",
    "title": "5  Factor Analysis",
    "section": "5.25 Correlations",
    "text": "5.25 Correlations\n\n\n\nCorrelations between dimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthick\nwidth\nlength\nvolume\ndensity\nweight\nsurface\ncrosssec\nedge\ndiagonal\ncost\n\n\n\n\nthick\n1.00\n0.49\n0.24\n0.84\n-0.13\n0.59\n0.74\n0.46\n0.61\n0.51\n-0.02\n\n\nwidth\n0.49\n1.00\n0.61\n0.77\n-0.15\n0.55\n0.87\n0.92\n0.88\n0.78\n0.03\n\n\nlength\n0.24\n0.61\n1.00\n0.58\n-0.02\n0.45\n0.72\n0.83\n0.84\n0.86\n-0.02\n\n\nvolume\n0.84\n0.77\n0.58\n1.00\n-0.22\n0.65\n0.97\n0.81\n0.87\n0.85\n-0.11\n\n\ndensity\n-0.13\n-0.15\n-0.02\n-0.22\n1.00\n0.44\n-0.20\n-0.15\n-0.15\n-0.18\n0.62\n\n\nweight\n0.59\n0.55\n0.45\n0.65\n0.44\n1.00\n0.65\n0.56\n0.61\n0.57\n0.24\n\n\nsurface\n0.74\n0.87\n0.72\n0.97\n-0.20\n0.65\n1.00\n0.92\n0.97\n0.91\n-0.07\n\n\ncrosssec\n0.46\n0.92\n0.83\n0.81\n-0.15\n0.56\n0.92\n1.00\n0.96\n0.93\n-0.03\n\n\nedge\n0.61\n0.88\n0.84\n0.87\n-0.15\n0.61\n0.97\n0.96\n1.00\n0.92\n-0.04\n\n\ndiagonal\n0.51\n0.78\n0.86\n0.85\n-0.18\n0.57\n0.91\n0.93\n0.92\n1.00\n-0.12\n\n\ncost\n-0.02\n0.03\n-0.02\n-0.11\n0.62\n0.24\n-0.07\n-0.03\n-0.04\n-0.12\n1.00"
  },
  {
    "objectID": "factor_analysis.html#eigenvalues-1",
    "href": "factor_analysis.html#eigenvalues-1",
    "title": "5  Factor Analysis",
    "section": "5.26 Eigenvalues > 1",
    "text": "5.26 Eigenvalues > 1"
  },
  {
    "objectID": "factor_analysis.html#orthogonal-rotation-1",
    "href": "factor_analysis.html#orthogonal-rotation-1",
    "title": "5  Factor Analysis",
    "section": "5.27 Orthogonal Rotation",
    "text": "5.27 Orthogonal Rotation\n\n\n\nLoadings:\n         RC1    RC3    RC2    RC5    RC4   \nthick            0.968                     \nwidth                          0.734       \nlength    0.986                            \nvolume           0.754                     \ndensity                 0.864              \nweight                                     \nsurface   0.703                            \ncrosssec  0.829                            \nedge      0.819                            \ndiagonal  0.875                            \ncost                                  0.955\n\n                 RC1   RC3   RC2   RC5   RC4\nSS loadings    4.425 2.662 1.318 1.225 1.106\nProportion Var 0.402 0.242 0.120 0.111 0.101\nCumulative Var 0.402 0.644 0.764 0.876 0.976"
  },
  {
    "objectID": "factor_analysis.html#orthogonal-rotation-with-loadings-.70",
    "href": "factor_analysis.html#orthogonal-rotation-with-loadings-.70",
    "title": "5  Factor Analysis",
    "section": "5.28 Orthogonal Rotation with Loadings > .70",
    "text": "5.28 Orthogonal Rotation with Loadings > .70\n\n\n\nLoadings:\n         RC1    RC3    RC2   \nthick            0.947       \nwidth     0.801              \nlength    0.936              \nvolume           0.744       \ndensity                 0.930\nweight                       \nsurface   0.792              \ncrosssec  0.942              \nedge      0.892              \ndiagonal  0.905              \ncost                    0.841\n\n                 RC1   RC3   RC2\nSS loadings    5.298 2.699 1.868\nProportion Var 0.482 0.245 0.170\nCumulative Var 0.482 0.727 0.897"
  },
  {
    "objectID": "factor_analysis.html#r-code-for-chapter-2",
    "href": "factor_analysis.html#r-code-for-chapter-2",
    "title": "5  Factor Analysis",
    "section": "5.29 R Code for Chapter 2",
    "text": "5.29 R Code for Chapter 2\n\n## ----------------------- Chapter 2: Factor Analysis ------------------\nlibrary(\"MPsychoR\")\ndata(\"YouthDep\")\nitem1 <- YouthDep[, 1]\nlevels(item1) <- c(\"0\", \"1\", \"1\")\nitem2 <- YouthDep[, 14]\nlevels(item2) <- c(\"0\", \"1\", \"1\")\ntable(item1, item2)\n\n## ------ correlation coefficients\nlibrary(\"psych\")\ntetcor <- tetrachoric(cbind(item1, item2))\ntetcor\nitem1 <- YouthDep[, 1]\nitem2 <- YouthDep[, 14]\npolcor <- polychoric(cbind(item1, item2))\npolcor\n\nDepItems <- YouthDep[,1:26] \nDepnum <- data.matrix(DepItems) - 1  ## convert to numeric   \nRdep <- polychoric(Depnum)\n\ndata(\"Rmotivation\")\nvind <- grep(\"ext|int\", colnames(Rmotivation)) \nRmotivation1 <- Rmotivation[, vind]\nRmot1 <- tetrachoric(Rmotivation1, smooth = FALSE)\ntail(round(eigen(Rmot1$rho)$values, 3))\nRmot <- tetrachoric(Rmotivation1)\ntail(round(eigen(Rmot$rho)$values, 3))\n\n## ----- exploratory factor analysis \nmotFA <- fa(Rmot$rho, nfactors = 2, rotate = \"none\", fm = \"ml\")\nprint(motFA$loadings, cutoff = 0.2)\nround(motFA$communality, 2)\n\nmotFA2 <- fa(Rmot$rho, nfactors = 2, rotate = \"varimax\", fm = \"ml\")\nplot(motFA$loadings, asp = 1, xlim = c(-0.2, 0.9), ylim = c(-0.5, 0.9), type = \"n\", xlab = \"Factor 1\", ylab = \"Factor 2\", main = \"Loadings Plot\")\ntext(motFA$loadings, labels = rownames(motFA$loadings), cex = 0.8, col = \"gray\")\nabline(h = 0, v = 0, col = \"lightgray\", lty = 2)\ntext(motFA2$loadings, labels = rownames(motFA2$loadings), col = 1, cex = 0.8)\nlegend(\"bottomleft\", legend = c(\"rotated\", \"unrotated\"), col = c(\"black\", \"gray\"), pch = 19)\n\nRmot2 <- tetrachoric(Rmotivation[,1:36])\nmotFA3 <- fa(Rmot2$rho, nfactors = 3, rotate = \"oblimin\", fm = \"ml\")\nmotFA3$loadings\nround(motFA3$Phi, 3)\n\nmotFA2 <- fa(Rmotivation1, nfactors = 2, rotate = \"varimax\", cor = \"tet\", fm = \"ml\", scores = \"regression\",\n             missing = TRUE, impute = \"median\")\ndim(motFA2$scores)\n\nRdep <- polychoric(Depnum)$rho\nevals <- eigen(Rdep)$values\nscree(Rdep, factors = FALSE)\n(evals/sum(evals)*100)[1:2]\n\nset.seed(123)\nresPA <- fa.parallel(Depnum, fa = \"pc\", cor = \"poly\", fm = \"ml\")  \nresvss <- vss(Rdep, fm = \"ml\", n.obs = nrow(Depnum), plot = FALSE)\nresvss\n\nfadep <- fa(Depnum, 1, cor = \"poly\", fm = \"ml\")\nsummary(fadep)\n\nresnf <- nfactors(Depnum, n = 8, fm = \"ml\", cor = \"poly\")\nresnf\n\n## ----- Bayesian exploratory factor analysis\nlibrary(\"MPsychoR\")\nlibrary(\"corrplot\")\nlibrary(\"BayesFM\")\ndata(\"Privacy\")\nPrivstd <- scale(Privacy)\ncorrplot(cor(Privstd))\n\nNid <- 2              ## minimum number of variables per factor\npmax <- trunc(ncol(Privstd)/Nid)   ## maximum number of factors\npmax\n\nset.seed(123)\nRsim <- simul.R.prior(pmax, nu0 = pmax + c(1, 2, 5, 7, 10))\nplot(Rsim)\n\nKsim <- simul.nfac.prior(nvar = ncol(Privstd), Nid = Nid, Kmax = pmax, kappa = c(.1, .2, .5, 1))\nplot(Ksim)\n\nset.seed(222)\nfitbefa <- befa(Privstd, Nid = 2, Kmax = pmax, nu0 = 10, kappa = 0.2, kappa0 = 0.1, xi0 = 0.1,\n                burnin = 5000, iter = 50000)\nfitbefa <- post.column.switch(fitbefa)   ## column reordering\nfitbefa <- post.sign.switch(fitbefa)     ## sign switching\nsumbefa <- summary(fitbefa)\n\n## ----- confirmatory factor analysis\nlibrary(\"MPsychoR\")\nlibrary(\"lavaan\")\ndata(\"Rmotivation\")\nvind <- grep(\"ext|int\", colnames(Rmotivation)) ## ext/int items\nRmot <- na.omit(Rmotivation[, vind])\nmot_model <- ' \n  extrinsic  =~ ext1 + ext2 + ext3 + ext4 + ext5 + ext6 + \n                ext7 + ext8 + ext9 + ext10 + ext11 + ext12      \n  intrinsic =~  int1 + int2 + int3 + int4 + int5'\nfitMot <- lavaan::cfa(mot_model, data = Rmot, ordered = names(Rmot))\n\nlibrary(\"semPlot\")\nsemPaths(fitMot, what = \"est\", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 4.5, asize = 2.5,\n         intercepts = FALSE, rotation = 4, thresholdColor = \"red\", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)\n\ninspect(fitMot, what = \"est\")$theta\ninspect(fitMot, what = \"est\")$lambda\ninspect(fitMot, what = \"std\")$lambda\ninspect(fitMot, what = \"est\")$psi\ninspect(fitMot, what = \"std\")$psi\n\nparameterEstimates(fitMot, standardized = TRUE)\nsummary(fitMot, standardized = TRUE, fit.measures = TRUE)\nparameterEstimates(fitMot)[5,]\n\nmot_model2 <- '\n  extrinsic  =~ ext1 + ext2 + ext3 + ext4 + ext6 + ext7 + \n                ext8 + ext9 + ext10 + ext11 + ext12\n  intrinsic =~  int1 + int2 + int3 + int4 + int5'\nfitMot2 <- lavaan::cfa(mot_model2, data = Rmot, ordered = names(Rmot)[-5])\nvind <- c(1:4, 13:16, 32:35)\nRmot2 <- na.omit(Rmotivation[, vind])\n\nmot_model3 <- '\n  extrinsic  =~ ext1 + ext2 + ext3 + ext4 \n  hybrid =~ hyb1 + hyb2 + hyb3 + hyb4              \n  intrinsic =~  int1 + int2 + int3 + int4 \n  motivation =~ extrinsic + hybrid + intrinsic'\nfitMot3 <- lavaan::cfa(mot_model3, data = Rmot2, ordered = names(Rmot2))\n\nsemPaths(fitMot3, what = \"std\", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5,\n         intercepts = FALSE, rotation = 4, thresholdColor = \"red\", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)\n\nsummary(fitMot3, standardized = TRUE, fit.measures = TRUE)\n\nvind <- c(1:4, 13:16, 32:35, 39:41)\nRmot3 <- na.omit(Rmotivation[, vind])\nmot_model4 <- '\n  extrinsic  =~ ext1 + ext2 + ext3 + ext4 \n  hybrid =~ hyb1 + hyb2 + hyb3 + hyb4              \n  intrinsic =~  int1 + int2 + int3 + int4 \n  motivation =~ extrinsic + hybrid + intrinsic\n  motivation ~ npkgs + phd'\nfitMot4 <- lavaan::cfa(mot_model4, data = Rmot3, ordered = names(Rmot3[1:12]))\n\nsemPaths(fitMot4, what = \"std\", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 5, asize = 2.5,\n         intercepts = FALSE, rotation = 4, thresholdColor = \"red\", mar = c(1, 5, 1.5, 5), fade = FALSE, nCharNodes = 4)\nparameterEstimates(fitMot4)[16:17,]\n\nlibrary(\"semTools\")\ndata(\"Bergh\")\nGP_model <- 'GP =~ EP + HP + DP + SP'\nminvfit1 <- measEq.syntax(GP_model, data = Bergh, group = \"gender\", return.fit = TRUE)\nminvfit2 <- measEq.syntax(GP_model, data = Bergh, group = \"gender\", \n                          group.equal = c(\"loadings\"), return.fit = TRUE)\nminvfit3 <- measEq.syntax(GP_model, data = Bergh, group = \"gender\", \n                          group.equal = c(\"loadings\", \"intercepts\"), return.fit = TRUE)\nminvfit4 <- measEq.syntax(GP_model, data = Bergh, group = \"gender\", \n                          group.equal = c(\"loadings\", \"intercepts\", \"means\"), return.fit = TRUE)\nanova(minvfit1, minvfit2, minvfit3, minvfit4)\n\nGP_model <- 'GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + SP'\nfitBase <- lavaan::cfa(GP_model, data = Bergh, group = \"gender\", estimator = \"MLR\")\n\nGP_model <- 'GP =~ EP + HP + DP + SP'\nfitBase <- lavaan::cfa(GP_model, data = Bergh, group = \"gender\", group.equal = c(\"loadings\"),\n                       group.partial = c(\"GP=~ SP\"), estimator = \"MLR\")\n\nfitBase1 <- lavaan::cfa(GP_model, data = Bergh, group = \"gender\", group.equal = c(\"loadings\", \"intercepts\"), \n                        group.partial = c(\"GP=~SP\", \"DP~1\", \"HP~1\", \"SP~1\"), estimator = \"MLR\")\n\nGP_model2 <- 'GP =~ c(v1,v1)*EP + c(v2,v2)*HP + c(v3,v3)*DP + c(NA, 0)*SP'\nfitIO <- lavaan::cfa(GP_model2, data = Bergh, group = \"gender\", group.equal = c(\"intercepts\"), \n                     group.partial = c(\"DP~1\", \"HP~1\", \"SP~1\"), estimator = \"MLR\")\n\nfitMarg <- lavaan::cfa(GP_model, data = Bergh, group = \"gender\", group.equal = c(\"loadings\", \"intercepts\"),\n                       group.partial = c(\"DP~1\", \"HP~1\", \"SP~1\"), estimator = \"MLR\")\n\nanova(fitMarg, fitBase1)\n\nlibrary(\"MPsychoR\")\nlibrary(\"lavaan\")\ndata(\"SDOwave\")\nmodel_sdo1 <- '\n  SDO1996 =~ 1*I1.1996 + a2*I2.1996 + a3*I3.1996 + a4*I4.1996\n  SDO1998 =~ 1*I1.1998 + a2*I2.1998 + a3*I3.1998 + a4*I4.1998\n  SDO1996 ~~ SDO1998\n\n  ## intercepts\n  I1.1996 ~ int1*1; I1.1998 ~ int1*1\n  I2.1996 ~ int2*1; I2.1998 ~ int2*1 \n  I3.1996 ~ int3*1; I3.1998 ~ int3*1\n  I4.1996 ~ int4*1; I4.1998 ~ int4*1\n\n  ## residual covariances\n  I1.1996 ~~ I1.1998\n  I2.1996 ~~ I2.1998\n  I3.1996 ~~ I3.1998\n  I4.1996 ~~ I4.1998\n\n  ## latent means: 1996 as baseline\n  SDO1996 ~ 0*1\n  SDO1998 ~ 1'\nfitsdo1 <- cfa(model_sdo1, data = SDOwave, estimator = \"MLR\")\nparameterEstimates(fitsdo1)[22:23,]\n\nmodel_sdo2 <- '\n  ## 1st CFA level, constant loadings across time\n  SDOD1996 =~ 1*I1.1996 + d1*I2.1996\n  SDOD1998 =~ 1*I1.1998 + d1*I2.1998\n  SDOD1999 =~ 1*I1.1999 + d1*I2.1999 \n  SDOE1996 =~ 1*I3.1996 + a1*I4.1996 \n  SDOE1998 =~ 1*I3.1998 + a1*I4.1998\n  SDOE1999 =~ 1*I3.1999 + a1*I4.1999\n\n  ## 2nd CFA level, constant loadings across time\n  SDO1996 =~ 1*SDOD1996 + sd1*SDOE1996\n  SDO1998 =~ 1*SDOD1998 + sd1*SDOE1998\n  SDO1999 =~ 1*SDOD1999 + sd1*SDOE1999\n\n  ## Constant 1st level intercepts\n  I1.1996 ~ iI1*1; I1.1998 ~ iI1*1; I1.1999 ~ iI1*1\n  I2.1996 ~ iI2*1; I2.1998 ~ iI2*1; I2.1999 ~ iI2*1\n  I3.1996 ~ iI3*1; I3.1998 ~ iI3*1; I3.1999 ~ iI3*1\n  I4.1996 ~ iI4*1; I4.1998 ~ iI4*1; I4.1999 ~ iI4*1\n\n  ## residual covariances:\n  I1.1999 ~~ I1.1998; I1.1996 ~~ I1.1998; I1.1999 ~~ I1.1996\n  I2.1999 ~~ I2.1998; I2.1996 ~~ I2.1998; I2.1999 ~~ I2.1996\n  I3.1999 ~~ I3.1998; I3.1996 ~~ I3.1998; I3.1999 ~~ I3.1996\n  I4.1999 ~~ I4.1998; I4.1996 ~~ I4.1998; I4.1999 ~~ I4.1996\n\n  ## latent means\n  SDO1996 ~ 0*1    ## 1996 baseline year\n  SDO1998 ~ 1      ## 1998 vs. 1996\n  SDO1999 ~ 1      ## 1999 vs. 1996\n'\nfitsdo2 <- cfa(model_sdo2, data = SDOwave, estimator = \"MLR\")\n\nsemPaths(fitsdo2, what = \"est\", edge.label.cex = 0.7, edge.color = 1, esize = 1, sizeMan = 6, asize = 2.5,\n         intercepts = FALSE, rotation = 4, thresholdColor = \"red\", mar = c(1, 5, 1.5, 5), fade = FALSE)\nparameterEstimates(fitsdo2)[43:45,]\n\ndata(\"FamilyIQ\")\nmodelIQ <- '\n level: 1\n  numeric =~ wordlist + cards + matrices\n  perception =~ figures + animals + occupation\n level: 2\n  general =~ wordlist + cards +  matrices + figures + animals +\n             occupation'\nfitIQ <- cfa(modelIQ, data = FamilyIQ, cluster = \"family\", std.lv = TRUE)\nfitIQ\n\n## ----- bayesian confirmatory factor analysis\nlibrary(\"blavaan\")\ndpriors()[c(\"lambda\", \"itheta\", \"ipsi\")]\n\nlibrary(\"MPsychoR\")\ndata(\"Bergh\")\nGP_model <- 'GP =~ EP + HP + DP + SP'\nset.seed(123)\nfitBCFA <- bcfa(GP_model, data = Bergh, burnin = 2000, sample = 10000, n.chains = 2)\n\nplot(fitBCFA, pars = 1:2, plot.type = \"trace\")\nplot(fitBCFA, pars = 1:2, plot.type = \"autocorr\")\nsummary(fitBCFA)"
  }
]