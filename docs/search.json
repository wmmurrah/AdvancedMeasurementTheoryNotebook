[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "",
    "text": "1 Introduction to the Course\nWelcome! This is a notebook for ERMA 8350 Advanced Measurement Theory. The class will be using the textbook Handbook of Educational Measurement and Psychometrics Using R (Desjardins and Bulut 2018), which will be the primary source for learning to use R for the methods covered in this course. I will use this notebook to make available additional readings to help you learn the theory behind these methods and to provide published examples of their use. It may include some examples from the textbook, with some elaborations, additional readings, and some more details about implementing the methods in R. These web-based notes will make it easy for you to use code, by allowing you to copy and paste code found within. Some of you will have experience with R and others not. So I will try to also point you to additional resources that may be helpful. For example, in this preface I will provide links to resources to help you setup R and RStudio. RStudio is a platform to make using R more productive. I will use it extensively in this course.\nThere are at least two way you can access the software needed for this course. You can use the virtual labs on campus. I know at least the education virtual labs have R and RStudio installed. IF you go this route you can watch the following video. Note you will need Duo setup for this to work.\nUsing Vlab to acces R/RStudio\nA better option if you have a laptop, you can install both programs on your computer. They are both absolutely free and available on all major operating systems, so you will not have to worry about transferring information across computers, limited connection speeds, or other hassles inherent with the VLab route.\nThe following links take you to videos instructing you how to install them.\nInstalling R and RStudio\nOrganizing Projects in RStudio"
  },
  {
    "objectID": "index.html#resources-for-learning-r",
    "href": "index.html#resources-for-learning-r",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "Resources for Learning R",
    "text": "Resources for Learning R\nWhile such experience is certainly helpful, I do not assume you have prior knowledge of using R. I will demonstrate the use of R and provide (particularly in this notebook) the R code needed to use the methods we will learn. However, even if you have prior experience with R, you should plan to spend time learning to program in R. Some people find this intimidating initially, but most of you will grow to find R programming rewarding, and even fun by the end of the course. But, there will be frustration for sure.\nHere are some good places to start learning R:\nCRAN"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Advanced Measurement Theory: a Computational Model-Based Approach",
    "section": "R Packages",
    "text": "R Packages\nR is, among other paradigms, a functional programming language, which means is heavily utilizes functions. R’s functions are stored in packages. While base R has a long list of very useful functions, to fully realize the power of R you will have to use additional packages. So, learning how to install packages (downloading from the web to your computer) and loading packages (making the package’s functions accessible to your current R session) are important skills to master.\n\n\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press."
  },
  {
    "objectID": "intro_R_programming.html",
    "href": "intro_R_programming.html",
    "title": "2  Introduction to R Programming",
    "section": "",
    "text": "Before we can start learning about advanced measurement theory, we need a basic understanding of the software we will use.\nTo understand why I use R, learn how to install it, and for an introduction to its use, go to https://www.statistical-thinking.com and follow the link in the left-hand column called “Introduction to R and RStudio” under the “Statistical Software” section. Read through the materials and watch the linked videos.\n\n\n\n\n\n\nUnder Contruction\n\n\n\nMany sections on the website referenced above are in draft form.\n\n\nWhen you have worked through that section of the website read through chapter 1 of the textbook “Introduction to the R Programming Language”. Make sure you try many of the coding examples within these resources. Don’t worry if you are having trouble with some tasks. The important thing is to come to the next class with good questions."
  },
  {
    "objectID": "ctt.html#measurement-in-science",
    "href": "ctt.html#measurement-in-science",
    "title": "3  Classic Test Theory",
    "section": "3.1 Measurement in Science",
    "text": "3.1 Measurement in Science\nMeasurement is the quantification of theoretical constructs by means of assigning labels or numbers to observation, in a systematic way. This is one way in which we simplify reality as a means to better understand it. The vast majority, maybe all, of the constructs we want to learn about are not directly measurable. When we make measurement, we inevitably must leave out some information about what we are observing, hence simplifying it.\nA very important issue in measurement is validity. Validity generally is the extent to which our measures reflect what we are attempting to measure, in a particular context. A similar concept is reliability, which deals with the consistency of our measures in a given context. If we were to take the same measurement of the same thing in the same context, we would expect to get the same measurement. To the extent that this is true, the measure is reliable. Note that to be valid an instrument must be reliable, but just because an instrument is reliable does not mean it is valid.\nThe concept of invariance relates to the extent to which scores on a measure are independent of examinee characteristics not relevant to the construct attempting to be measured. These characteristics can include things like gender, ethnicity, cultural background etc.\n\n3.1.1 Scales of Measurement\nThey way we quantify or classify constructs to generate measures can be classified by the scheme in the following figure.\n\n\n\n\nflowchart TB\n  A[Scales of Measurement] --> B[Qualitative/Categorical]\n  A --> C[Quantitative/Numeric]\n  B --> D[Nominal]\n  B --> E[Ordinal]\n  C --> F[Interval]\n  C --> G[Ratio]\n\n\n\n\n\n\n\n\nTo understand the scales of measurement, let’s use a data example. First, we will import a small data set of questions I ask students in some of my statistics classes.\n\n# File location on github:\nfile_location <- \"https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/data/student_survey.csv\"\n\n# Import data from csv file:\nstudent_survey <- read.csv(file = file_location,\n                           header = TRUE)\n# View raw data\nstudent_survey\n\n\n\n\n\n  \n\n\n\nYou can look at the codebook for this data below in section 3.5.1. We can see that all of the variables are coded as integers (see the <int> under the variable name in the data frame), with the exception of gender with is a character string (<chr>). But if you look over the variables, and read the variable descriptions in the codebook, you may realize that some of the variables are not best considered numeric. We will need to recode those variables. While we do that we will discuss scales of measurement.\nFor example, we can see that the sem variable quantifies the current semester (Spring, Fall, or Summer) for the student taking the survey. Here we table this variable\n\ntable(student_survey$sem)\n\n\n 1  2 \n 1 54 \n\n\nSemester is clearly either a nominal or ordinal scale of measurement. It could be ordinal because, in a given calendar year spring comes before summer which comes before fall. But for our purposes of this survey the ordering is not important, so we will ignore it for now and create a nominal variable. In R we do this with the factor() function as follows:\n\nstudent_survey$sem <- factor(student_survey$sem,\n                             levels = c(1:3),\n                             labels = c(\"Spring\", \"Fall\", \"Summer\"))\n\nThis code tells R to create an object in side of student_survey called sem. Because this object already exists here, this code will replace the existing object with the new one. Then the factor() function take an object as the first argument (student_survey$sem) which is the old object. So, essentially we are going take the old object turn it into a factor and replace the old object with the newly created factor. The next argument levels = c(1:3) tells R that the values of the original object are the integers 1, 2, and 3. Then, the labels = c(\"Spring\", \"Fall\", \"Summer\") argument maps the three character strings (“Spring”, “Fall”, “Summer”) onto the integers 1, 2, and 3. The ordering of the two vectors (1, 2, and 3 on the one hand and “Spring”, “Fall”, “Summer” on the other are important. “Spring” is mapped onto 1, “Fall” onto 2, and “Summer” onto 3. After doing this we can table this variable again and see what happened.\n\ntable(student_survey$sem)\n\n\nSpring   Fall Summer \n     1     54      0 \n\n\nWe can do something similar with the hand variable, which the codebook states captures the student’s handedness, and also is a nominal variable. But this time instead of saving the new variable over the old, I will create a new variable I will call handedness.\n\nstudent_survey$handedness <- factor(student_survey$hand, \n                                    levels = c(1,2),\n                                    labels = c(\"left\", \"right\"))\n\nThe major difference here is on the left side of the assignment operator (<-). Instead of using the same name of the original object hand, I gave it a new name handedness. Also note that in the levels argument, instead of the 1:2 shortcut I used c(1,2), does the same thing.\n\ntable(student_survey$handedness)\n\n\n left right \n    4    51 \n\n\nWe have two more nominal variables gender and course. Next, let’s recode gender. Because this variable contains character strings, which we can use as the labels, the code is simpler, we do not have to pass the levels or labels arguments.\n\nstudent_survey$gender <- factor(student_survey$gender)\n\n\ntable(student_survey$gender)\n\n\nfemale   male \n    16     39 \n\n\nOur final nominal variable is course, which measures which course the student taking the survey was enrolled. Because the labels are a bit more cumbersome, and to keep the code readable, we will first create a vector of the labels called lbls. Then we can use that vector in the factor() function. When we are done with the lbls object we will remove it with the rm() function. Finally, we will table the new variable.\n\n# Create temporary labels for course factor.\nlbls <- c(\"ERMA 7200 Basic Methods in Education Research\",\n          \"ERMA 7300 Design and Analysis I\",\n          \"ERMA 7310 Design and Analysis II\",\n          \"ERMA 8340 Advanced Psychometrics\")\nstudent_survey$course <- factor(student_survey$course, \n                                levels = c(1,2,3,4),\n                                labels = lbls)\nrm(lbls) # Remove labels object\n\ntable(student_survey$course)\n\n\nERMA 7200 Basic Methods in Education Research \n                                            0 \n              ERMA 7300 Design and Analysis I \n                                           55 \n             ERMA 7310 Design and Analysis II \n                                            0 \n             ERMA 8340 Advanced Psychometrics \n                                            0 \n\n\nOrdinal Variables are those that have a natural order but the interval between those variables is not necessary the same across the different values. It the student survey data an example is birth which measured the birth order of students.\n\nstudent_survey$birth <- ordered(student_survey$birth)\n\ntable(student_survey$birth)\n\n\n 1  2  3  4  5  6 \n25 10  8  5  1  1 \n\n\nThe last 20 variables of the student survey are question that ask about research and statistics. These are also measured as integers but should be ordinal variables. Creating a vector of labels as we did with the course variable, is also useful when you need to recode several variables with the same labels, such as in a set of variables that use the same Likert scale, as is the case for the Research and Statistics questions in the student survey. Below, we again create a object called lbls with the Likert labels. Then we create a vector of the column numbers that contain the Likert items, which are the 15th through the 31st columns, and name it cols. In R the square brackets are indexing functions and it allows us to use only a subset of the columns in the data frame. Then we use the lapply to repeat the factor() function for each of the Likert columns.\n\n# Likert labels\nlbls <- c(\"strongly disagree\", \"disagree\", \"neither agree/disagree\", \n          \"agree\", \"strongly agree\")\n\n# Column numbers containing Likert variables.\ncols <- 12:31\n\n# Use indexing to transform all Likert items to ordered factor.\nstudent_survey[ ,cols] <- lapply(student_survey[ ,cols], \n                               function(x) factor(x, \n                                                  levels = c(1,2,3,4,5),\n                                                  labels = lbls, \n                                                  ordered = TRUE))\n\nNote that to make a function ordered, which is the way to create ordinal variables in R, you pass the value TRUE to the ordered function. It will use the order of levels to order the values.\nHere is the new dataframe\n\nstudent_survey"
  },
  {
    "objectID": "ctt.html#classical-true-score-model",
    "href": "ctt.html#classical-true-score-model",
    "title": "3  Classic Test Theory",
    "section": "3.2 Classical True Score Model",
    "text": "3.2 Classical True Score Model\nThe true score model is: \\[\nX = T + E \\tag{1}\n\\] where \\(X\\) is the observed score, \\(T\\) is the true score, which is unknown, and \\(E\\) is the error\nFour assumptions to the model above:\n\n\\(E(X) = T\\), the expected value of the observed score \\(X\\) is the true score \\(T\\).\n\\(Cov(T,E) = 0\\), the true score ane error are independent(not correlated)\n\\(Cov(E_1, E)2 = 0\\), errors across test forms are independent.\n\\(Cov(E_1, T_2) = 0\\), error on one form of test is independent of the true score on another form.\n\nWhich leads to a re-expression of equation (1) above:\n\\[\n\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\n\\]\nTo demonstrate this let’s assume we have the following data 1 , which was generated to meet these assumptions.\n\n# Filepath to data on github. \nfilepath <- \"https://raw.githubusercontent.com/wmmurrah/AdvancedMeasurementTheoryNotebook/main/code/generateToy_CTTdata.R\"\nsource(filepath)\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau\n1   1    1  3  6  5  3  5  5  4  5  3   3   4\n2   1    2  6  3  5  3  4  2  4  4  3   5   4\n3   1    3  4  4  2  4  4  3  5  3  4   5   4\n4   2    1  3  6  8  6  5  4  5  5  5   5   5\n5   2    2  6  4  6  6  4  6  6  5  4   4   5\n6   2    3  4  6  6  5  5  5  1  3  6   4   5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6\n8   3    2  6  6  6  7  5  6  6  6  6   7   6\n9   3    3  6  5  8  6  6  6  7  7  5   7   6\n10  4    1  4  3  5  4  2  3  3  5  5   2   4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4\n12  4    3  2  4  4  4  6  4  3  4  5   4   4\n13  5    1  5  6  5  4  5  5  5  6  5   6   5\n14  5    2  6  6  4  6  4  5  4  5  5   5   5\n15  5    3  6  4  5  4  5  5  4  4  5   5   5\n16  6    1  6  6  7  8  6  6  7  6  6   4   6\n17  6    2  4  5  7  5  5  7  4  5  6   7   6\n18  6    3  5  6  6  6  4  5  4  5  7   6   6\n\n\nwhere id is a variable indicating individual test-takers, time indicated which of 3 times each individual was assessed, x1 - x10 are the scores on 10 items that comprise the test, and Tau is the true value of the individuals ability. I use Tau here instead of T, because T is a protected symbol in R which is short-hand for TRUE. Note that we would not know Tau in most situations, but because this is simulated data we will pretend we do.\nWe can create a composite score for the ten items for each individual on each occasion by averaging columns 3 through 12.\n\nCTTdata$X <- rowMeans(CTTdata[ ,3:12])\n\nAnd we can also create E, the error with:\n\nCTTdata$E <- CTTdata$X - CTTdata$Tau\n\nAgain, in practice we would not be able to directly compute E because we would not know Tau, but we will use it to build an understanding of what error is.\nNow we have:\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6\n\n\nLook over the last three columns and make sure you understand their relation. For example, in the first row, note that X is .2 points above Tau, which is exactly the value of E we computed (\\(X_1 - T_1 = E_1 = 4.2 - 4 = .2\\)). The 1 subscript in the previous expression indicated row 1 (i.e. i = 1).\n\nCTTdata$X_t <- round(ave(CTTdata$X, CTTdata$id, FUN = mean),1)\n\n\nCTTdata\n\n   id time x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 Tau   X    E X_t\n1   1    1  3  6  5  3  5  5  4  5  3   3   4 4.2  0.2 4.0\n2   1    2  6  3  5  3  4  2  4  4  3   5   4 3.9 -0.1 4.0\n3   1    3  4  4  2  4  4  3  5  3  4   5   4 3.8 -0.2 4.0\n4   2    1  3  6  8  6  5  4  5  5  5   5   5 5.2  0.2 4.9\n5   2    2  6  4  6  6  4  6  6  5  4   4   5 5.1  0.1 4.9\n6   2    3  4  6  6  5  5  5  1  3  6   4   5 4.5 -0.5 4.9\n7   3    1  6  5  6  6  6  6  9  6  6   5   6 6.1  0.1 6.2\n8   3    2  6  6  6  7  5  6  6  6  6   7   6 6.1  0.1 6.2\n9   3    3  6  5  8  6  6  6  7  7  5   7   6 6.3  0.3 6.2\n10  4    1  4  3  5  4  2  3  3  5  5   2   4 3.6 -0.4 4.0\n11  4    2  4  5  5  4  5  5  3  5  3   4   4 4.3  0.3 4.0\n12  4    3  2  4  4  4  6  4  3  4  5   4   4 4.0  0.0 4.0\n13  5    1  5  6  5  4  5  5  5  6  5   6   5 5.2  0.2 5.0\n14  5    2  6  6  4  6  4  5  4  5  5   5   5 5.0  0.0 5.0\n15  5    3  6  4  5  4  5  5  4  4  5   5   5 4.7 -0.3 5.0\n16  6    1  6  6  7  8  6  6  7  6  6   4   6 6.2  0.2 5.7\n17  6    2  4  5  7  5  5  7  4  5  6   7   6 5.5 -0.5 5.7\n18  6    3  5  6  6  6  4  5  4  5  7   6   6 5.4 -0.6 5.7"
  },
  {
    "objectID": "ctt.html#reliability",
    "href": "ctt.html#reliability",
    "title": "3  Classic Test Theory",
    "section": "3.3 Reliability",
    "text": "3.3 Reliability\n\\[\n\\text{reliability} = \\frac{\\sigma^2_T}{\\sigma^2_X} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E} = \\rho^2_{XT}\n\\]\nThe reliability is the proportion of variance of \\(T\\) in \\(X\\), which is also the squared correlation between \\(X\\) and \\(T\\).\n\nTau <- CTTdata$Tau\nX <- CTTdata$X\nE <- CTTdata$X - CTTdata$Tau\n\n\nvar(Tau)/var(X)\n\n[1] 0.9170806\n\n\n\nvar(Tau)/(var(Tau) + var(E))\n\n[1] 0.8898776\n\n\n\ncor(Tau, X)^2\n\n[1] 0.886766\n\n\n\nlibrary(hemp)\nsplit_half(CTTdata, type = \"alternate\")\n\n[1] 0.887\n\n\n\ncoef_alpha(CTTdata)\n\n[1] 0.894\n\n\n\nplot(x = CTTdata$Tau, y = CTTdata$id, xlim = c(1,10),\n     ylim = c(0,7))\npoints(x = CTTdata$X, y = jitter(CTTdata$id), pch = 3, col = \"red\")\npoints(x = ave(x = CTTdata$X, factor(CTTdata$id), FUN = mean), y = CTTdata$id, \n       col = \"blue\", pch = 18)\n\n points(x = CTTdata$X_t, pch = 2, factor(CTTdata$id))"
  },
  {
    "objectID": "ctt.html#sapa-example",
    "href": "ctt.html#sapa-example",
    "title": "3  Classic Test Theory",
    "section": "3.4 SAPA Example",
    "text": "3.4 SAPA Example\nIn this section I wil use data from the hemp package.\n\nlibrary(hemp)\ndata(\"SAPA\")\n\nTake a few minutes to look at the data description.\n\n?SAPA\n\nYou can explore individual items as follows:\n\nprop.table(table(SAPA$reason.4))\n\n\n        0         1 \n0.3598162 0.6401838 \n\nbarplot(prop.table(table(SAPA$reason.4)))\n\n\n\n\nYou can look at the proportion correct for all items.\n\n# Proportion correct for each item:\ncbind(proportion_correct = colMeans(SAPA, na.rm = TRUE))\n\n          proportion_correct\nreason.4           0.6401838\nreason.16          0.6981627\nreason.17          0.6973079\nreason.19          0.6152331\nletter.7           0.5997375\nletter.33          0.5712410\nletter.34          0.6132633\nletter.58          0.4439344\nmatrix.45          0.5259357\nmatrix.46          0.5498688\nmatrix.47          0.6139199\nmatrix.55          0.3740157\nrotate.3           0.1936967\nrotate.4           0.2127380\nrotate.6           0.2994091\nrotate.8           0.1850394\n\n\n\nnum_miss(SAPA)\n\n          num_miss perc_miss\nreason.4         2      0.13\nreason.16        1      0.07\nreason.17        2      0.13\nreason.19        2      0.13\nletter.7         1      0.07\nletter.33        2      0.13\nletter.34        2      0.13\nletter.58        0      0.00\nmatrix.45        2      0.13\nmatrix.46        1      0.07\nmatrix.47        2      0.13\nmatrix.55        1      0.07\nrotate.3         2      0.13\nrotate.4         2      0.13\nrotate.6         2      0.13\nrotate.8         1      0.07\n\n\n\n3.4.1 Reliability\n\nsplit_half(SAPA, type = \"alternate\")\n\n[1] 0.758\n\n\nThe split-half reliability coefficient is known to be downwardly biased. The Spearman-Brown formula can adjust for this. To get the Spearman-Brown reliability estimate use the following.\n\nsplit_half(SAPA, sb = TRUE)\n\n[1] 0.8623436\n\n\nWe might wish to estimate what length of test is needed to achieve a particular reliability.\n\ntest_length(SAPA, r = .95, r_type = \"split\")\n\n[1] 49\n\n\n\n3.4.1.1 Cronbach’s \\(\\alpha\\)\n\ncoef_alpha(SAPA)\n\n[1] 0.841\n\n\n\npsych::alpha(SAPA)\n\n\nReliability analysis   \nCall: psych::alpha(x = SAPA)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.84      0.84    0.85      0.25 5.3 0.006 0.49 0.25     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.83  0.84  0.85\nDuhachek  0.83  0.84  0.85\n\n Reliability if an item is dropped:\n          raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nreason.4       0.83      0.83    0.83      0.25 4.9   0.0064 0.0056  0.22\nreason.16      0.83      0.83    0.84      0.25 5.0   0.0063 0.0057  0.23\nreason.17      0.83      0.83    0.83      0.25 4.9   0.0064 0.0054  0.22\nreason.19      0.83      0.83    0.84      0.25 5.0   0.0063 0.0057  0.23\nletter.7       0.83      0.83    0.83      0.25 4.9   0.0064 0.0053  0.22\nletter.33      0.83      0.83    0.84      0.25 5.0   0.0063 0.0055  0.23\nletter.34      0.83      0.83    0.83      0.25 4.9   0.0064 0.0052  0.22\nletter.58      0.83      0.83    0.84      0.25 4.9   0.0064 0.0058  0.22\nmatrix.45      0.83      0.84    0.84      0.25 5.1   0.0062 0.0057  0.24\nmatrix.46      0.83      0.84    0.84      0.25 5.1   0.0062 0.0056  0.23\nmatrix.47      0.83      0.83    0.84      0.25 5.0   0.0063 0.0059  0.22\nmatrix.55      0.84      0.84    0.84      0.26 5.2   0.0061 0.0054  0.25\nrotate.3       0.83      0.83    0.83      0.25 5.0   0.0062 0.0042  0.24\nrotate.4       0.83      0.83    0.83      0.25 4.9   0.0063 0.0044  0.23\nrotate.6       0.83      0.83    0.83      0.25 4.9   0.0063 0.0049  0.23\nrotate.8       0.83      0.84    0.84      0.25 5.1   0.0062 0.0044  0.24\n\n Item statistics \n             n raw.r std.r r.cor r.drop mean   sd\nreason.4  1523  0.59  0.58  0.54   0.50 0.64 0.48\nreason.16 1524  0.53  0.53  0.48   0.45 0.70 0.46\nreason.17 1523  0.59  0.58  0.55   0.50 0.70 0.46\nreason.19 1523  0.56  0.55  0.51   0.47 0.62 0.49\nletter.7  1524  0.58  0.58  0.54   0.50 0.60 0.49\nletter.33 1523  0.56  0.55  0.50   0.46 0.57 0.50\nletter.34 1523  0.59  0.59  0.55   0.51 0.61 0.49\nletter.58 1525  0.58  0.57  0.53   0.49 0.44 0.50\nmatrix.45 1523  0.51  0.50  0.44   0.41 0.53 0.50\nmatrix.46 1524  0.52  0.50  0.45   0.42 0.55 0.50\nmatrix.47 1523  0.55  0.54  0.49   0.46 0.61 0.49\nmatrix.55 1524  0.45  0.44  0.37   0.34 0.37 0.48\nrotate.3  1523  0.51  0.53  0.50   0.43 0.19 0.40\nrotate.4  1523  0.56  0.58  0.55   0.48 0.21 0.41\nrotate.6  1523  0.55  0.57  0.53   0.47 0.30 0.46\nrotate.8  1524  0.48  0.51  0.46   0.40 0.19 0.39\n\nNon missing response frequency for each item\n             0    1 miss\nreason.4  0.36 0.64    0\nreason.16 0.30 0.70    0\nreason.17 0.30 0.70    0\nreason.19 0.38 0.62    0\nletter.7  0.40 0.60    0\nletter.33 0.43 0.57    0\nletter.34 0.39 0.61    0\nletter.58 0.56 0.44    0\nmatrix.45 0.47 0.53    0\nmatrix.46 0.45 0.55    0\nmatrix.47 0.39 0.61    0\nmatrix.55 0.63 0.37    0\nrotate.3  0.81 0.19    0\nrotate.4  0.79 0.21    0\nrotate.6  0.70 0.30    0\nrotate.8  0.81 0.19    0\n\n\nTo get bootstraped confidence intervals for the hemp coefficient alpha.\n\nlibrary(boot)\n\nalpha_fun <- function(data, row){\n  coef_alpha(data[row, ])\n}\n\nalpha_boot <- boot(data = SAPA, statistic = alpha_fun, \n                   R = 1e4)\nalpha_boot\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = SAPA, statistic = alpha_fun, R = 10000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1*    0.841 -0.0004028 0.005489842\n\n\n\nplot(alpha_boot)\n\n\n\n\n\nboot.ci(alpha_boot, type = \"bca\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = alpha_boot, type = \"bca\")\n\nIntervals : \nLevel       BCa          \n95%   ( 0.829,  0.851 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n3.4.2 Validity\nValidity is more complicated to estimate than reliability. One useful from of validity is criterion related validity which is assessed by looking at how closely aligned scores on the scale you are evaluating are with some other established measure of this construct.\n\ndata(\"interest\")\n\nprint(cor(interest[ ,c(\"vocab\", \"reading\", \"sentcomp\")]), digits= 2)\n\n         vocab reading sentcomp\nvocab     1.00    0.80     0.81\nreading   0.80    1.00     0.73\nsentcomp  0.81    0.73     1.00\n\n\n\n\n3.4.3 Item Analysis\nItem difficulty is an estimate of how hard a particular item is. A fairly straight-forward way to assess item difficulty is looking at the proportion of participants who answered each item correctly. If our items are score 0 or 1 for incorrect and correct answers respectively, we can calculate the column (item) means to get the proportion correct.\n\nitem_difficulty <- colMeans(SAPA, na.rm = TRUE)\nround(cbind(item_difficulty), digits = 2)\n\n          item_difficulty\nreason.4             0.64\nreason.16            0.70\nreason.17            0.70\nreason.19            0.62\nletter.7             0.60\nletter.33            0.57\nletter.34            0.61\nletter.58            0.44\nmatrix.45            0.53\nmatrix.46            0.55\nmatrix.47            0.61\nmatrix.55            0.37\nrotate.3             0.19\nrotate.4             0.21\nrotate.6             0.30\nrotate.8             0.19\n\n\nNote that a more intuitive name for this estimate would be item easiness, as the higher the number, the easier the item is. But we use item difficulty for historical reasons.\nWe can aslo calculate the item discrimination, which is a measure of how well an item discriminates between participants with high ability vs. those with low ability. The most common way to do this is to calcualte the point-biserial correlation between a participants score on an item and their total score.\n\ntotal_score <- rowSums(SAPA, na.rm = TRUE)\nitem_discrimination <- cor(SAPA,\n                           total_score,\n                           use = \"pairwise.complete.obs\")\n\nitem_discrimination\n\n               [,1]\nreason.4  0.5875787\nreason.16 0.5326660\nreason.17 0.5859068\nreason.19 0.5582773\nletter.7  0.5835910\nletter.33 0.5569431\nletter.34 0.5946924\nletter.58 0.5750172\nmatrix.45 0.5095047\nmatrix.46 0.5138256\nmatrix.47 0.5478686\nmatrix.55 0.4468619\nrotate.3  0.5100778\nrotate.4  0.5559848\nrotate.6  0.5542336\nrotate.8  0.4807175\n\n\nhigher values (closer to 1.00) mean the item has good discrimination, while values close to zero suggest little or not relation, and high negative numbers, suggest that people who do well on the rest of the instrument tend to do poorly in this item. This last situation often suggests something unintended is going on with the item or, said differently, the item is not “behaving” well.\nAnother way to calculate discrimination of items is to calculate the item discrimination index which splits the test takers into a high and low group based on their total score and then correlate this grouping variable with each item response.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .27)\n\nUpper 27% Lower 27% \n 0.805136  0.194864 \n\n\n\niri(SAPA)\n\n               [,1]\nreason.4  0.2820989\nreason.16 0.2451971\nreason.17 0.2692675\nreason.19 0.2717135\nletter.7  0.2865325\nletter.33 0.2757209\nletter.34 0.2897118\nletter.58 0.2863221\nmatrix.45 0.2544930\nmatrix.46 0.2562540\nmatrix.47 0.2668171\nmatrix.55 0.2161230\nrotate.3  0.2016459\nrotate.4  0.2276081\nrotate.6  0.2539219\nrotate.8  0.1867207\n\n\nFinally, for multiple choice tests, you may also want look at the distractors, which are the incorrect answers to such a question. This is done by looking at barplots or the proportion of test takers that answer each choice. If there are wrong choices that many test takers select, you may want to reconsider the distractors. There may be something confusing about the choices. If you have a popular distractor and a low discrimination measure, changing the distractor may help."
  },
  {
    "objectID": "ctt.html#r-scripts-and-data",
    "href": "ctt.html#r-scripts-and-data",
    "title": "3  Classic Test Theory",
    "section": "3.5 R Scripts and Data",
    "text": "3.5 R Scripts and Data\n\n3.5.1 Student Survey Data Codebook\n\n\nDocumentation for Student Survey data\n\nThis is a real data set from a survey given to graduate students in a statistics class.\n\nVariables\n\nDuration.inseconds. - length in seconds to complete survey.\n\ncourse  - Which course are you completing this survey for?\n          1 = ERMA 7200 Basic Methods in Education Research\n          2 = ERMA 7300 Design and Analysis I\n          3 = ERMA 7300 Design and Analysis II\n          4 = ERMA 8340 Advanced Psychometrics\n\nsem - Which is the current semester? 1 = Spring, 2 = Fall, 3 = Summer\n\nyear - What is the current year (enter 4 numerals. For example 2019)?\n\nexer - How many minutes do you exercise in a typical week?\n\nsoda - How much soda (in ounces) have you consumed in the last 24 hours?\n\ntvmin - How many minutes of television do you watch in a typical day?\n\nsiblings - How many siblings do you have?\n \nbirth - What is your birth order among you and your siblings?\n\ngender - what is your gender? 1 = male, 2 = female\n\nhand - Which is your dominant hand? 1 = left, 2 = right, 3 neither/both\n\nnumTVS - How many TVs do you own?\n\nResearch and Statistics Questions\n          1 = Strongly agree, 2 = Disagree, 3 = Neither agree/disagree, \n          4 = Agree, 5 = Strongly agree\n          \nstats_1 - Involvement in research will enhance my job/career opportunities.\n\nstats_2 - People I respect would approve of my involvement in research.       \n\nstats_3 - Research involvement will allow me to contribute to practitioners’ knowledge base.\n\nstats_4 - Doing research will increase my sense of self-worth.\n\nstats_5 - Becoming involved research will lead to the kind of career I most want.\n\nstats_6 - Research involvement is valued by significant people in my life.\n\nstats_7 - My peers think highly of me if I become involved in research.\n\nstats_8 - Research involvement will enable me to associate with the kind of people I value most.\n\nstats_9 - Involvement on a research team leads to close personal connections.\n\nstats_10 - Research involvement leads to a sense of satisfaction.\n\nstats_11 - Being involved in research contributes to my development as a professional.\n\nstats_12 - I believe research skills will be fruitful for my career.\n\nstats_13 - My involvement in research will lead to meaningful contributions to the field.\n\nstats _14 - Involvement in research will take time away from my significant relationships.\n\nstats_15 - Involvement in research takes time from leisure activities.\n\nstats_16 - Involvement in research helps me to understand the current issues in my profession.\n\nstats _17 - My analytical skills will become more developed because of my involvement in research activities.\n\nstats_18 - I believe that research involvement will lead to becoming well-known and respected in the field.\n\nstats_19 - Research involvement will lead to increased financial opportunities.\n\nstats_20 - Involvement in research will positively influence my applied skills.\n\n\n\n\n3.5.2 Simulating CTT data\n\n#------------------------------------------------------------------------\n# Title: simulate_CTTdata\n# Author: William Murrah\n# Description: Simulate data to demonstrate CTT and reliability\n# Created: Monday, 09 August 2021\n# R version: R version 4.1.0 (2021-05-18)\n# Project(working) directory: /Users/wmm0017/Projects/Courses/\n#   AdvancedMeasurementTheoryNotebook\n#------------------------------------------------------------------------\n\nsimx <- function(truescore, sigmax = 1) {\n  x <- rnorm(18, truescore, sigmax)\n  return(round(x))\n}\nid <- rep(1:6, each = 3)\nTau <- rep(rep(4:6, each = 3),2)\nset.seed(20210805)\nCTTdata <- data.frame(\n  id = id,\n  time = rep(1:3, 6),\n  x1 = simx(Tau),\n  x2 = simx(Tau),\n  x3 = simx(Tau),\n  x4 = simx(Tau),\n  x5 = simx(Tau),\n  x6 = simx(Tau),\n  x7 = simx(Tau),\n  x8 = simx(Tau),\n  x9 = simx(Tau),\n  x10 = simx(Tau),\n  Tau = Tau\n)\nrm(id, Tau, simx)"
  },
  {
    "objectID": "generalizability.html#one-facet-design",
    "href": "generalizability.html#one-facet-design",
    "title": "4  Generalizability Theory",
    "section": "4.1 One-Facet Design",
    "text": "4.1 One-Facet Design\nTo start with we will consider a simple design taken from chapter 3 of Desjardins and Bulut (2018), in which 30 participants were administered an executive functioning (EF) instrument, consisting of 10 dichotomously scored items (scored 0, or 1). In addition to the variation across participants, this design has one facet, which is item. In all there are three sources of variance in EF scores, one source due to participant (\\(\\sigma^2_p\\)), one source due to items (\\(\\sigma^2_i\\)) and one source the interaction of the two, which is also confounded with the residual of the model (\\(\\sigma^2_{pi,e}\\)). These are represented in ?eq-efvar, and graphically displayed in Figure 4.2.\n\\[\n\\sigma^2(X_{pi})= \\sigma^2_p + \\sigma^2_i + \\sigma^2_{pi,e}\n\\tag{4.5}\\]\n\n\n\n\n\nFigure 4.2: Venn diagram of one-facet G theory model.\n\n\n\n\nFirst, load the hypothetical data for this example from the hemp package.\n\nlibrary(hemp)\nlibrary(psych) # for descriptives and the headTail function\ndata(\"efData\") # from hemp package\n\nThee we can look at the data as follows:\n\nstr(efData)\n\n'data.frame':   350 obs. of  3 variables:\n $ Items       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Score       : int  0 1 1 1 1 1 1 1 1 1 ...\n $ Participants: int  1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"reshapeLong\")=List of 4\n  ..$ varying:List of 1\n  .. ..$ score: chr [1:10] \"item.1\" \"item.2\" \"item.3\" \"item.4\" ...\n  .. ..- attr(*, \"v.names\")= chr \"score\"\n  .. ..- attr(*, \"times\")= int [1:10] 1 2 3 4 5 6 7 8 9 10\n  ..$ v.names: chr \"score\"\n  ..$ idvar  : chr \"participant\"\n  ..$ timevar: chr \"item\"\n\nheadTail(efData)\n\n      Items Score Participants\n1.1       1     0            1\n2.1       1     1            2\n3.1       1     1            3\n4.1       1     1            4\n...     ...   ...          ...\n32.10    10     0           32\n33.10    10     1           33\n34.10    10     0           34\n35.10    10     0           35\n\n\nWe can see that this is a balanced design by tallying the number of items per participant.\n\naggregate(Items ~ Participants , data = efData, length)\n\n   Participants Items\n1             1    10\n2             2    10\n3             3    10\n4             4    10\n5             5    10\n6             6    10\n7             7    10\n8             8    10\n9             9    10\n10           10    10\n11           11    10\n12           12    10\n13           13    10\n14           14    10\n15           15    10\n16           16    10\n17           17    10\n18           18    10\n19           19    10\n20           20    10\n21           21    10\n22           22    10\n23           23    10\n24           24    10\n25           25    10\n26           26    10\n27           27    10\n28           28    10\n29           29    10\n30           30    10\n31           31    10\n32           32    10\n33           33    10\n34           34    10\n35           35    10\n\n\n\n4.1.1 G Study\nA G study can be conducted to estimate the three variance components. This is done using the lme4 package in R, which is automatically loaded when you load the hemp package (but could be loaded otherwise with library(lme4)). This package was developed to estimate linear mixed effects models. You can learn more about this package type ?lme4 in the R console and to learn more about the function used below type ?lmer. Briefly, the lmer function allows inclusion of fixed effects, like those included in standard regression models and most ANOVA models, along with random effects as done here. The random effects are each contained in a set of parentheses in the formula argument (e.g. (1 | Participant) and (1 | Items) below). We estimate a model and name it one_facet_model which estimates the scores on the EF instrument with two random effects, one for participant and one for items. Then we perform a G study with the gstudy() function from the hemp package and loo at the estimates of the three variance components.\n\none_facet_model <- lmer(Score ~ (1 | Participants) + (1 | Items), \n                        data = efData)\n\none_facet_gstudy <- gstudy(one_facet_model)\none_facet_gstudy\n\n        Source Est.Variance Percent.Variance\n1 Participants       0.0258             9.9%\n2        Items       0.0959            36.8%\n3     Residual       0.1387            53.3%\n\n\nA nice feature of the gstudy() function is the proportion of variances given in the output. We see that about 37% of the variance in EF scores are estimated to be due to the items, while only about 10% is due to differences across participants. These variances represent the magnitude of the error in generalizing from a participants score on a specific item of the EF instrument to that participants universe score. These components do not tell us the variance in generalizing based on an instrument with 10 items. To get that estimate we can do a D study and set the test length to 10 (see below). Note that the large residual error of 53% represents both the participant by item interaction and random error. We are unable to distinguish between these two sources of error.\n\nparticipant_means <- aggregate(Score ~ Participants, data = efData, mean)\ncolnames(participant_means) <- c(\"Participant\", \"Mean\")\nhist(participant_means$Mean, xlab = \"Proportion of Items Correct\",\n     main = \"\", breaks= 7)\n\n\n\nlattice::histogram(participant_means$Mean, type = \"count\", \n                   xlab = \"Proportion of Items Correct\")\n\n\n\n\n\nitem_means <- aggregate(Score ~ Items, efData, mean)\ncolnames(item_means) <- c(\"Item\", \"Mean\")\nitem_means\n\n   Item       Mean\n1     1 0.94285714\n2     2 0.68571429\n3     3 0.68571429\n4     4 0.08571429\n5     5 0.74285714\n6     6 0.77142857\n7     7 0.08571429\n8     8 0.60000000\n9     9 0.45714286\n10   10 0.11428571\n\n\n\n\n4.1.2 D Study\nOne purpose of a D study is to explore how manipulating the facets might impact the reliability of the instrument as a whole in the relevant context. For example, we mentioned above that the G study does not give us information about our instrument as a whole. If we want to estimate the reliability of this instrument with 10 items, which is what was done in this hypothetical study we could do the following:\n\none_facet_dstudy <- dstudy(one_facet_gstudy, unit = \"Participants\", n = c(\"Items\" = 10))\none_facet_dstudy\n\n        Source Est.Variance   N Ratio of Var:N\n1 Participants       0.0258 350        0.02580\n2        Items       0.0959  10        0.00959\n3     Residual       0.1387  10        0.01387\n\nThe generalizability coefficient is: 0.6503655.\nThe dependability coefficient is: 0.5237515.\n\n\nThe output of the D study contains three types of information. First, the estimated variances of each source of variance (e.g. Participant, Item, and Residual) are given. Second, the generalizability coefficient, which is analogous to the reliability coefficient in CTT is given. Here is is estimated at .65, which is fairly low.\nWe might also want to determine what impact the number of items has on the reliability of the instrument. That way we can estimate the impact of adding more items.\n\ndstudy_plot(one_facet_gstudy, unit = \"Participants\", \n            facets = list(Items = c(10, 20, 30, 40, 50, 60)),\n            g_coef = FALSE)"
  },
  {
    "objectID": "generalizability.html#two-facet-crossed-design",
    "href": "generalizability.html#two-facet-crossed-design",
    "title": "4  Generalizability Theory",
    "section": "4.2 Two-Facet Crossed Design",
    "text": "4.2 Two-Facet Crossed Design"
  },
  {
    "objectID": "generalizability.html#additional-readings",
    "href": "generalizability.html#additional-readings",
    "title": "4  Generalizability Theory",
    "section": "4.3 Additional Readings",
    "text": "4.3 Additional Readings\nFor more information of G theory, see Raykov and Marcoulides (2011). For an example using the R package lavaan with G theory, see Jorgensen (2021).\n\n\n\n\nBrennan, Robert L. 2010. “Generalizability Theory and Classical Test Theory.” Applied Measurement in Education 24 (1): 1–21. https://doi.org/10.1080/08957347.2011.532417.\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press.\n\n\nJorgensen, Terrence D. 2021. “How to Estimate Absolute-Error Components in Structural Equation Models of Generalizability Theory.” Psych 3 (2): 113–33. https://doi.org/10.3390/psych3020011.\n\n\nRaykov, Tenko, and George A Marcoulides. 2011. Introduction to Psychometric Theory. Routledge."
  },
  {
    "objectID": "efa.html#the-common-factor-model",
    "href": "efa.html#the-common-factor-model",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.1 The Common Factor Model",
    "text": "5.1 The Common Factor Model\nIn the common factor model, variances can be partitioned into common variances and unique variances. Common variances are those due to the common factor, while unique variances include all sources of variance not attributable to the common factor. Unique variance can be decomposed into specific and error variances. Specific variance is due to systematic sources of variability that are unique to the observed indicator. Error variance is due to random measurement variance.\n\\[\nx = \\tau + \\varepsilon\n\\tag{5.1}\\]\n\\[\n\\sigma^2_x = \\sigma^2_T + \\sigma^2_{\\varepsilon}\n\\tag{5.2}\\]\nwhich we can directly compare to the common factor model we will learn about in this chapter:\n\\[\nx = \\mu + \\Lambda f + \\varepsilon\n\\tag{5.3}\\]\n\\[\nVar(x) = \\Lambda \\Phi \\Lambda^T + \\psi = \\Sigma(\\theta)\n\\tag{5.4}\\]\n\n5.1.1 Think about these situations\nWhat do you do when you have a large number of variables you are considering as predictors of a dependent variable?\n\nOften, subsets of these variables are measuring the same, or very similar things.\nWe might like to reduce the variables to a smaller number of predictors.\n\nWhat if you are developing a measurement scale and have a large number of items you think measure the same construct\n\nYou might want to see how strongly the items are related to the construct.\n\nTo make sense of these abstract goals, and to help explain the procedures of EFA in this chapter, I will use an example from chapter 4 of Desjardins and Bulut (2018).\nThis example uses as subset of the interest data which fictitious survey data on cognitive, personality and interest items. A subset of items related to cognition will be used here to follow the description in that text.\nFirst, load the hemp package and create a data frame called cognition.\n\nlibrary(hemp)\ncognition <- subset(interest, select = vocab:analyrea)\n\nThen look at a summary of the data.\n\nsummary(cognition)\n\n     vocab             reading           sentcomp           mathmtcs      \n Min.   :-2.62000   Min.   :-2.4700   Min.   :-2.47000   Min.   :-3.7100  \n 1st Qu.:-0.60500   1st Qu.:-0.5175   1st Qu.:-0.55000   1st Qu.:-0.4925  \n Median : 0.04000   Median : 0.1850   Median : 0.10500   Median : 0.1000  \n Mean   : 0.09016   Mean   : 0.1350   Mean   : 0.07356   Mean   : 0.1055  \n 3rd Qu.: 0.86000   3rd Qu.: 0.7975   3rd Qu.: 0.77500   3rd Qu.: 0.9200  \n Max.   : 2.63000   Max.   : 2.7000   Max.   : 2.73000   Max.   : 3.0600  \n    geometry          analyrea      \n Min.   :-3.3200   Min.   :-2.8300  \n 1st Qu.:-0.5600   1st Qu.:-0.4825  \n Median : 0.0900   Median : 0.2000  \n Mean   : 0.1125   Mean   : 0.1750  \n 3rd Qu.: 0.7675   3rd Qu.: 0.8375  \n Max.   : 3.8600   Max.   : 3.5000  \n\n\nLooking at the output, we see that the means of these variables are close to zero, with the spread of the data being roughly symmetrical around the mean, suggesting that these variables may have been standardized, which entails transforming them into z-scores. If this is the case, we would expect the variances (and standard deviations) to be about 1. We can look at the variances of each of the variables with the combination of the apply() and var() functions. The apply() function allows us to repeat another function over either the rows or columns of a two dimensional data object, such as a data frame. The MARGIN = 2, argument tells the function to appply var() over the columns (MARGIN = 1 would use rows). And the FUN = var argument identifies the function to apply over columns. Note the absence of parentheses after the function var in the call.\n\napply(cognition, MARGIN = 2, FUN = var)\n\n    vocab   reading  sentcomp  mathmtcs  geometry  analyrea \n0.9966514 0.9811568 0.9834142 1.1117325 1.0686631 1.1170926 \n\n\nThis supports our suspicion of standardized variables. It is often good to have items on the same or similar scales for factor analysis.\nThe cognition data are organized so that each individual is represented by one row, and their score on each item is contained in a separate column. This format is often called the wide format. To conduct factor analysis (and many other types of analysis) the long format is needed. Instead of separate columns containing the scores for each item, the long format would have all scores in one column and a categorical variable that captures which item each score represents. Therefore, each individual would be represented by multiple rows, one for each item, hence the long in long format.\nBelow, the reshape() function from base R is used to reshape the data from wide to long format, and store this new data frame in an object named cognition_l (note that the lowercase letter L is appended to the name, not the number 1). This function takes the cognition data (data = cognition), indicated that the first through the sixth columns are the ones to convert (varying = 1:6), names the variable that will contain the cell values from the original data as “score” (v.names = \"score\"), names the new character variable that describes which column the scores came from as “indicator” (timevar = \"indicator\"), uses the column names of those columns as the what will become the labels in the new categorical variable, which is a factor in R (times = names(cognition)), and finally tells the function to tranform the data into the “long” format (direction = \"long\"). The new To learn more about this function you can type ?reshape in to the R console. This transformation create the indicator variable as a charater variable, so we also convert it into a factor.\n\ncognition_l <- reshape(data = cognition,\n                       varying = 1:6,\n                       v.names = \"score\",\n                       timevar = \"indicator\",\n                       times = names(cognition),\n                       direction = \"long\")\n\n# Convert \"indicator\" into factor using the values as labels\ncognition_l$indicator <- factor(cognition_l$indicator)\n\nLooking at the first few rows, we can see that this data indeed has one column for the numeric values in the cells of the original data, and a factor variable containing the indicator type.\n\nhead(cognition_l[order(cognition_l$id), ])\n\n           indicator score id\n1.vocab        vocab  1.67  1\n1.reading    reading  1.67  1\n1.sentcomp  sentcomp  1.46  1\n1.mathmtcs  mathmtcs  0.90  1\n1.geometry  geometry  0.49  1\n1.analyrea  analyrea  1.65  1\n\n\nWith this long data frame, we can look at the univariate distribution of the 6 indicators. Below I show how to do that with the lattice package and the ggplot2 package.\n\nlibrary(lattice)\nhistogram(~ score | indicator, data = cognition_l)\n\n\n\n\n\nlibrary(ggplot2)\nggplot(cognition_l, aes(x = score)) + geom_histogram() +\n  facet_wrap(~ indicator )\n\n\n\n\nFrom these plots it seems reasonable to assume these variables are normally distributed, and we also see that these indicators have similar variances and are centered around zero, consistent with them being standardized.\nNext we can explore bivariate relations between the indicators. Before calculating correlation coefficients, it is helpful to plot the bivariate relations to evalute the assumptions of correlation coefficients as well as for the factor analysis. The pairs() function is useful for this.\n\npairs(cognition)\n\n\n\n\n\n\n5.1.2 Correlation Coefficient\nPearson product-moment correlation:\n\\[\nr_{xy} = \\frac{\\Sigma_{n=1}^n (x_k - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{n=1}^n(x_i - \\bar{x})^2} \\sqrt{\\Sigma_{n=1}^n(y_i - \\bar{y})^2}} = \\frac{s_{xy}}{s_x s_y}.\n\\]\nThe equation looks very daunting, until you see that it is just the covariance of \\(x\\) and \\(y\\) divided by the product of their standard deviations.\n\ncorrelations <- cor(cognition)\nround(correlations, 3)\n\n         vocab reading sentcomp mathmtcs geometry analyrea\nvocab    1.000   0.803    0.813    0.708    0.633    0.673\nreading  0.803   1.000    0.725    0.660    0.526    0.636\nsentcomp 0.813   0.725    1.000    0.618    0.575    0.618\nmathmtcs 0.708   0.660    0.618    1.000    0.774    0.817\ngeometry 0.633   0.526    0.575    0.774    1.000    0.715\nanalyrea 0.673   0.636    0.618    0.817    0.715    1.000\n\n\n\ncor_diff <- correlations - cor(cognition[-c(202, 53, 111), ])\nround(cor_diff, 3)\n\n         vocab reading sentcomp mathmtcs geometry analyrea\nvocab    0.000   0.005    0.009    0.000    0.021    0.009\nreading  0.005   0.000    0.010    0.016    0.025    0.021\nsentcomp 0.009   0.010    0.000    0.009    0.027    0.014\nmathmtcs 0.000   0.016    0.009    0.000   -0.004    0.005\ngeometry 0.021   0.025    0.027   -0.004    0.000    0.015\nanalyrea 0.009   0.021    0.014    0.005    0.015    0.000\n\n\n\nbollen_plot(cognition, crit.value = 0.06)\n\n\n\n\n\ncognition[c(202, 53, 111), ]\n\n    vocab reading sentcomp mathmtcs geometry analyrea\n202  2.63    2.23     2.55     1.38     3.86     3.50\n53  -0.38    0.99    -0.50     1.79    -0.19     2.13\n111 -2.01   -2.47    -2.47    -3.71    -2.59    -2.67\n\n\n\napply(cognition, 2, min)\n\n   vocab  reading sentcomp mathmtcs geometry analyrea \n   -2.62    -2.47    -2.47    -3.71    -3.32    -2.83 \n\napply(cognition, 2, max)\n\n   vocab  reading sentcomp mathmtcs geometry analyrea \n    2.63     2.70     2.73     3.06     3.86     3.50"
  },
  {
    "objectID": "efa.html#solutions",
    "href": "efa.html#solutions",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.2 Solutions",
    "text": "5.2 Solutions\n\nPrincipal Components Analysis\n\ntransforming the original variables into a new set of linear combinations (pricipal components).\n\nFactor Analysis\n\nsetting up a mathematical model to estimate the number or factors\n\n\n\n5.2.1 Principal Components Analysis\n\nConcerned with explaining variance-covariance structure of a set of variables.\nPCA attempts to explain as much of the total variance among the observed variables as possible with a smaller number of components.\nBecause the variables are standardized prior to analysis, the total amount of variance available is the number of variables.\nThe goal is data reduction for subsequent analysis.\nVariables cause components.\nComponents are not representative of any underlying theory.\n\n\n\n5.2.2 Factor Analysis\n\nThe goal is understanding underlying constructs.\nUses a modified correlation matrix (reduced matrix)\nfactors cause the variables.\nFactors represent theoretical constructs.\nFocuses on the common variance of the variables, and purges the unique variance.\n\n\n\n5.2.3 Steps in Factor Analysis\n\nChoose extraction method\n\nSo far we’ve focused on PCA\nEFA is often preferred if you are developing theory\n\nDetermine the number of components/factors\n\nKaiser method: eigenvalues > 1\nScree plot: All components before leveling off\nHorn’s parallel analysis: components/factors greater than simulated values from random numbers\n\nRotate Factors\n\nOrthogonal\nOblique\n\nInterpret Components/Factors\n\n\n\n5.2.4 “Little Jiffy” method of factor analysis\n\nExtraction method : PCA\nNumber of factors: eigenvalues > 1\nRotation: orthogonal(varimax)\nInterpretation\n\nFollowing these steps without thought can lead to many problems (see Preacher and MacCallum 2003)\n\n\n5.2.5 Eigenvalues\nEigenvalues represent the variance in the variables explained by the success components.\n\n\n5.2.6 Determining the Number of Factors\n\nKaiser criterion: Retain only factors with eigenvalues > 1. (generally accurate)\nScree plot: plot eigenvalues and drop factors after leveling off.\nParallel analysis: compare observed eigenvalues to parallel set of data from randomly generated data. Retain factors in original if eigenvalue is greater than random eigenvalue.\nFactor meaningfulness is also very important to consider.\n\n\n5.2.6.1 Kaiser\nRetain factors with eigenvalues greater than 1\n\neigen_decomp <- eigen(correlations)\nround(eigen_decomp$values, 3)\n\n[1] 4.436 0.676 0.322 0.245 0.168 0.152\n\n\n\n\n5.2.6.2 Scree Plot\n\nscree(cognition, pc = FALSE)\n\n\n\n\n\n\n5.2.6.3 Horn’s Parallel Analysis\n\nfa.parallel(cognition, fm = \"ml\")\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  1 \n\n\nUsing these conventions we can rewrite the classic test score model as:\n\nprincipal(correlations)\n\nPrincipal Components Analysis\nCall: principal(r = correlations)\nStandardized loadings (pattern matrix) based upon correlation matrix\n          PC1   h2   u2 com\nvocab    0.90 0.81 0.19   1\nreading  0.84 0.71 0.29   1\nsentcomp 0.84 0.71 0.29   1\nmathmtcs 0.89 0.79 0.21   1\ngeometry 0.82 0.67 0.33   1\nanalyrea 0.86 0.75 0.25   1\n\n                PC1\nSS loadings    4.44\nProportion Var 0.74\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.98\n\n\n\none_factor <- fa(r = cognition, nfactors = 1, rotate = \"oblimin\")\none_factor\n\nFactor Analysis using method =  minres\nCall: fa(r = cognition, nfactors = 1, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n          MR1   h2   u2 com\nvocab    0.89 0.79 0.21   1\nreading  0.81 0.65 0.35   1\nsentcomp 0.80 0.65 0.35   1\nmathmtcs 0.87 0.76 0.24   1\ngeometry 0.77 0.59 0.41   1\nanalyrea 0.84 0.70 0.30   1\n\n                MR1\nSS loadings    4.13\nProportion Var 0.69\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  15  with the objective function =  5.11 with Chi Square =  1257.24\ndf of  the model are 9  and the objective function was  0.71 \n\nThe root mean square of the residuals (RMSR) is  0.07 \nThe df corrected root mean square of the residuals is  0.1 \n\nThe harmonic n.obs is  250 with the empirical chi square  41.32  with prob <  4.4e-06 \nThe total n.obs was  250  with Likelihood Chi Square =  173.3  with prob <  1.3e-32 \n\nTucker Lewis Index of factoring reliability =  0.779\nRMSEA index =  0.27  and the 90 % confidence intervals are  0.236 0.307\nBIC =  123.61\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1\nCorrelation of (regression) scores with factors   0.97\nMultiple R square of scores with factors          0.94\nMinimum correlation of possible factor scores     0.87\n\n\n\ntwo_factor <- fa(r = cognition, nfactors = 2, rotate = \"oblimin\")\ntwo_factor\n\nFactor Analysis using method =  minres\nCall: fa(r = cognition, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n           MR1   MR2   h2   u2 com\nvocab     0.95  0.00 0.90 0.10   1\nreading   0.83  0.02 0.72 0.28   1\nsentcomp  0.87 -0.02 0.74 0.26   1\nmathmtcs -0.03  0.96 0.89 0.11   1\ngeometry -0.02  0.84 0.68 0.32   1\nanalyrea  0.07  0.81 0.76 0.24   1\n\n                       MR1  MR2\nSS loadings           2.36 2.31\nProportion Var        0.39 0.38\nCumulative Var        0.39 0.78\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n With factor correlations of \n    MR1 MR2\nMR1 1.0 0.8\nMR2 0.8 1.0\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  15  with the objective function =  5.11 with Chi Square =  1257.24\ndf of  the model are 4  and the objective function was  0.05 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  250 with the empirical chi square  1.42  with prob <  0.84 \nThe total n.obs was  250  with Likelihood Chi Square =  11.64  with prob <  0.02 \n\nTucker Lewis Index of factoring reliability =  0.977\nRMSEA index =  0.087  and the 90 % confidence intervals are  0.031 0.148\nBIC =  -10.44\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.97\nMultiple R square of scores with factors          0.94 0.94\nMinimum correlation of possible factor scores     0.88 0.87\n\n\n\n\n\n5.2.7 EFA with Categorical Data\n\nSAPA_subset <- subset(SAPA, select = c(letter.7:letter.58,\n                                       rotate.3:rotate.8))\n\nfa.parallel(SAPA_subset, cor = \"poly\")\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\nEFA_SAPA <- fa(r = SAPA_subset, nfactors = 2, rotate = \"oblimin\",\n               cor = \"poly\")\nEFA_SAPA\n\nFactor Analysis using method =  minres\nCall: fa(r = SAPA_subset, nfactors = 2, rotate = \"oblimin\", cor = \"poly\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n            MR1   MR2   h2   u2 com\nletter.7  -0.02  0.79 0.60 0.40 1.0\nletter.33  0.01  0.70 0.50 0.50 1.0\nletter.34 -0.02  0.80 0.63 0.37 1.0\nletter.58  0.21  0.54 0.46 0.54 1.3\nrotate.3   0.86 -0.02 0.72 0.28 1.0\nrotate.4   0.82  0.10 0.77 0.23 1.0\nrotate.6   0.77  0.05 0.64 0.36 1.0\nrotate.8   0.86 -0.09 0.65 0.35 1.0\n\n                       MR1  MR2\nSS loadings           2.84 2.13\nProportion Var        0.36 0.27\nCumulative Var        0.36 0.62\nProportion Explained  0.57 0.43\nCumulative Proportion 0.57 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.56\nMR2 0.56 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  28  with the objective function =  4.26 with Chi Square =  6477.4\ndf of  the model are 13  and the objective function was  0.06 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  1523 with the empirical chi square  22.69  with prob <  0.046 \nThe total n.obs was  1525  with Likelihood Chi Square =  97.39  with prob <  5.3e-15 \n\nTucker Lewis Index of factoring reliability =  0.972\nRMSEA index =  0.065  and the 90 % confidence intervals are  0.053 0.078\nBIC =  2.1\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.95 0.92\nMultiple R square of scores with factors          0.91 0.84\nMinimum correlation of possible factor scores     0.81 0.68"
  },
  {
    "objectID": "efa.html#another-example",
    "href": "efa.html#another-example",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.3 Another Example",
    "text": "5.3 Another Example\n\nlibrary(\"MPsychoR\")\ndata(\"YouthDep\")\nitem1 <- YouthDep[, 1]\nlevels(item1) <- c(\"0\", \"1\", \"1\")\nitem2 <- YouthDep[, 14]\nlevels(item2) <- c(\"0\", \"1\", \"1\")\ntable(item1, item2)\n\n     item2\nitem1    0    1\n    0 1353  656\n    1  115  166\n\n\n\n## ------ correlation coefficients\nlibrary(\"psych\")\ntetcor <- tetrachoric(cbind(item1, item2))\ntetcor\n\nCall: tetrachoric(x = cbind(item1, item2))\ntetrachoric correlation \n      item1 item2\nitem1 1.00       \nitem2 0.35  1.00 \n\n with tau of \nitem1 item2 \n 1.16  0.36 \n\nitem1 <- YouthDep[, 1]\nitem2 <- YouthDep[, 14]\npolcor <- polychoric(cbind(item1, item2))\npolcor\n\nCall: polychoric(x = cbind(item1, item2))\nPolychoric correlations \n      item1 item2\nitem1 1.00       \nitem2 0.33  1.00 \n\n with tau of \n         1   2\nitem1 1.16 2.3\nitem2 0.36 1.2\n\ndraw.tetra(r = .35, t1 = 1.16, t2 = .36)\n\n\n\nDepItems <- YouthDep[,1:26] \nDepnum <- data.matrix(DepItems) - 1  ## convert to numeric   \nRdep <- polychoric(Depnum)\n\n\n5.3.1 Example data\n\nlower <- \"\n1.00\n0.70 1.00\n0.65 0.66 1.00\n0.62 0.63 0.60 1.00\n\"\ncormat <- getCov(lower, names = c(\"d1\", \"d2\", \"d3\", \"d4\"))\n\ncormat\n\n     d1   d2   d3   d4\nd1 1.00 0.70 0.65 0.62\nd2 0.70 1.00 0.66 0.63\nd3 0.65 0.66 1.00 0.60\nd4 0.62 0.63 0.60 1.00\n\n\n\n\n5.3.2 Kaiser\nRetain factors with eigenvalues greater than 1\n\neigen(cormat)$values\n\n[1] 2.9311792 0.4103921 0.3592372 0.2991916\n\n\n\n\n5.3.3 Scree Plot\n\nscree(cormat, factors = FALSE)\n\n\n\n\n\n\n5.3.4 Horn’s Parallel Analysis\n\nfa.parallel(cormat, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1"
  },
  {
    "objectID": "efa.html#another-example-1",
    "href": "efa.html#another-example-1",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.4 Another example",
    "text": "5.4 Another example\n\nfa.parallel(Harman74.cor$cov, fa = \"pc\")\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n\n\n5.4.1 Rotation\n\nPrincipal components are derived to maximize the variance accounted for (data reduction).\nRotation is done to make the factors more interpretable (i.e. meaningful).\nTwo major classes of rotation:\n\nOrthogonal - new factors are still uncorrelated, as were the initial factors.\nOblique - new factors are allowed to be correlated.\n\n\nEssentially reallocates the loadings. The first factor may not be the one accounting for the most variance.\n\n\n5.4.2 Orthogonal Rotation\n\nQuartimax - idea is to clean up the variables. Rotation done so each variable loads mainly on one factor. Problematic if there is a general factor on which most or all variables load on (think IQ).\nVarimax - to clean up factors. So each factor has high correlation with a smaller number of variables, low correlation with the other variables. Generally makes interpretation easier.\n\n\n\n5.4.3 Oblique Rotation\n\nOften correlated factors are more reasonable.\nTherefore, oblique rotation is often preferred.\nBut interpretation is more complicated.\n\n\n\n5.4.4 Factor Matrices\n\nFactor pattern matrix:\n\nincludes pattern coefficients analogous to standardized partial regression coefficients.\nIndicated the unique importance of a factor to a variable, holding other factors constant.\n\nFactor structure matrix:\n\nincludes structure coefficients which are simple correlations of the variables with the factors.\n\n\n\n\n5.4.5 Which matrix should we interpret?\n\nWhen orthogonal rotation is used interpret structural coefficients (but they are the same as pattern coefficients).\nWhen oblique rotation is used pattern coefficients are preferred because they account for the correlation between the factors and they are parameters of the correlated factor model (which we will discuss next class).\n\n\n\n5.4.6 Which variables should be used to interpret each factor?\n\nThe idea is to use only those variables that have a strong association with the factor.\nTypical thresholds are |.30| or |.40|.\nContent knowledge is critical."
  },
  {
    "objectID": "efa.html#tom-swifts-electric-factor-analysis-factory",
    "href": "efa.html#tom-swifts-electric-factor-analysis-factory",
    "title": "5  Exploratory Factor Analysis",
    "section": "5.5 Tom Swift’s Electric Factor Analysis Factory",
    "text": "5.5 Tom Swift’s Electric Factor Analysis Factory\n\n5.5.1 Steps in Factor Analysis\n\nChoose extraction method\n\nSo far we’ve focused on PCA\n\nDetermine the number of components/factors\n\nKaiser method: eigenvalues > 1\nScree plot: All components before leveling off\nHorn’s parallel analysis: components/factors greater than simulated values from random numbers\n\nRotate Factors\n\nOrthogonal\nOblique\n\nInterpret Components/Factors\n\n\n\n5.5.2 “Little Jiffy” method of factor analysis\n\nExtraction method : PCA\nNumber of factors: eigenvalues > 1\nRotation: orthogonal(varimax)\nInterpretation\n\n\n\n5.5.3 Metal Boxes\n\n\n\nFunctional Definitions of Tom Swift’s Original 11 Variables\n\n\nDimension\nDerivation\n\n\n\n\nThickness\nx\n\n\nWidth\ny\n\n\nLength\nz\n\n\nVolume\nxyz\n\n\nDensity\nd\n\n\nWeight\nxyzd\n\n\nSurface area\n2(xy + xz + yz)\n\n\nCross-section\nyz\n\n\nEdge length\n4(x + y + z)\n\n\nDiagonal length\n(x^2)\n\n\nCost/lb\nc\n\n\n\n\n\n\n\n'data.frame':   63 obs. of  11 variables:\n $ thick   : num  1.362 1.83 0.567 1.962 1.762 ...\n $ width   : num  1.71 4.01 1.86 1.71 1.95 ...\n $ length  : num  4.93 5.2 4.31 3.91 4.1 ...\n $ volume  : num  10.02 39.59 8.02 16.02 15.92 ...\n $ density : int  17 6 4 14 7 4 1 1 14 8 ...\n $ weight  : num  170 240.1 32.1 224 111.6 ...\n $ surface : num  34 76.1 28.1 39.5 40.4 ...\n $ crosssec: num  9.87 19.94 7.94 8.22 8.24 ...\n $ edge    : num  31.9 43.9 27.9 31.5 31.7 ...\n $ diagonal: num  900 2025 441 576 576 ...\n $ cost    : num  9.73 14.13 -2.92 6.78 7.62 ...\n\n\n\n\n5.5.4 Correlations\n\n\n\nCorrelations between dimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthick\nwidth\nlength\nvolume\ndensity\nweight\nsurface\ncrosssec\nedge\ndiagonal\ncost\n\n\n\n\nthick\n1.00\n0.43\n0.28\n0.78\n-0.16\n0.53\n0.69\n0.40\n0.60\n0.48\n-0.01\n\n\nwidth\n0.43\n1.00\n0.50\n0.78\n-0.10\n0.54\n0.86\n0.88\n0.84\n0.74\n0.04\n\n\nlength\n0.28\n0.50\n1.00\n0.60\n0.13\n0.54\n0.73\n0.82\n0.82\n0.87\n0.22\n\n\nvolume\n0.78\n0.78\n0.60\n1.00\n-0.09\n0.69\n0.97\n0.81\n0.90\n0.86\n0.07\n\n\ndensity\n-0.16\n-0.10\n0.13\n-0.09\n1.00\n0.53\n-0.05\n0.01\n-0.02\n0.04\n0.83\n\n\nweight\n0.53\n0.54\n0.54\n0.69\n0.53\n1.00\n0.71\n0.64\n0.69\n0.68\n0.55\n\n\nsurface\n0.69\n0.86\n0.73\n0.97\n-0.05\n0.71\n1.00\n0.93\n0.97\n0.92\n0.11\n\n\ncrosssec\n0.40\n0.88\n0.82\n0.81\n0.01\n0.64\n0.93\n1.00\n0.95\n0.94\n0.15\n\n\nedge\n0.60\n0.84\n0.82\n0.90\n-0.02\n0.69\n0.97\n0.95\n1.00\n0.92\n0.14\n\n\ndiagonal\n0.48\n0.74\n0.87\n0.86\n0.04\n0.68\n0.92\n0.94\n0.92\n1.00\n0.17\n\n\ncost\n-0.01\n0.04\n0.22\n0.07\n0.83\n0.55\n0.11\n0.15\n0.14\n0.17\n1.00\n\n\n\n\n\n\n\n5.5.5 Eigenvalues > 1\n\n\n\n\n\n\n\n5.5.6 Orthogonal Rotation\n\n\n\nLoadings:\n         RC1    RC4    RC3    RC2    RC5   \nthick                   0.962              \nwidth            0.926                     \nlength    0.955                            \nvolume                                     \ndensity                        0.942       \nweight                                     \nsurface                                    \ncrosssec         0.705                     \nedge                                       \ndiagonal  0.775                            \ncost                           0.960       \n\n                 RC1   RC4   RC3   RC2   RC5\nSS loadings    3.165 2.913 2.226 2.193 0.263\nProportion Var 0.288 0.265 0.202 0.199 0.024\nCumulative Var 0.288 0.552 0.755 0.954 0.978\n\n\n\n\n5.5.7 Orthogonal Rotation with Loadings > .70\n\n\n\nLoadings:\n         RC1    RC3    RC2   \nthick            0.937       \nwidth     0.795              \nlength    0.885              \nvolume    0.709              \ndensity                 0.962\nweight                       \nsurface   0.843              \ncrosssec  0.967              \nedge      0.903              \ndiagonal  0.923              \ncost                    0.933\n\n                 RC1   RC3   RC2\nSS loadings    5.564 2.265 2.233\nProportion Var 0.506 0.206 0.203\nCumulative Var 0.506 0.712 0.915\n\n\n\n\n\n\nDesjardins, Christopher D, and Okan Bulut. 2018. Handbook of Educational Measurement and Psychometrics Using r. CRC Press.\n\n\nPreacher, Kristopher J., and Robert C. MacCallum. 2003. “Repairing Tom Swifts electric Factor Analysis Machine.” Understanding Statistics 2 (1): 13–43. https://doi.org/10.1207/s15328031us0201_02."
  },
  {
    "objectID": "cfa.html#additional-cfa-specification-issues",
    "href": "cfa.html#additional-cfa-specification-issues",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.1 Additional CFA Specification Issues",
    "text": "6.1 Additional CFA Specification Issues\n\nSelect good indicators: Those that represent the breadth of the hypothetical construct (latent variable).\nIdeally, at least 3 indicators of each factor (minimum of 2 for multiple factors)\nReverse code any negatively worded items\nFactor analysis is appropriate for reflective (effect) measurement. The factors cause variance in the indicators.\nformative measurement is when the indicators cause the latent factor (e.g. SES).\n\n\n6.1.1 Scaling Latent Variables\n\nIn addition to the requirement that \\(df \\ge 0\\), identification requires latent variables to be scaled\nThis reduces the number of free parameters to be estimated (i.e., increases df)\nThis is similar to providing a solution to one unknown in an equation with more than one unknown. ex.: \\(a \\times b = 6\\) to \\(a \\times 1 = 6\\)\nThis is required to estimate the parameters\nthe value 1 is used so as not to artificially inflate or shrink other values\n\n\n\n6.1.2 Identification of CFA Models\n\nBecause latent variables (factors) are not observed, there is no inherent scale, they must be constrained for the model to be identified and estimated\nThree basic ways to scale factors:\n\nReference variable method - Constrain the loading of one indicator per factor to be 1.00\nFactor variance method - Constrain each factor variances to be 1.00\nEffect coding method - Constrain the average of the factor loadings to be 1.00"
  },
  {
    "objectID": "cfa.html#fallacies-about-factors-and-indicators",
    "href": "cfa.html#fallacies-about-factors-and-indicators",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.2 Fallacies about Factors and Indicators",
    "text": "6.2 Fallacies about Factors and Indicators\n\nReification is the fallacy that because you created a factor the latent construct must exist in reality. Just because you create a factor does not mean it exists in reality!\nThe Naming Fallacy is the fallacy that because you give a factor a name, that is what it must be measuring. Just becasue you give a factor a name does not mean that is what it is measuring.\nThe Jingle Fallacy is the false belief that because two things (indicators) have the same name they are the same thing. Giving two things the same name does not mean they are measuring the same thing\nThe Jangle Fallacy is the false belief that because two things have different names they are different things. Just because two things have different names does not mean they are measuring different things."
  },
  {
    "objectID": "cfa.html#interpreting-estimates-in-cfa",
    "href": "cfa.html#interpreting-estimates-in-cfa",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.3 Interpreting Estimates in CFA",
    "text": "6.3 Interpreting Estimates in CFA\n\nPattern coefficients (factor loadings) are interpreted as regression coefficeints\nFor simple indicators (load on one factor) standardized pattern coefficients are interpreted as Pearson correlations, and squared coefficients are the proportion of the indicator explained by the factor.\nFor complex indicators (load on more than one factor) standardized pattern coefficients are interpreted as beta weights (standardized regression coefficients)\nThe ratio of an unstandardized error variance over the observed variance of an indicator is the proportion of unexplained variance. One minus this value is the explained variance.\nThe Pearson correlation between an indicator and a factor is a structure coefficient\nThe standardized pattern coefficient for a simple indicator is a structure coefficient, but not for a complex indicator."
  },
  {
    "objectID": "cfa.html#standardization-in-cfa",
    "href": "cfa.html#standardization-in-cfa",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.4 Standardization in CFA",
    "text": "6.4 Standardization in CFA\n\nSTD in Mplus can be used to only standardize the factors. This can be useful if the scale of the indicators is meaningful and you want to retain the scale (e.g. you want to estimate how much a one unit change in the factor would impact reaction time in the indicators).\nSTDYX is Mplus will standardize all latent (factors) and observed variables (indicators). This might be useful if indicators are on different scales or the scale of the indicators is not of interest."
  },
  {
    "objectID": "cfa.html#cfa-when-sample-size-is-not-large",
    "href": "cfa.html#cfa-when-sample-size-is-not-large",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.5 CFA when sample size is not large",
    "text": "6.5 CFA when sample size is not large\n\nUse indicators with good psychometric properties (reliability, validity), standardized pattern coefficients > .70.\nConsider imposing equality constraints on indicators of the same factor if metric is on same scale.\nConsider using parcels instead of latent factors."
  },
  {
    "objectID": "cfa.html#respecification-of-cfa",
    "href": "cfa.html#respecification-of-cfa",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.6 Respecification of CFA",
    "text": "6.6 Respecification of CFA\n\nChanges involving the indicators\n\nchange factors\nsimple to complex (load on multiple factors)\ncorrelate the residuals of indicators\n\nChange the factor structure\n\nchange number of factors\n\n\nStart by inspecting the residuals and modification indices"
  },
  {
    "objectID": "cfa.html#analyzing-likert-scale-items",
    "href": "cfa.html#analyzing-likert-scale-items",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.7 Analyzing Likert-Scale Items",
    "text": "6.7 Analyzing Likert-Scale Items\n\nML is not appropriate when the using categorical indicators with small number of categories (e.g. <= 5) or when response distributions are asymmetrical\nRobust Weighted Least Squares (WLS) estimation can be used\nWLS makes no distributional assumptions, and can be used with categorical and continuous variables"
  },
  {
    "objectID": "cfa.html#wls-parameterization",
    "href": "cfa.html#wls-parameterization",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.8 WLS Parameterization",
    "text": "6.8 WLS Parameterization\n\nDelta scaling:\n\ntotal variance of latent response fixed to 1.0\npattern coefficients estimate amount of standard deviation change in lantent response given 1 standard deviation change in common factor\nthresholds are normal deviates that correspond to cummulative area of the curve to the left of particular category.\n\nTheta scaling:\n\nresidual variance of latent response fixed to 1.0\npattern coefficeints estimate amount of change in probit (normal deviate) give a 1 unit change in the factor\nthresholds are predicted normal deviates for next lowest response category where the latent response variable is not standardized"
  },
  {
    "objectID": "cfa.html#reliability-of-factor-measurement",
    "href": "cfa.html#reliability-of-factor-measurement",
    "title": "6  Confirmatory Factor Analysis",
    "section": "6.9 Reliability of Factor Measurement",
    "text": "6.9 Reliability of Factor Measurement\n\nThere is a substantial methodological literature detailing the problems with using coefficient alpha as an estimate of scale reliability\nThere is a substantial applied literature which ignores the substantial methodological literature.\nCFA provides a much better way to evaluate scale reliability (McDonald’s Omega, AVE, etc.)\n\n\niq_mod <- \"\nverb =~ info + comp + arith + simil + digit + vocab\nperf =~ pictcomp + parang + block + object + coding\n\"\n\n\niq_fit <- cfa(iq_mod, data = wiscsem)\n\n\ninspect(iq_fit)\n\n$lambda\n         verb perf\ninfo        0    0\ncomp        1    0\narith       2    0\nsimil       3    0\ndigit       4    0\nvocab       5    0\npictcomp    0    0\nparang      0    6\nblock       0    7\nobject      0    8\ncoding      0    9\n\n$theta\n         info comp arith simil digit vocab pctcmp parang block object coding\ninfo       10                                                               \ncomp        0   11                                                          \narith       0    0    12                                                    \nsimil       0    0     0    13                                              \ndigit       0    0     0     0    14                                        \nvocab       0    0     0     0     0    15                                  \npictcomp    0    0     0     0     0     0     16                           \nparang      0    0     0     0     0     0      0     17                    \nblock       0    0     0     0     0     0      0      0    18              \nobject      0    0     0     0     0     0      0      0     0     19       \ncoding      0    0     0     0     0     0      0      0     0      0     20\n\n$psi\n     verb perf\nverb   21     \nperf   23   22\n\n\n\nsummary(iq_fit)\n\nlavaan 0.6.15 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           175\n\nModel Test User Model:\n                                                      \n  Test statistic                                70.640\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.005\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  verb =~                                             \n    info              1.000                           \n    comp              0.926    0.108    8.609    0.000\n    arith             0.589    0.084    7.013    0.000\n    simil             1.012    0.115    8.764    0.000\n    digit             0.477    0.099    4.805    0.000\n    vocab             1.020    0.107    9.548    0.000\n  perf =~                                             \n    pictcomp          1.000                           \n    parang            0.719    0.156    4.614    0.000\n    block             1.060    0.187    5.675    0.000\n    object            0.921    0.177    5.215    0.000\n    coding            0.119    0.147    0.810    0.418\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  verb ~~                                             \n    perf              2.263    0.515    4.397    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .info              3.566    0.507    7.034    0.000\n   .comp              4.572    0.585    7.815    0.000\n   .arith             3.602    0.420    8.571    0.000\n   .simil             5.096    0.662    7.702    0.000\n   .digit             6.162    0.680    9.056    0.000\n   .vocab             3.487    0.506    6.886    0.000\n   .pictcomp          5.526    0.757    7.296    0.000\n   .parang            5.463    0.658    8.298    0.000\n   .block             3.894    0.640    6.083    0.000\n   .object            5.467    0.719    7.600    0.000\n   .coding            8.159    0.874    9.335    0.000\n    verb              4.867    0.883    5.514    0.000\n    perf              3.035    0.844    3.593    0.000\n\n\n\nsummary(iq_fit, standardized = TRUE, fit.measures = TRUE,\n        rsquare = TRUE)\n\nlavaan 0.6.15 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                           175\n\nModel Test User Model:\n                                                      \n  Test statistic                                70.640\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.005\n\nModel Test Baseline Model:\n\n  Test statistic                               519.204\n  Degrees of freedom                                55\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.940\n  Tucker-Lewis Index (TLI)                       0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4491.822\n  Loglikelihood unrestricted model (H1)      -4456.502\n                                                      \n  Akaike (AIC)                                9029.643\n  Bayesian (BIC)                              9102.433\n  Sample-size adjusted Bayesian (SABIC)       9029.600\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.061\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.085\n  P-value H_0: RMSEA <= 0.050                    0.233\n  P-value H_0: RMSEA >= 0.080                    0.103\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.059\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  verb =~                                                               \n    info              1.000                               2.206    0.760\n    comp              0.926    0.108    8.609    0.000    2.042    0.691\n    arith             0.589    0.084    7.013    0.000    1.300    0.565\n    simil             1.012    0.115    8.764    0.000    2.232    0.703\n    digit             0.477    0.099    4.805    0.000    1.053    0.390\n    vocab             1.020    0.107    9.548    0.000    2.250    0.770\n  perf =~                                                               \n    pictcomp          1.000                               1.742    0.595\n    parang            0.719    0.156    4.614    0.000    1.253    0.473\n    block             1.060    0.187    5.675    0.000    1.846    0.683\n    object            0.921    0.177    5.215    0.000    1.605    0.566\n    coding            0.119    0.147    0.810    0.418    0.207    0.072\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  verb ~~                                                               \n    perf              2.263    0.515    4.397    0.000    0.589    0.589\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .info              3.566    0.507    7.034    0.000    3.566    0.423\n   .comp              4.572    0.585    7.815    0.000    4.572    0.523\n   .arith             3.602    0.420    8.571    0.000    3.602    0.681\n   .simil             5.096    0.662    7.702    0.000    5.096    0.506\n   .digit             6.162    0.680    9.056    0.000    6.162    0.848\n   .vocab             3.487    0.506    6.886    0.000    3.487    0.408\n   .pictcomp          5.526    0.757    7.296    0.000    5.526    0.646\n   .parang            5.463    0.658    8.298    0.000    5.463    0.777\n   .block             3.894    0.640    6.083    0.000    3.894    0.533\n   .object            5.467    0.719    7.600    0.000    5.467    0.680\n   .coding            8.159    0.874    9.335    0.000    8.159    0.995\n    verb              4.867    0.883    5.514    0.000    1.000    1.000\n    perf              3.035    0.844    3.593    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    info              0.577\n    comp              0.477\n    arith             0.319\n    simil             0.494\n    digit             0.152\n    vocab             0.592\n    pictcomp          0.354\n    parang            0.223\n    block             0.467\n    object            0.320\n    coding            0.005\n\n\n\niq_mod_nocode <- \"\nverb =~ info + comp + arith + simil + digit + vocab\nperf =~ pictcomp + parang + block + object + 0*coding\n\"\niq_fit_nocode <- cfa(iq_mod_nocode, data = wiscsem)\nsummary(iq_fit_nocode, standardized = TRUE, fit.measures = TRUE,\n        rsquare = TRUE)\n\nlavaan 0.6.15 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           175\n\nModel Test User Model:\n                                                      \n  Test statistic                                71.287\n  Degrees of freedom                                44\n  P-value (Chi-square)                           0.006\n\nModel Test Baseline Model:\n\n  Test statistic                               519.204\n  Degrees of freedom                                55\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.941\n  Tucker-Lewis Index (TLI)                       0.927\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4492.145\n  Loglikelihood unrestricted model (H1)      -4456.502\n                                                      \n  Akaike (AIC)                                9028.291\n  Bayesian (BIC)                              9097.916\n  Sample-size adjusted Bayesian (SABIC)       9028.249\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.060\n  90 Percent confidence interval - lower         0.032\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA <= 0.050                    0.253\n  P-value H_0: RMSEA >= 0.080                    0.088\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.061\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  verb =~                                                               \n    info              1.000                               2.207    0.760\n    comp              0.926    0.107    8.611    0.000    2.042    0.691\n    arith             0.589    0.084    7.009    0.000    1.299    0.565\n    simil             1.012    0.115    8.773    0.000    2.234    0.704\n    digit             0.476    0.099    4.798    0.000    1.051    0.390\n    vocab             1.019    0.107    9.546    0.000    2.249    0.769\n  perf =~                                                               \n    pictcomp          1.000                               1.761    0.602\n    parang            0.711    0.154    4.625    0.000    1.252    0.472\n    block             1.041    0.183    5.689    0.000    1.833    0.678\n    object            0.911    0.174    5.236    0.000    1.604    0.566\n    coding            0.000                               0.000    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  verb ~~                                                               \n    perf              2.288    0.518    4.417    0.000    0.589    0.589\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .info              3.564    0.507    7.032    0.000    3.564    0.423\n   .comp              4.572    0.585    7.815    0.000    4.572    0.523\n   .arith             3.604    0.420    8.573    0.000    3.604    0.681\n   .simil             5.087    0.661    7.696    0.000    5.087    0.505\n   .digit             6.165    0.681    9.057    0.000    6.165    0.848\n   .vocab             3.491    0.507    6.891    0.000    3.491    0.408\n   .pictcomp          5.460    0.757    7.217    0.000    5.460    0.638\n   .parang            5.467    0.659    8.298    0.000    5.467    0.777\n   .block             3.943    0.641    6.155    0.000    3.943    0.540\n   .object            5.468    0.720    7.595    0.000    5.468    0.680\n   .coding            8.202    0.877    9.354    0.000    8.202    1.000\n    verb              4.869    0.883    5.515    0.000    1.000    1.000\n    perf              3.101    0.854    3.633    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    info              0.577\n    comp              0.477\n    arith             0.319\n    simil             0.495\n    digit             0.152\n    vocab             0.592\n    pictcomp          0.362\n    parang            0.223\n    block             0.460\n    object            0.320\n    coding            0.000"
  },
  {
    "objectID": "irt_dichotomous.html#introduction-to-item-response-theory",
    "href": "irt_dichotomous.html#introduction-to-item-response-theory",
    "title": "7  Item Response Theory: Dichotomous Items",
    "section": "7.1 Introduction to Item Response Theory",
    "text": "7.1 Introduction to Item Response Theory\n\n7.1.1 Synthetic Aperture Personality Assessment (SAPA)\nTo learn more about this data you can use the R help function after loading the hemp package as follows,\n\nlibrary(hemp)\ndata(SAPA)\n?SAPA\n\nand you can also go the the SAPA website at https://www.sapa-project.org/.\nYou should spend some time familiarizing yourself with the data, a process you should be able to do on your own.\n\nstr(SAPA)\n\n'data.frame':   1525 obs. of  16 variables:\n $ reason.4 : num  0 0 0 1 0 1 1 0 1 1 ...\n $ reason.16: num  0 0 1 0 1 1 1 1 1 1 ...\n $ reason.17: num  0 1 1 0 1 1 1 0 0 1 ...\n $ reason.19: num  0 0 0 0 0 1 1 0 1 1 ...\n $ letter.7 : num  0 1 1 0 0 1 1 0 0 0 ...\n $ letter.33: num  1 0 0 0 1 1 1 0 1 0 ...\n $ letter.34: num  0 1 0 1 0 1 1 0 1 1 ...\n $ letter.58: num  0 0 0 0 0 1 1 0 1 0 ...\n $ matrix.45: num  0 0 1 0 1 1 1 0 1 1 ...\n $ matrix.46: num  0 0 1 0 1 1 1 1 0 1 ...\n $ matrix.47: num  0 0 0 0 0 1 1 1 0 0 ...\n $ matrix.55: num  1 0 0 0 0 0 0 0 0 0 ...\n $ rotate.3 : num  0 0 0 0 0 1 1 0 0 0 ...\n $ rotate.4 : num  0 0 0 0 0 1 1 1 0 0 ...\n $ rotate.6 : num  0 1 0 0 0 1 1 0 0 0 ...\n $ rotate.8 : num  0 0 0 0 0 0 1 0 0 0 ...\n\nsummary(SAPA)\n\n    reason.4        reason.16        reason.17        reason.19     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6402   Mean   :0.6982   Mean   :0.6973   Mean   :0.6152  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :2        NA's   :1        NA's   :2        NA's   :2       \n    letter.7        letter.33        letter.34        letter.58     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :0.0000  \n Mean   :0.5997   Mean   :0.5712   Mean   :0.6133   Mean   :0.4439  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1        NA's   :2        NA's   :2                        \n   matrix.45        matrix.46        matrix.47        matrix.55    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :0.000  \n Mean   :0.5259   Mean   :0.5499   Mean   :0.6139   Mean   :0.374  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :2        NA's   :1        NA's   :2        NA's   :1      \n    rotate.3         rotate.4         rotate.6         rotate.8    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.000  \n Mean   :0.1937   Mean   :0.2127   Mean   :0.2994   Mean   :0.185  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :2        NA's   :2        NA's   :2        NA's   :1      \n\n\nExample of a test item:\nWhich of the following is an example of a chemical reaction?\nA. A rainbow\nB. Lightning\nC. Burning wood\nD. Melting snow\nWhat must one know to answer this question?\n\nbe able to read and comprehend English\nunderstand the question format\nknow the meaning of “chemical reaction”\nknow they can make only one choice, and how to record it\nthat a rainbow results from refracting light\nlightning is an electric discharge\nmelting snow is a change of state\nthat burning wood is combination of the molecular structure of wood with oxygen to yield a different compound\n\nThis fairly straight-forward question is complex!\n\n\n7.1.2 Logit Scale\nA Logit is just the log odds and it is a s-shaped functional form. For IRT higher logits means a higher probability of answering a question correctly. The use of a logit scale (there are other options, such as the probit scale) allows us to do mathematical operations to results gathered from items on the nominal or ordinal scales.\n\n\n7.1.3 Item Parameters\n\n7.1.3.1 Item Difficulty\nEach scale item has an item difficulty value represented as its location within the range of ability, which is often represented as theta (\\(\\theta\\)). For dichotomous items, the item difficulty is defined as the point on the ability range that represents the person having a 50% chance of endorsing the item (e.g. answering it correctly). People with a higher ability (i.e. higher \\(\\theta\\), which is on the logit scale) will have a higher likelihood of answering the item correctly, and those with lower ability will be less likely to answer it correctly. Items with higher logit values are more difficult, requiring persons to have a higher \\(\\theta\\) to endorse the item with the same likelihood.\n\n\n7.1.3.2 Item Discrimination\nThe item difficulty is the point on the s-curve where the slope is steepest (and also is in the middle of the curve as it is symmetrical). The logit for this location is zero. The steepness of the s-shaped curve in the middle represents the item discrimination, and the steeper the curve the better the item is at discriminating between persons higher in ability from those lower in ability. Generally, items with higher item discrimination are preferred.\n\n\n7.1.3.3 General Assumptions Underlying Item Response Theory Models\n\nUnidimensionality of the latent construct\nParallel item characteristic curves\nLocal independence after considering person parameter"
  },
  {
    "objectID": "irt_dichotomous.html#unidimensional-irt-models-for-dichotomous-items",
    "href": "irt_dichotomous.html#unidimensional-irt-models-for-dichotomous-items",
    "title": "7  Item Response Theory: Dichotomous Items",
    "section": "7.2 Unidimensional IRT Models for Dichotomous Items",
    "text": "7.2 Unidimensional IRT Models for Dichotomous Items\nIn this section we will explore four unidimensional IRT models for dichotomous items. These include the one-parameter, two-parameter, three-parameter, and four-parameter models.\n\n7.2.1 1-PL IRT Model or the Rasch Model\nTo understand these parameters we will consider how they are related in IRT models. We will start with the simplest model known as the one parameter logistic IRT model and also as the Rasch model. The “one-parameter” is because this model only has one item parameter, namely, item difficulty. Item discrimination is assumed to be equal across items.\n\\[\nP(Y_{ij} = 1 | \\theta_j, a, b_j) = \\frac{\\text{exp}(Da(\\theta_j - b_i))}{1 + \\text{exp}(Da(\\theta_j - b_i))}\n\\]\nwhere \\(\\theta_j\\) is the level of the latent trait for person \\(j\\), \\(a\\) is the item discrimination parameter. The parameter \\(b_i\\), represents the item difficulty for item \\(i\\), and \\(D\\) is a constant, that scales the monotonic function for the logistic model onto a normal ogive model, where \\(D = 1.7\\).\nIRT is a probabilistic model of responses to a given item based on an underlying latent ability. “A latent trait is a characteristic or ability of an individual that is not directly observable but instead must be inferred based on some aspect of a person’s performance or presentation” (Baylor et al. 2011).\nThe probability of item endorsement is referred to as theta (\\(\\theta\\)), and us a is a monotonically increasing function of the latent ability.\nLet’s see this model in action with our example data\n\n# install.packages(\"mirt\")\nlibrary(\"mirt\")\nlibrary(\"hemp\")\n\nLook at the help file for mirt, and browse the vignettes.\nFirst, we will define the model as follows:\n\nonepl_mod <- \"\nF = 1 - 16\nCONSTRAIN = (1 - 16, a1)\n\"\n\nThe F represents the latent ability, and is manifested by columns 1-16 in the data. The CONSTRAIN command constrains the used items to have the same item discrimination (\\(a\\)). Note mirt uses \\(a1\\) to represent item discrimination instead of \\(a\\). This model object is then passed to the mirt() function as follows:\n\nonepl_fit <- mirt(data = SAPA, model = onepl_mod,\n                  SE = TRUE)\n\n\nIteration: 1, Log-Lik: -13494.679, Max-Change: 0.23406\nIteration: 2, Log-Lik: -13335.854, Max-Change: 0.12756\nIteration: 3, Log-Lik: -13292.524, Max-Change: 0.07878\nIteration: 4, Log-Lik: -13277.030, Max-Change: 0.05217\nIteration: 5, Log-Lik: -13270.789, Max-Change: 0.03431\nIteration: 6, Log-Lik: -13268.209, Max-Change: 0.02244\nIteration: 7, Log-Lik: -13266.529, Max-Change: 0.00847\nIteration: 8, Log-Lik: -13266.376, Max-Change: 0.00575\nIteration: 9, Log-Lik: -13266.308, Max-Change: 0.00382\nIteration: 10, Log-Lik: -13266.259, Max-Change: 0.00131\nIteration: 11, Log-Lik: -13266.255, Max-Change: 0.00084\nIteration: 12, Log-Lik: -13266.253, Max-Change: 0.00062\nIteration: 13, Log-Lik: -13266.251, Max-Change: 0.00017\nIteration: 14, Log-Lik: -13266.251, Max-Change: 0.00013\nIteration: 15, Log-Lik: -13266.251, Max-Change: 0.00009\n\nCalculating information matrix...\n\n\nWith the model estimated, we can save the parameters to another object, which can be useful, as there are many parameters estimated.\n\nonepl_params <- coef(onepl_fit, IRTpars = TRUE,\n                     simplify = TRUE)\n\nBy setting IRTpars = TRUE we will get traditional IRT parameters, instead of an intercept and a slope with is the default in mirt.\n\\[\nb_i = \\frac{-d_i}{a1_i}\n\\] where \\(d\\) is the intercept parameter, which represents item easiness (think CTT), \\(a1_i\\) is the slope parameter, which represents item discrimination, and \\(b_i\\) item difficulty and is the traditional IRT parameter we want.\nThe simplify = TRUE puts the item parameters into a data frame for ease of use.\nTo explore the parameters, we will look at the item parameters first. We start with looking at the first few.\n\nonepl_items <- onepl_params$items\nhead(onepl_items)\n\n                 a          b g u\nreason.4  1.445587 -0.5557199 0 1\nreason.16 1.445587 -0.8020747 0 1\nreason.17 1.445587 -0.7980649 0 1\nreason.19 1.445587 -0.4546611 0 1\nletter.7  1.445587 -0.3923381 0 1\nletter.33 1.445587 -0.2810892 0 1\n\n\nNotice that the a parameter is estimated to be 1.45 and are all the same, which makes sense in light of constraining them to be the same. The b parameter varies across items, and reflects the estimated difficulty of these items. The g parameter is the lower asymptote, or the lowest value of on the y-axis of the s-curve, which represents the guessing parameter (we will discuss later). Finally, u is the upper asymptote, or the maximum value on the y-axis of the s-curve. Again these last two parameters are not estimated here, and will be important for more complex models later.\nTo see the standard errors of the estimates we do the following\n\nonepl_se <- coef(onepl_fit, printSE = TRUE)\nnames(onepl_se)\n\n [1] \"reason.4\"  \"reason.16\" \"reason.17\" \"reason.19\" \"letter.7\"  \"letter.33\"\n [7] \"letter.34\" \"letter.58\" \"matrix.45\" \"matrix.46\" \"matrix.47\" \"matrix.55\"\n[13] \"rotate.3\"  \"rotate.4\"  \"rotate.6\"  \"rotate.8\"  \"GroupPars\"\n\n\n\nplot(onepl_fit, type = \"trace\", which.items = 1:2)\n\n\n\n\n\nitemplot(onepl_fit, type = \"infoSE\", item =1, )\n\n\n\n\n\n\n7.2.2 Two-Parameter Logistic Model\nRecall the equation for the one parameter logistics model: \\[\nP(Y_{ij} = 1 | \\theta_j, a, b_j) = \\frac{\\text{exp}(Da(\\theta_j - b_i))}{1 + \\text{exp}(Da(\\theta_j - b_i))}\n\\tag{7.1}\\]\nThe equation \\[\nP(Y_{ij} = 1 | \\theta_j, a, b_j) = \\frac{\\text{exp}(Da_i(\\theta_j - b_i))}{1 + \\text{exp}(Da_i(\\theta_j - b_i))}\n\\tag{7.2}\\]\n\ntwopl_mod <- \"F = 1 - 16\"\ntwopl_fit <- mirt(data = SAPA, model = twopl_mod,\n                  itemtype = \"2PL\", SE = TRUE,\n                  verbose = FALSE)\ntwopl_params <- coef(twopl_fit, IRTpars = TRUE, \n                     simplify = TRUE)\ntwopl_items <- twopl_params$items\ntwopl_items\n\n                  a          b g u\nreason.4  1.6924256 -0.5127258 0 1\nreason.16 1.4616058 -0.7967194 0 1\nreason.17 1.8568189 -0.7052519 0 1\nreason.19 1.4429276 -0.4544282 0 1\nletter.7  1.5739581 -0.3749607 0 1\nletter.33 1.3512472 -0.2906578 0 1\nletter.34 1.6568903 -0.4165187 0 1\nletter.58 1.4637541  0.2090402 0 1\nmatrix.45 1.0649705 -0.1241352 0 1\nmatrix.46 1.1060157 -0.2292152 0 1\nmatrix.47 1.3463316 -0.4666121 0 1\nmatrix.55 0.8786048  0.6793708 0 1\nrotate.3  1.7878172  1.1986461 0 1\nrotate.4  2.0841977  1.0317428 0 1\nrotate.6  1.6388551  0.7524753 0 1\nrotate.8  1.5855260  1.3201267 0 1\n\n\nNote that unlike with the 1-PL model in which the a (discrimination) parameter was constant but the b (difficulty) parameter varied across items, for the 2-PL model both parameters are estimated for each item. We can see this by plotting the item characteristics curves (ICC).\n\nplot(twopl_fit, type = \"trace\", which.items = c(12, 14))\n\n\n\n\nFigure 7.1: Two-Parameter Logistic Model ICC for Items 12 (matrix.55) and 14 (rotate.4)\n\n\n\n\nThe steepness of the curve reflects the dicrimination of the item, so rotate.4 better discriminates participants low and high on the latent trait compared to matrix.55.\nBelow, we plot two items with very similar discriminations, but different difficulties. Here\n\nplot(twopl_fit, type = \"trace\", which.items = c(5, 16),\n     facet_items = FALSE, auto.key = list(points = FALSE,\n                                          lines = TRUE,\n                                          columns = 2),\n     par.settings = simpleTheme(lty = 1:2))\n\n\n\n\nFigure 7.2: Two Items from the 2-PL model with similar discriminations but where one (rotate.8) has a higher difficulty than the other (letter.7).\n\n\n\n\n\n\n7.2.3 Three-Parameter Logistic Model\nThe three-parameter logistic (3-PL) IRT model extents the 2-PL model by allowing the lower asymptote to be a value other than zero and to vary across items. This new parameter is known as the pseudo-guessing parameter and represents the likelihood of endorsing the item based solely on chance. This equation looks a bit different from the 1-PL and 2-PL. Of note here, the pseudo-guessing parameter is represented as \\(c_i\\) in Equation 7.3.\n\\[\nP(Y_{ij} = 1 |\\theta_j,a_i,b_i,c_i) = c_i + \\frac{1 - c_i}{1 + \\text{exp}(-Da_i(\\theta_j - b_i))}\n\\tag{7.3}\\]\nUsing R code similar to the 2-PL model we can obtain similar output for the 3-PL model, simply by passing “3PL” instead of “2PL” to the itemtype argument.\n\nthreepl_mod <- \"F = 1 - 16\"\nthreepl_fit <- mirt(data = SAPA, model = threepl_mod,\n                    itemtype = \"3PL\", SE = TRUE,\n                    verbose = FALSE)\n\nEM cycles terminated after 500 iterations.\n\nthree_params <- coef(threepl_fit, IRtpars = TRUE,\n                     simplify = TRUE)\nthreepl_items <- three_params$items\nthreepl_items\n\n                a1          d            g u\nreason.4  1.965832  0.5641179 0.1220243277 1\nreason.16 1.419231  1.1347790 0.0022155571 1\nreason.17 1.780427  1.2599642 0.0020240288 1\nreason.19 1.379131  0.6226675 0.0005308146 1\nletter.7  1.535846  0.5564295 0.0007297169 1\nletter.33 1.336533  0.3677506 0.0010139285 1\nletter.34 1.599364  0.6512575 0.0004802221 1\nletter.58 1.440834 -0.3409123 0.0020967366 1\nmatrix.45 1.046398  0.1154139 0.0009894579 1\nmatrix.46 1.080147  0.2242550 0.0059583434 1\nmatrix.47 1.317104  0.5978160 0.0034682297 1\nmatrix.55 1.070360 -0.9567629 0.0847850503 1\nrotate.3  5.094621 -5.6138172 0.0519131011 1\nrotate.4  5.084681 -5.1611727 0.0499032687 1\nrotate.6  2.972703 -2.5725099 0.0867770483 1\nrotate.8  3.613054 -4.3917883 0.0559861724 1\n\n\nNow the g parameter, which represents \\(c_i\\) in Equation 7.3, not only is clealy no longer a column of zeros, but also varies across items. Higher values represent a higher likelihood of guessing correctly. We can see this in the Figure 7.3.\n\nplot(threepl_fit, type = \"trace\", which.items = c(1,4),\n     facet_items = FALSE, auto.key = list(points = FALSE,\n                                          lines = TRUE,\n                                          columns = 2),\n     par.settings = simpleTheme(lty = 1:2))\n\n\n\n\nFigure 7.3: Two items with similar difficulties and discriminations, but where one (reason.4) has a higher guessing parameter than the other (reason.19).\n\n\n\n\n\n\n7.2.4 Four-Parameter Logistic Model\nThe four-parameter logistic (4-PL) IRT model add to the 3-PL model by allowing the upper asymptote to be a value other than 1, and to vary across items. This parameter, which is represented by a \\(u_i\\) in Equation 7.4, looks very similar to Equation 7.3. Notice the \\(u_i\\) replaces a 1 in Equation 7.3. \\[\nP(Y_{ij} = 1 |\\theta_j,a_i,b_i,c_i, u_i) = c_i + \\frac{u_i - c_i}{1 + \\text{exp}(-Da_i(\\theta_j - b_i))}\n\\tag{7.4}\\]\nThis parameter can be thought of as a ceiling parameter that prevents the probability of correctly answering the question from approaching one, no matter how high the examanee is on the latent trait. Examples of model that might use the 4-PL include personality traits for which the highest probability of endorsement may be thought not to realized.\nTo estimate this model simply pass “4PL” to the itemtype argument in the mirt() function.\n\nfourpl_mod <- \"F = 1 - 16\"\nfourpl_fit <- mirt(data =SAPA, model = fourpl_mod,\n                   itemtype = \"4PL\", SE = TRUE, verbose = FALSE)\nfourpl_params <- coef(fourpl_fit, IRTpars = TRUE,\n                      simplify = TRUE)\nfourpl_items <- fourpl_params$items\nfourpl_items\n\n                   a           b            g         u\nreason.4   1.9766590 -0.33880191 0.1106491560 0.9912226\nreason.16  1.6049333 -0.85635560 0.0003777105 0.9670460\nreason.17  2.0646331 -0.74692890 0.0006907810 0.9718755\nreason.19  1.9129082 -0.62321056 0.0004655221 0.9056812\nletter.7   3.5986603 -0.35051970 0.1257669007 0.8856442\nletter.33  3.4005447 -0.30938988 0.1338977317 0.8529766\nletter.34  4.4887329 -0.39805679 0.1218857142 0.8843403\nletter.58  2.3238599  0.03893132 0.0621158007 0.8449203\nmatrix.45  1.8124989 -0.41699175 0.0522580364 0.8166662\nmatrix.46  4.1276339 -0.10604923 0.2430138966 0.8131703\nmatrix.47  1.4774957 -0.52830296 0.0012448589 0.9639032\nmatrix.55  0.9454504  0.78368331 0.0469519665 0.9838315\nrotate.3  19.8215065  0.88294556 0.0543281486 0.7832056\nrotate.4  11.6723112  0.85784348 0.0551411351 0.8415898\nrotate.6   3.8382098  0.86748083 0.1092746342 0.9857768\nrotate.8  11.5020149  0.99039594 0.0658973790 0.7838166\n\n\n\nplot(fourpl_fit, type = \"trace\", which.items = c(13, 15))\n\n\n\n\nFigure 7.4: Ploting items 13 and 15, with different ceiling parameters"
  },
  {
    "objectID": "irt_dichotomous.html#ability-estimation-in-irt-models",
    "href": "irt_dichotomous.html#ability-estimation-in-irt-models",
    "title": "7  Item Response Theory: Dichotomous Items",
    "section": "7.3 Ability Estimation in IRT Models",
    "text": "7.3 Ability Estimation in IRT Models\nUp to this point, we have been considering the parameter estimates that describe the psychometric properties of the items. But IRT can also estimate characteristics of the persons completing the items. Specifically, when an examinee completes the items that comprise the instrument, IRT can obtain estimates of the latent trait underlying the instrument, and – given the items difficulty, discrimination, guessing, and ceiling parameters – can be used to predict each examinee’s probability of getting the item correct (\\(P(\\theta_j,a_i,b_i,c_i)\\)) or incorrect (\\(Q(\\theta_j,a_i,b_i,c_i)\\)). Note that \\(Q(\\theta_j,a_i,b_i,c_i)\\) is simply 1 - \\(P(\\theta_j,a_i,b_i,c_i)\\).\nWith these we can calculate the probabilities of the items in the response pattern to obtain the joint likelihood function for the items with:\n\\[\nL(\\theta_j) = \\prod^{N}_{i=1}P(\\theta_j,a_i,b_i,c_i)^{x_i}Q(\\theta_j,a_i,b_i,c_i)^{1-x_i}\n\\tag{7.5}\\] where \\(x_i\\) is person \\(j\\)’s dichotomous (0,1) score on item \\(i\\). There are three methods to estimate the latent trait using the above joint likelihood:\nMaximum Likelihood Estimation (MLE): This method is used to find the latent trait that is most likely given the examinee’s observed response pattern and the estimated item parameters.\nMaximum a Posteriori (MAP): This method is a Bayesian version of the MLE method, in which the MLE is multiplied by a prior population distribution. The MAP computes the mode value of the final estimated distribution.\nExpected a Posteriori (EAP): This method is very similar to MAP but uses the mean of the posterior distribution instead of the mode.\nThe last two have the advantage that they can be esimated for examinee’s who get all items correct or incorrect, which cannot be done with MLE. All these estimates can be calculated with the mirt package using the fscores() function as follows for the 2-PL model estimated above:\n\nlatent_mle <- fscores(twopl_fit, method = \"ML\",\n                      full.scores = TRUE, \n                      full.scores.SE = TRUE)\nlatent_map <- fscores(twopl_fit, method = \"MAP\",\n                      full.scores = TRUE, \n                      full.scores.SE = TRUE)\nlatent_eap <- fscores(twopl_fit, method = \"EAP\",\n                      full.scores = TRUE, \n                      full.scores.SE = TRUE)\n\nHere the three sets of results are collected in a data frame and the first few and last few estimates with each method are displayed for comparison.\n\nlatent <- data.frame(MLE = latent_mle[ ,1],\n                     MAP = latent_map[ ,1],\n                     EAP = latent_eap[ ,1])\nrbind(head(latent), \"...\", tail(latent))\n\n                     MLE                 MAP                 EAP\n1      -1.72332501674418   -1.33653276190771   -1.40673697449049\n2      -0.73118860933391  -0.630788539587229   -0.65052152456826\n3      -0.67713195527688  -0.585477156176144  -0.602873121952331\n4       -1.3901748948739   -1.13381014095268   -1.18705645246632\n5     -0.712748737308236   -0.61537679672198  -0.634301931580828\n6       1.81752499584468    1.43562869182637    1.49358568876196\n7                    ...                 ...                 ...\n1520  -0.400302535759441   -0.34859411298161  -0.355642391059989\n1521    1.64557318056452    1.33051689148733    1.37970201174774\n1522    2.59018651486936    1.76241570987896    1.85138058713949\n1523   -1.15433557155277  -0.966829958769407   -1.00746116203187\n1524   0.240973815382656   0.209308642501458   0.215710675610587\n1525 -0.0202728370669997 -0.0176636995119004 -0.0152250463736297\n\n\nYou can see that all three methods give similar results. Below we can see that for examinees with all correct or incorrect the MLE estimates show as Inf and -Inf respectively, which reflects this methods inability to estimate theta for those individuals:\n\nlatent[c(73, 89, 103, 105), ]\n\n     MLE       MAP       EAP\n73   Inf  1.985339  2.096258\n89   Inf  1.985339  2.096258\n103  Inf  1.985339  2.096258\n105 -Inf -1.864351 -1.980879\n\n\nTo understand the distribution of the estimated latent trait of the examinees we can calculate descriptive statistics and correlations. We will store these in an object we call latent_est and we will remove the examinees with either an Inf or -Inf.\n\nlatent_est <- latent[is.finite(latent$MLE), ]\n\nWe can take this objecdt and get a summary of the three types of estimates with:\n\napply(latent_est, 2, summary)\n\n                 MLE         MAP          EAP\nMin.    -2.439114424 -1.62922178 -1.725578546\n1st Qu. -0.689406667 -0.59580053 -0.613718783\nMedian  -0.025201555 -0.02195921 -0.019611185\nMean     0.002659149  0.00357282  0.001146392\n3rd Qu.  0.664415606  0.57240513  0.584675590\nMax.     2.590186515  1.76241571  1.851380587\n\n\nThis function applies the summary() function to each column in the latent_est object (which is a data frame). We can also get a sense of the dispersion of these latent trait estimates with:\n\napply(latent_est, 2, sd)\n\n      MLE       MAP       EAP \n1.0112851 0.8021994 0.8345610 \n\n\nTo calculate the correlation between these estimate use:\n\ncor(latent_est)\n\n          MLE       MAP       EAP\nMLE 1.0000000 0.9973045 0.9978970\nMAP 0.9973045 1.0000000 0.9999447\nEAP 0.9978970 0.9999447 1.0000000\n\n\nAll three estimates are extremely highly correlated with each other. We can visualize these correlations with a scatterplot matrix.\n\npairs(latent_est)\n\n\n\n\nFinally, we can calculate the root mean squared deviation (RMSD) of the estimates as follows>\n\nrmsd(latent_est$MLE, latent_est$MAP)\n\n[1] 0.03492328\n\n\n\nrmsd(latent_est$MLE, latent_est$EAP)\n\n[1] 0.05782216\n\n\n\nrmsd(latent_est$MAP, latent_est$EAP)\n\n[1] 0.09274544"
  },
  {
    "objectID": "irt_dichotomous.html#model-diagnostics",
    "href": "irt_dichotomous.html#model-diagnostics",
    "title": "7  Item Response Theory: Dichotomous Items",
    "section": "7.4 Model Diagnostics",
    "text": "7.4 Model Diagnostics\nModel diagnostics are important for evaluating how well the model fits the data, and can be examined at the level of items, persons, and the model.\n\n7.4.1 Item Fit\nThere are two ways commonly used to assess item fit in IRT, graphical analysis and item fit statistics.\n\nrasch_mod <- \"F = 1 - 16\"\nrasch_fit <- mirt(data = SAPA, model = rasch_mod,\n                  itemtype = \"Rasch\", SE = TRUE)\n\n\nIteration: 1, Log-Lik: -13381.676, Max-Change: 0.37124\nIteration: 2, Log-Lik: -13304.781, Max-Change: 0.28794\nIteration: 3, Log-Lik: -13277.916, Max-Change: 0.18825\nIteration: 4, Log-Lik: -13269.603, Max-Change: 0.11076\nIteration: 5, Log-Lik: -13267.193, Max-Change: 0.06138\nIteration: 6, Log-Lik: -13266.517, Max-Change: 0.03287\nIteration: 7, Log-Lik: -13266.327, Max-Change: 0.01775\nIteration: 8, Log-Lik: -13266.276, Max-Change: 0.00885\nIteration: 9, Log-Lik: -13266.262, Max-Change: 0.00460\nIteration: 10, Log-Lik: -13266.257, Max-Change: 0.00244\nIteration: 11, Log-Lik: -13266.256, Max-Change: 0.00120\nIteration: 12, Log-Lik: -13266.255, Max-Change: 0.00063\nIteration: 13, Log-Lik: -13266.255, Max-Change: 0.00034\nIteration: 14, Log-Lik: -13266.255, Max-Change: 0.00016\nIteration: 15, Log-Lik: -13266.255, Max-Change: 0.00009\n\nCalculating information matrix...\n\nitemfit(rasch_fit, empirical.plot = 1)\n\n\n\n\n\nitemfit(rasch_fit, fit_stats = c(\"Zh\", \"infit\"), impute = 10, na.rm = TRUE)\n\nSample size after row-wise response data removal: 1523\n\n\n        item     Zh outfit z.outfit infit z.infit\n1   reason.4  4.936  0.777   -4.604 0.865  -5.118\n2  reason.16  2.928  0.836   -2.723 0.911  -2.983\n3  reason.17  5.250  0.749   -4.348 0.836  -5.689\n4  reason.19  3.404  0.869   -2.795 0.904  -3.681\n5   letter.7  4.642  0.819   -4.085 0.874  -4.998\n6  letter.33  2.969  0.903   -2.267 0.918  -3.275\n7  letter.34  5.179  0.836   -3.563 0.850  -5.910\n8  letter.58  3.872  0.841   -3.963 0.901  -3.993\n9  matrix.45  0.304  0.969   -0.738 0.995  -0.183\n10 matrix.46  0.622  0.963   -0.853 0.985  -0.604\n11 matrix.47  2.869  0.866   -2.855 0.926  -2.818\n12 matrix.55 -2.507  1.101    2.021 1.068   2.406\n13  rotate.3  3.067  0.975   -0.231 0.834  -4.185\n14  rotate.4  4.517  0.782   -2.685 0.803  -5.364\n15  rotate.6  3.541  0.858   -2.392 0.878  -4.042\n16  rotate.8  2.103  0.894   -1.086 0.888  -2.666\n\n\n\n\n7.4.2 Person Fit\n\n\n7.4.3 Model Selection\nModel comparison with models estimated with mirt can be done with the anova() function.\n\nanova(onepl_fit, twopl_fit)\n\n               AIC    SABIC       HQ      BIC    logLik     X2 df p\nonepl_fit 26566.50 26603.10 26600.23 26657.11 -13266.25            \ntwopl_fit 26464.79 26533.69 26528.28 26635.34 -13200.40 131.71 15 0\n\n\n\nanova(twopl_fit, threepl_fit)\n\n                 AIC    SABIC       HQ      BIC    logLik      X2 df p\ntwopl_fit   26464.79 26533.69 26528.28 26635.34 -13200.40             \nthreepl_fit 26336.74 26440.08 26431.96 26592.56 -13120.37 160.058 16 0\n\n\n\nanova(threepl_fit, fourpl_fit)\n\n                 AIC    SABIC       HQ      BIC    logLik      X2 df p\nthreepl_fit 26336.74 26440.08 26431.96 26592.56 -13120.37             \nfourpl_fit  26259.96 26397.75 26386.92 26601.06 -13065.98 108.778 16 0"
  },
  {
    "objectID": "irt_dichotomous.html#references",
    "href": "irt_dichotomous.html#references",
    "title": "7  Item Response Theory: Dichotomous Items",
    "section": "7.5 References",
    "text": "7.5 References\n\n\n\n\nBaylor, Carolyn, William Hula, Neila J Donovan, Patrick J Doyle, Diane Kendall, and Kathryn Yorkston. 2011. “An Introduction to Item Response Theory and Rasch Models for Speech-Language Pathologists.”"
  },
  {
    "objectID": "irt_polytomous.html#chapter-overview",
    "href": "irt_polytomous.html#chapter-overview",
    "title": "8  Item Response Theory for Polytomous Items",
    "section": "8.1 Chapter Overview",
    "text": "8.1 Chapter Overview\n\n8.1.1 Dichotomous vs Polytomous Items\nSo far we have required that responses be dichotomous (or scored dichotomously). This could be correct (1) and incorrect(0) or endorse (1) and not endorse (0). Which of the following items are dichotomous and which are polytomous?\n1. Which of the following is an example of a chemical reaction?    \n  A. A rainbow    (0)  \n  B. Lightning    (0)  \n  C. Burning wood (1)  \n  D. Melting snow (0)  \n\n2. I have dropped many of my interests and activities.   \n  A. Agree    (1)   \n  B. Disagree (0)   \n\n3. On the whole, I am satisfied with myself.    \n  A. Strongly Agree    (3)  \n  B. Agree             (2)\n  C. Disagree          (1)\n  D. Strongly Disagree (0)\nThe third item is polytomous, because this item cannot be scored as either endorsed or not endorsed. Instead the Likert scaling are aimed at determining level of endorsement.\nIn this chapter we will consider a number of models for instruments with polytomous items. These will consist of polytomous Rasch models for ordinal items, polytomous non-Rasch models for ordinal items, and polytomous models for nominal models.\n\n\n8.1.2 Data Example: The Rosenberg Self-Esteem Scale\nFor most of the examples in this chapter we will use the data from the Roseberg Self-Esteem Scale. This data entitled rse is part of the hemp package and can be loaded, along with the needed packages, as follows.\n\nlibrary(hemp)\nlibrary(mirt)\ndata(rse)\n\nBelow is information that can be obtained in the help file for this data (e.g. typing ?rse in the console after the hemp package is loaded).\n\n8.1.2.1 Description\nThe RSE data set was obtained via online with an interactive version of the Rosenberg Self-Esteem Scale (Rosenberg, 1965). Individuals were informed at the start of the test that their data would be saved. When they completed the scale, they were asked to confirm that the responses they had given were accurate and could be used for research, only those who confirmed are included in this dataset. A random sample of 1000 participants who completed all of the items in the scale were included in the RSE data set. All of the 10 rating scale items were rated on a 4-point scale (i.e., 1=strongly disagree, 2=disagree, 3=agree, and 4=strongly agree). Items 3, 5, 8, 9 and 10 were reversed-coded in order to place all the items in the same direction. That is, higher scores indicate higher self-esteem.\n\n\n8.1.2.2 Format\nA data frame with 1000 participants who responded to 10 rating scale items in an interactive version of the Rosenberg Self-Esteem Scale (Rosenberg, 1965). There are also additional demographic items about the participants:\n\n\n8.1.2.3 Questions\n\n\nTable 8.1: Rosenberg Self-Esteem Scale Questions\n\n\n\n\n\n\nQuestion\nDescription\n\n\n\n\nQ1\nI feel that I am a person of worth, at least on an equal plane with others.\n\n\nQ2\nI feel that I have a number of good qualities.\n\n\nQ3*\nAll in all, I am inclined to feel that I am a failure.\n\n\nQ4\nI am able to do things as well as most other people.\n\n\nQ5*\nI feel I do not have much to be proud of.\n\n\nQ6\nI take a positive attitude toward myself.\n\n\nQ7\nOn the whole, I am satisfied with myself.\n\n\nQ8*\nI wish I could have more respect for myself.\n\n\nQ9*\nI certainly feel useless at times.\n\n\nQ10*\nAt times, I think I am no good at all.\n\n\ngender\nChosen from a drop down list (1=male, 2=female, 3=other; 0=none was chosen)\n\n\nage\nEntered as a free response. (0=response that could not be converted to integer)\n\n\nsource\nHow the user came to the web page of the RSE scale (1=Front page of personality website, 2=Google search, 3=other)\n\n\ncountry\nInferred from technical information using MaxMind GeoLite\n\n\nperson\nParticipant identifier\n\n\n\n\nNote: * = indicates a reverse coded item.\n\n\n8.1.2.4 Source\nThe The Rosenberg Self-Esteem Scale is available at http://personality-testing.info/tests/RSE.php.\n\n\n8.1.2.5 Exploring rse Date\n\nround(cor(rse[ ,1:10]),2)\n\n      Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10\nQ1  1.00 0.69 0.49 0.58 0.47 0.61 0.56 0.35 0.39 0.49\nQ2  0.69 1.00 0.45 0.53 0.50 0.57 0.52 0.29 0.39 0.46\nQ3  0.49 0.45 1.00 0.45 0.63 0.59 0.59 0.41 0.56 0.61\nQ4  0.58 0.53 0.45 1.00 0.39 0.48 0.48 0.28 0.37 0.41\nQ5  0.47 0.50 0.63 0.39 1.00 0.55 0.56 0.38 0.52 0.57\nQ6  0.61 0.57 0.59 0.48 0.55 1.00 0.74 0.47 0.52 0.61\nQ7  0.56 0.52 0.59 0.48 0.56 0.74 1.00 0.47 0.52 0.58\nQ8  0.35 0.29 0.41 0.28 0.38 0.47 0.47 1.00 0.51 0.53\nQ9  0.39 0.39 0.56 0.37 0.52 0.52 0.52 0.51 1.00 0.74\nQ10 0.49 0.46 0.61 0.41 0.57 0.61 0.58 0.53 0.74 1.00\n\n\n\nheatmap(cor(rse[ ,1:10]), )"
  },
  {
    "objectID": "irt_polytomous.html#polytomous-rasch-models-for-ordinal-items",
    "href": "irt_polytomous.html#polytomous-rasch-models-for-ordinal-items",
    "title": "8  Item Response Theory for Polytomous Items",
    "section": "8.2 Polytomous Rasch Models for Ordinal Items",
    "text": "8.2 Polytomous Rasch Models for Ordinal Items\n\n8.2.1 Partial Credit Model\n\\[\nP(X_i | \\theta, \\delta_{ih}) = \\frac{exp[\\Sigma_{h=0}^{x_i}(\\theta - \\delta_{ih})]}{\\Sigma_{k=0}^{m_i} exp[\\Sigma_{h=0}^{k}(\\theta - \\delta_{ih})]}\n\\tag{8.1}\\]\n\n\\(\\theta\\) is the latent trait\n\\(\\delta_{ih}\\) is the step parameter (and difficulty) that represents obtaining \\(h\\) points over \\(h - 1\\) points.\n\\(m_i\\) is the maximum response category\nThe probability of obtaining \\(X_i\\) points, where \\(X_i = 0, 1,..., m_i\\)\n\n\npcm_mod <- \"selfesteem = 1 - 10\"\npcm_fit <- mirt(data = rse[ ,1:10], \n                model = pcm_mod,\n                itemtype = \"Rasch\", SE = TRUE,\n                verbose = FALSE) # suppress messages\npcm_params <- coef(pcm_fit, IRTpars = TRUE,\n                   simplify = TRUE)\n\n\npcm_items <- pcm_params$items\npcm_items\n\n    a        b1         b2        b3\nQ1  1 -2.862010 -1.6407355 0.9905954\nQ2  1 -3.054737 -2.1650857 1.0886321\nQ3  1 -2.221950 -0.5364370 1.7623392\nQ4  1 -3.350431 -1.4546799 1.8055787\nQ5  1 -1.849256 -0.0803302 1.4718476\nQ6  1 -2.183106 -0.1655968 2.0228575\nQ7  1 -1.783392  0.1374231 2.3139519\nQ8  1 -1.581559  0.8743338 1.9704969\nQ9  1 -1.235833  1.2172079 2.0237022\nQ10 1 -1.399323  0.6126267 1.1601504\n\n\n\nplot(pcm_fit, type = \"trace\", which.items = c(2,5),\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(pcm_fit, type = \"infotrace\", which.items = c(2,5), \n     par.settings = simpleTheme(lwd = 2))\n\n\n\n\n\nplot(pcm_fit, type = \"infotrace\", \n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(pcm_fit, type = \"infotrace\", facet_items = FALSE,\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(pcm_fit, type = \"info\", \n     par.settings = simpleTheme(lwd = 2))\n\n\n\n\n\n\n8.2.2 Rating Scale Model\nThe rating scale model (RSM) is a restricted version of the PCM, where all items are constrained to have the same form. This is common for Likert scale instruments.\n\\[\nP(X_i | \\theta,\\lambda_i,  \\delta_1,...\\delta_m) = \\frac{exp[\\Sigma_{j=0}^{c}(\\theta - (\\lambda_i +  \\delta_{j})]}{\\Sigma_{h=0}^{m} exp[\\Sigma_{j=0}^{h}(\\theta - (\\lambda_i + \\delta_{j}))]}\n\\]\n\nrsm_mod <- \"selfesteem = 1 - 10\"\nrsm_fit <- mirt(data = rse[ ,1:10], model = rsm_mod,\n                itemtype = \"rsm\")\n\n\nIteration: 1, Log-Lik: -11753.833, Max-Change: 1.76612\nIteration: 2, Log-Lik: -10565.189, Max-Change: 0.26864\nIteration: 3, Log-Lik: -10547.907, Max-Change: 0.16988\nIteration: 4, Log-Lik: -10539.801, Max-Change: 0.12067\nIteration: 5, Log-Lik: -10535.661, Max-Change: 0.08239\nIteration: 6, Log-Lik: -10533.453, Max-Change: 0.05637\nIteration: 7, Log-Lik: -10531.077, Max-Change: 0.07378\nIteration: 8, Log-Lik: -10530.422, Max-Change: 0.01457\nIteration: 9, Log-Lik: -10530.088, Max-Change: 0.01126\nIteration: 10, Log-Lik: -10529.038, Max-Change: 0.01621\nIteration: 11, Log-Lik: -10528.924, Max-Change: 0.00449\nIteration: 12, Log-Lik: -10528.853, Max-Change: 0.00400\nIteration: 13, Log-Lik: -10528.574, Max-Change: 0.00287\nIteration: 14, Log-Lik: -10528.559, Max-Change: 0.00202\nIteration: 15, Log-Lik: -10528.547, Max-Change: 0.00178\nIteration: 16, Log-Lik: -10528.497, Max-Change: 0.00126\nIteration: 17, Log-Lik: -10528.494, Max-Change: 0.00074\nIteration: 18, Log-Lik: -10528.491, Max-Change: 0.00072\nIteration: 19, Log-Lik: -10528.483, Max-Change: 0.00040\nIteration: 20, Log-Lik: -10528.482, Max-Change: 0.00033\nIteration: 21, Log-Lik: -10528.482, Max-Change: 0.00031\nIteration: 22, Log-Lik: -10528.480, Max-Change: 0.00024\nIteration: 23, Log-Lik: -10528.480, Max-Change: 0.00014\nIteration: 24, Log-Lik: -10528.480, Max-Change: 0.00013\nIteration: 25, Log-Lik: -10528.479, Max-Change: 0.00010\nIteration: 26, Log-Lik: -10528.479, Max-Change: 0.00006\n\nrsm_params <- coef(rsm_fit, simplify = TRUE)\nrsm_items <- as.data.frame(rsm_params$items)\nrsm_items\n\n    a1       b1        b2        b3          c\nQ1   1 -3.10133 -1.339071 0.7753553  0.0000000\nQ2   1 -3.10133 -1.339071 0.7753553  0.1813605\nQ3   1 -3.10133 -1.339071 0.7753553 -0.8685441\nQ4   1 -3.10133 -1.339071 0.7753553 -0.2943792\nQ5   1 -3.10133 -1.339071 0.7753553 -1.0881623\nQ6   1 -3.10133 -1.339071 0.7753553 -1.1163886\nQ7   1 -3.10133 -1.339071 0.7753553 -1.4311151\nQ8   1 -3.10133 -1.339071 0.7753553 -1.7382441\nQ9   1 -3.10133 -1.339071 0.7753553 -2.0169971\nQ10  1 -3.10133 -1.339071 0.7753553 -1.4522770\n\n\n\nplot(rsm_fit, type = \"trace\", which.items = c(2, 9), \n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(rsm_fit, type = \"trace\", facet = FALSE, \n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 2))"
  },
  {
    "objectID": "irt_polytomous.html#polytomous-non-rasch-models-for-ordinal-items",
    "href": "irt_polytomous.html#polytomous-non-rasch-models-for-ordinal-items",
    "title": "8  Item Response Theory for Polytomous Items",
    "section": "8.3 Polytomous Non-Rasch Models for Ordinal Items",
    "text": "8.3 Polytomous Non-Rasch Models for Ordinal Items\nThere are two models that can be viewed as polytomous versions of the 2PL IRT model, the generalized partial credit model (GPCM) and the graded response model (GRM)\n\n8.3.1 Generalized Partial Credit Model\n\\[\nP(X_{ik} | \\theta, a_i, \\delta_{ik}) = \\frac{exp[\\sum^{K_{ik}}_{h=1}a_i(\\theta - \\delta_{ih})]}{\\sum^{m_i}_{c=1} exp[\\sum^c_{h=1} a_i (\\theta - \\delta_{ih})]}\n\\]\n\\(a_i\\) is the discrimination parameter. The thresholds (\\(\\delta_{ik}\\)) are not restricted to be in the same order.\n\ngpcm_mod <- \"selfesteem = 1 - 10\"\ngpcm_fit <- mirt(data = rse[ ,1:10], model = gpcm_mod,\n                 itemtype = \"gpcm\", SE = TRUE)\n\n\nIteration: 1, Log-Lik: -10776.362, Max-Change: 2.86983\nIteration: 2, Log-Lik: -10351.776, Max-Change: 0.80786\nIteration: 3, Log-Lik: -10283.772, Max-Change: 0.23823\nIteration: 4, Log-Lik: -10261.491, Max-Change: 0.15392\nIteration: 5, Log-Lik: -10249.603, Max-Change: 0.13059\nIteration: 6, Log-Lik: -10242.348, Max-Change: 0.10220\nIteration: 7, Log-Lik: -10237.530, Max-Change: 0.09347\nIteration: 8, Log-Lik: -10234.445, Max-Change: 0.06972\nIteration: 9, Log-Lik: -10232.291, Max-Change: 0.06364\nIteration: 10, Log-Lik: -10231.438, Max-Change: 0.03688\nIteration: 11, Log-Lik: -10230.008, Max-Change: 0.03037\nIteration: 12, Log-Lik: -10229.195, Max-Change: 0.02323\nIteration: 13, Log-Lik: -10228.554, Max-Change: 0.01831\nIteration: 14, Log-Lik: -10228.158, Max-Change: 0.01575\nIteration: 15, Log-Lik: -10227.896, Max-Change: 0.01344\nIteration: 16, Log-Lik: -10227.479, Max-Change: 0.00796\nIteration: 17, Log-Lik: -10227.440, Max-Change: 0.00461\nIteration: 18, Log-Lik: -10227.415, Max-Change: 0.00412\nIteration: 19, Log-Lik: -10227.383, Max-Change: 0.00213\nIteration: 20, Log-Lik: -10227.377, Max-Change: 0.00186\nIteration: 21, Log-Lik: -10227.372, Max-Change: 0.00115\nIteration: 22, Log-Lik: -10227.370, Max-Change: 0.00143\nIteration: 23, Log-Lik: -10227.369, Max-Change: 0.00081\nIteration: 24, Log-Lik: -10227.368, Max-Change: 0.00094\nIteration: 25, Log-Lik: -10227.365, Max-Change: 0.00071\nIteration: 26, Log-Lik: -10227.365, Max-Change: 0.00018\nIteration: 27, Log-Lik: -10227.365, Max-Change: 0.00048\nIteration: 28, Log-Lik: -10227.365, Max-Change: 0.00050\nIteration: 29, Log-Lik: -10227.365, Max-Change: 0.00034\nIteration: 30, Log-Lik: -10227.365, Max-Change: 0.00037\nIteration: 31, Log-Lik: -10227.365, Max-Change: 0.00029\nIteration: 32, Log-Lik: -10227.364, Max-Change: 0.00042\nIteration: 33, Log-Lik: -10227.364, Max-Change: 0.00017\nIteration: 34, Log-Lik: -10227.364, Max-Change: 0.00013\nIteration: 35, Log-Lik: -10227.364, Max-Change: 0.00033\nIteration: 36, Log-Lik: -10227.364, Max-Change: 0.00010\n\nCalculating information matrix...\n\ngpcm_params <- coef(gpcm_fit, IRTpars = TRUE, simplify = TRUE)\ngpcm_items <- gpcm_params$items\ngpcm_items\n\n            a         b1          b2        b3\nQ1  1.8238891 -1.7202941 -0.95938961 0.5885070\nQ2  1.7307657 -1.8564721 -1.28862013 0.6510270\nQ3  1.8710897 -1.3231835 -0.31308686 1.0425363\nQ4  1.2811061 -2.2085974 -1.00156932 1.2138474\nQ5  1.4939839 -1.1502285 -0.04606415 0.9041157\nQ6  2.7132868 -1.2057255 -0.09730049 1.1220286\nQ7  2.3609630 -1.0210663  0.07245970 1.3106380\nQ8  0.8945663 -1.1800257  0.81206980 1.2679593\nQ9  1.4911761 -0.7707405  0.78235286 1.2329194\nQ10 1.9005838 -0.8381950  0.33153091 0.7308140\n\n\n\nplot(gpcm_fit, type = \"trace\", which.items = c(6, 8),\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, comlumns = 4))\n\n\n\n\n\nplot(gpcm_fit, type = \"trace\",\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, comlumns = 4))\n\n\n\n\n\nplot(gpcm_fit, type = \"info\", theta_lim = c(-6, 6))\n\n\n\nplot(gpcm_fit, type = \"SE\", theta_lim = c(-6, 6))\n\n\n\n\n\n\n8.3.2 Graded Response Model\nThis model retains the ordering of the response options.\n\\[\nP^*(X_i | \\theta, a_i, \\delta_{X_i}) = \\frac{e^{a_i(\\theta - \\delta_{x_i})}}{1 +e^{a_i(\\theta - \\delta_{x_i})} }\n\\]\n\ngrm_mod <- \"selfesteem = 1 - 10\"\ngrm_fit <- mirt(data = rse[ ,1:10], model = grm_mod,\n                itemtype = \"graded\", SE = TRUE, verbose = FALSE)\ngrm_params <- coef(grm_fit, IRTpars = TRUE, simplify = TRUE)\ngrm_items <- grm_params$items\ngrm_items\n\n           a         b1          b2        b3\nQ1  2.324817 -1.9096256 -0.86459285 0.6212696\nQ2  2.136179 -2.1468087 -1.15272898 0.6594750\nQ3  2.435460 -1.3941348 -0.25807123 1.0778067\nQ4  1.650816 -2.4179124 -0.86144982 1.2003060\nQ5  2.172579 -1.2073811 -0.05750731 1.0257647\nQ6  3.202812 -1.2249932 -0.08533037 1.1446882\nQ7  2.810958 -1.0605179  0.09078729 1.3417869\nQ8  1.420458 -1.2018548  0.51592382 1.7199115\nQ9  2.163341 -0.7728069  0.63992017 1.4899442\nQ10 2.645276 -0.8719370  0.23061101 0.9486129\n\n\n\nplot(grm_fit, type = \"trace\", which.items = c(5, 9),\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, comlumns = 4))\n\n\n\n\n\nplot(grm_fit, type = \"trace\",\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, comlumns = 4))\n\n\n\n\n\nplot(grm_fit, type = \"info\", theta_lim = c(-6, 6))\n\n\n\nplot(grm_fit, type = \"SE\", theta_lim = c(-6, 6))"
  },
  {
    "objectID": "irt_polytomous.html#polytomous-irt-models-for-nominal-items",
    "href": "irt_polytomous.html#polytomous-irt-models-for-nominal-items",
    "title": "8  Item Response Theory for Polytomous Items",
    "section": "8.4 Polytomous IRT Models for Nominal Items",
    "text": "8.4 Polytomous IRT Models for Nominal Items\nIf items do not have ordered response categories, but instead are not ordinal, we do not assume an ordinal transition. This is the case with nominal response categories.\n\n8.4.1 Nominal Response Model\n\\[\nP(X_{ik}|\\theta,\\mathbf{a},\\gamma) = \\frac{e^{\\gamma_{ik}+a_{ik}\\theta}}{\\sum^m_{h=1}e^{\\gamma_{ih}+a_{ih}\\theta} }\n\\]\nwhere \\(\\mathbf{a}\\) is a vector of item discrimination parameters, and \\(\\gamma\\) is a vector of difficulty parameters.\n\nnrm_mod <- \"agression = 1 - 24\"\nnrm_fit <- mirt(data = VerbAggWide[ ,4:27], model = nrm_mod,\n                itemtype = \"nominal\", SE = TRUE, verbose = FALSE)\nnrm_params <- coef(nrm_fit, IRTpars = TRUE, simlify = TRUE)\nnrm_items <- as.data.frame(nrm_params$items)\nnrm_items\n\ndata frame with 0 columns and 0 rows\n\n\n\n\n8.4.2 Nested Logit Model\n\nkey <- c(4, 3, 2, 3, 4, 3, 2, 3, 1,\n           4, 3, 2, 3, 3, 4, 2, 4, 3,\n           3, 2, 2, 1, 2, 1, 1, 2, 1)\n\n\n8.4.2.1 2PL NLM\n\ntwoplnlm_mod <- \"ability = 1 - 27\"\ntwoplnlm_fit <- mirt(data = multiplechoice,\n                     model = twoplnlm_mod, itemtype = \"2PLNRM\",\n                     SE = TRUE, key = key, verbose = FALSE)\ntwoplnlm_params <- coef(twoplnlm_fit, IRTpars = TRUE, simplify = TRUE)\ntwoplnlm_items <- as.data.frame(twoplnlm_params$items)\nhead(twoplnlm_items)\n\n              a          b g u         a1          a2          a3         c1\nitem1 0.5235573 -3.4086260 0 1 -0.4839230 -0.59603034  1.07995330 -0.2490648\nitem2 0.4940809 -2.8496901 0 1  0.2270783 -0.35252767  0.12544933  0.6741845\nitem3 0.5226378 -0.5159370 0 1  0.1343142 -0.05036814 -0.08394604  0.4162664\nitem4 0.6871739 -1.6939763 0 1  0.2516607  0.29305060 -0.54471132 -0.6398626\nitem5 0.2097941  2.9218435 0 1  0.2643272 -0.03633783 -0.22798933  0.8475738\nitem6 0.5709976 -0.4476874 0 1  0.1282541 -0.41405909  0.28580500 -0.4715466\n               c2         c3\nitem1 -0.05884263  0.3079074\nitem2 -0.04293720 -0.6312473\nitem3 -0.79558238  0.3793160\nitem4  0.66367996 -0.0238174\nitem5  0.50175068 -1.3493244\nitem6  0.22261253  0.2489341\n\n\n\nplot(twoplnlm_fit, type = \"trace\", which.items = c(8, 21),\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(twoplnlm_fit, type = \"trace\",\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\n\n8.4.2.2 3PL NLM\n\nthreeplnlm_mod <- \"ability = 1 - 27\"\nthreeplnlm_fit <- mirt(data = multiplechoice,\n                       model = threeplnlm_mod, itemtype = \"3PLNRM\",\n                       SE = TRUE, key = key, verbose = FALSE)\n\nEM cycles terminated after 500 iterations.\n\nthreeplnlm_params <- coef(threeplnlm_fit, IRTpars = TRUE, simplify = TRUE)\nthreeplnlm_items <- as.data.frame(threeplnlm_params$items)\nround(head(threeplnlm_items), 4)\n\n           a       b      g u      a1      a2      a3      c1      c2      c3\nitem1 0.6874 -1.0233 0.5512 1 -0.2967 -0.6796  0.9763 -0.1298 -0.1373  0.2671\nitem2 0.5021 -2.7623 0.0176 1  0.2133 -0.3827  0.1693  0.6785 -0.0668 -0.6118\nitem3 0.5759 -0.3280 0.0408 1  0.1494 -0.0432 -0.1062  0.4218 -0.7924  0.3707\nitem4 0.6464 -1.7032 0.0334 1  0.1639  0.3439 -0.5078 -0.6654  0.6694 -0.0040\nitem5 0.2192  2.8837 0.0064 1  0.2337 -0.0138 -0.2199  0.8453  0.5042 -1.3495\nitem6 0.8618  0.6145 0.2781 1  0.1429 -0.3940  0.2511 -0.4749  0.2358  0.2391\n\n\n\nplot(threeplnlm_fit, type = \"trace\", which.items = c(1, 17),\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nplot(threeplnlm_fit, type = \"trace\",\n     par.settings = simpleTheme(lty = 1:4, lwd = 2),\n     auto.key = list(points = FALSE, lines = TRUE, columns = 4))\n\n\n\n\n\nanova(twoplnlm_fit, threeplnlm_fit)\n\n                    AIC    SABIC       HQ      BIC    logLik     X2 df     p\ntwoplnlm_fit   24845.07 25012.34 25112.56 25526.53 -12260.53                \nthreeplnlm_fit 24841.70 25036.85 25153.78 25636.74 -12231.85 57.365 27 0.001"
  }
]