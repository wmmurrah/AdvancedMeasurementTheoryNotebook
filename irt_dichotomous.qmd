---
title: "Item Response Theory: Dichotomous Items"
bibliography: references.bib
---

## Introduction to Item Response Theory



### Synthetic Aperture Personality Assessment (SAPA)

To learn more about this data you can use the R help function after loading the `hemp` package as follows,

```{r}
#| message: false
library(hemp)
data(SAPA)
?SAPA
```
and you can also go the the SAPA website at [https://www.sapa-project.org/](https://www.sapa-project.org/).

You should spend some time familiarizing yourself with the data, a process you should be able to do on your own.

```{r}
str(SAPA)

summary(SAPA)
```


Example of a test item:

Which of the following is an example of a chemical reaction?

A. A rainbow\
B. Lightning\
C. Burning wood\
D. Melting snow\
What must one know to answer this question?

-   be able to read and comprehend English

-   understand the question format

-   know the meaning of "chemical reaction"

-   know they can make only one choice, and how to record it

-   that a rainbow results from refracting light

-   lightning is an electric discharge

-   melting snow is a change of state

-   that burning wood is combination of the molecular structure of wood with oxygen to yield a different compound

**This fairly straight-forward question is complex!**

### Logit Scale

A Logit is just the log odds and it is a s-shaped functional form. For IRT higher logits means a higher probability of answering a question correctly. The use of a logit scale (there are other options, such as the probit scale) allows us to do mathematical operations to results gathered from items on the nominal or ordinal scales. 

### Item Parameters

#### Item Difficulty

Each scale item has an *item difficulty* value represented as its location within the range of ability, which is often represented as theta ($\theta$). For dichotomous items, the item difficulty is defined as the point on the ability range that represents the person having a 50% chance of endorsing the item (e.g. answering it correctly). People with a higher ability (i.e. higher $\theta$, which is on the logit scale) will have a higher likelihood of answering the item correctly, and those with lower ability will be less likely to answer it correctly. Items with higher logit values are more difficult, requiring persons to have a higher $\theta$ to endorse the item with the same likelihood. 

#### Item Discrimination

The item difficulty is the point on the s-curve where the slope is steepest (and also is in the middle of the curve as it is symmetrical). The logit for this location is zero.
The steepness of the s-shaped curve in the middle represents the *item discrimination*, and the steeper the curve the better the item is at discriminating between persons higher in ability from those lower in ability. Generally, items with higher item discrimination are preferred.

#### General Assumptions Underlying Item Response Theory Models

1.  **Unidimensionality** of the latent construct
2.  **Parallel** item characteristic curves
3.  **Local independence** after considering person parameter


## Unidimensional IRT Models for Dichotomous Items

In this section we will explore four unidimensional IRT models for dichotomous items. These include the one-parameter, two-parameter, three-parameter, and four-parameter models.

### 1-PL IRT Model or the Rasch Model

To understand these parameters we will consider how they are related in IRT models. We will start with the simplest model known as the one parameter logistic IRT model and also as the Rasch model. The "one-parameter" is because this model only has one item parameter, namely, item difficulty. Item discrimination is assumed to be equal across items.

$$
P(Y_{ij} = 1 | \theta_j, a, b_j) = \frac{\text{exp}(Da(\theta_j - b_i))}{1 + \text{exp}(Da(\theta_j - b_i))}
$$

where $\theta_j$ is the level of the latent trait for person $j$, $a$ is the item discrimination parameter. The parameter $b_i$, represents the item difficulty for item $i$, and $D$ is a constant, that scales the monotonic function for the logistic model onto a normal ogive model, where $D = 1.7$.

IRT is a probabilistic model of responses to a given item based on an underlying latent ability. "A latent trait is a characteristic or ability of an individual that is not directly observable but instead must be inferred based on some aspect of a person's performance or presentation" [@baylor2011introduction].

The probability of item endorsement is referred to as theta ($\theta$), and us a is a monotonically increasing function of the latent ability.

Let's see this model in action with our example data

```{r}
#| message: false
# install.packages("mirt")
library("mirt")
library("hemp")
```

Look at the help file for `mirt`, and browse the vignettes.

First, we will define the model as follows:
```{r}
onepl_mod <- "
F = 1 - 16
CONSTRAIN = (1 - 16, a1)
"
```

The `F` represents the latent ability, and is manifested by columns 1-16 in the data. The `CONSTRAIN` command constrains the used items to have the same item discrimination ($a$). Note `mirt` uses $a1$ to represent item discrimination instead of $a$.
This model object is then passed to the `mirt()` function as follows:

```{r}
onepl_fit <- mirt(data = SAPA, model = onepl_mod,
                  SE = TRUE)
```

With the model estimated, we can save the parameters to another object, which can be useful, as there are many parameters estimated.
```{r}
onepl_params <- coef(onepl_fit, IRTpars = TRUE,
                     simplify = TRUE)
```

By setting `IRTpars = TRUE` we will get traditional IRT parameters, instead of an intercept and a slope with is the default in `mirt`. 

$$
b_i = \frac{-d_i}{a1_i}
$$
where $d$ is the intercept parameter, which represents item easiness (think CTT), $a1_i$ is the slope parameter, which represents item discrimination, and $b_i$ item difficulty and is the traditional IRT parameter we want.

The `simplify = TRUE` puts the item parameters into a data frame for ease of use.

To explore the parameters, we will look at the item parameters first. We start with looking at the first few.

```{r}
onepl_items <- onepl_params$items
head(onepl_items)
```

Notice that the `a` parameter is estimated to be 1.45 and are all the same, which makes sense in light of constraining them to be the same. 
The `b` parameter varies across items, and reflects the estimated difficulty of these items. The `g` parameter is the lower asymptote, or the lowest value of on the y-axis of the s-curve, which represents the guessing parameter (we will discuss later). Finally, `u` is the upper asymptote, or the maximum value on the y-axis of the s-curve. Again these last two parameters are not estimated here, and will be important for more complex models later.

To see the standard errors of the estimates we do the following
```{r}
onepl_se <- coef(onepl_fit, printSE = TRUE)
names(onepl_se)
```

```{r}
plot(onepl_fit, type = "trace", which.items = 1:2)
```

```{r}
itemplot(onepl_fit, type = "infoSE", item =1, )

```

### Two-Parameter Logistic Model

Recall the equation for the one parameter logistics model:
$$
P(Y_{ij} = 1 | \theta_j, a, b_j) = \frac{\text{exp}(Da(\theta_j - b_i))}{1 + \text{exp}(Da(\theta_j - b_i))}
$$ {#eq-rasch}

The equation
$$
P(Y_{ij} = 1 | \theta_j, a, b_j) = \frac{\text{exp}(Da_i(\theta_j - b_i))}{1 + \text{exp}(Da_i(\theta_j - b_i))}
$$ {#eq-2pl}



```{r}
twopl_mod <- "F = 1 - 16"
twopl_fit <- mirt(data = SAPA, model = twopl_mod,
                  itemtype = "2PL", SE = TRUE,
                  verbose = FALSE)
twopl_params <- coef(twopl_fit, IRTpars = TRUE, 
                     simplify = TRUE)
twopl_items <- twopl_params$items
twopl_items
```

Note that unlike with the 1-PL model in which the a (discrimination) parameter was constant but the b (difficulty) parameter varied across items, for the 2-PL model both parameters are estimated for each item.
We can see this by plotting the item characteristics curves (ICC).

```{r}
#| label: fig-2pl
#| fig-cap: "Two-Parameter Logistic Model ICC for Items 12 (matrix.55) and 14 (rotate.4)"
plot(twopl_fit, type = "trace", which.items = c(12, 14))
```

The steepness of the curve reflects the dicrimination of the item, so `rotate.4` better discriminates participants low and high on the latent trait compared to `matrix.55`.

Below, we plot two items with very similar discriminations, but different difficulties. Here
```{r}
#| label: fig-2pl_5_16
#| fig-cap: "Two Items from the 2-PL model with similar discriminations but where one (`rotate.8`) has a higher difficulty than the other (`letter.7`)."
plot(twopl_fit, type = "trace", which.items = c(5, 16),
     facet_items = FALSE, auto.key = list(points = FALSE,
                                          lines = TRUE,
                                          columns = 2),
     par.settings = simpleTheme(lty = 1:2))
```

### Three-Parameter Logistic Model

The three-parameter logistic (3-PL) IRT model extents the 2-PL model by allowing the lower asymptote to  be a value other than zero and to vary across items. This new parameter is known as the pseudo-guessing parameter and represents the likelihood of endorsing the item based solely on chance. This equation looks a bit different from the 1-PL and 2-PL. Of note here, the pseudo-guessing parameter is represented as $c_i$ in @eq-3pl.

$$
P(Y_{ij} = 1 |\theta_j,a_i,b_i,c_i) = c_i + \frac{1 - c_i}{1 + \text{exp}(-Da_i(\theta_j - b_i))}
$$ {#eq-3pl}

Using R code similar to the 2-PL model we can obtain similar output for the 3-PL model, simply by passing "3PL" instead of "2PL" to  the `itemtype` argument.

```{r}
threepl_mod <- "F = 1 - 16"
threepl_fit <- mirt(data = SAPA, model = threepl_mod,
                    itemtype = "3PL", SE = TRUE,
                    verbose = FALSE)
three_params <- coef(threepl_fit, IRtpars = TRUE,
                     simplify = TRUE)
threepl_items <- three_params$items
threepl_items
```

Now the `g` parameter, which represents $c_i$ in @eq-3pl, not only is clealy no longer a column of zeros, but also varies across items. Higher values represent a higher likelihood of guessing correctly.
We can see this in the @fig-3pl_1_4.

```{r}
#| label: fig-3pl_1_4
#| fig-cap: "Two items with similar difficulties and discriminations, but where one (`reason.4`) has a higher guessing parameter than the other (`reason.19`)."
plot(threepl_fit, type = "trace", which.items = c(1,4),
     facet_items = FALSE, auto.key = list(points = FALSE,
                                          lines = TRUE,
                                          columns = 2),
     par.settings = simpleTheme(lty = 1:2))
```

### Four-Parameter Logistic Model

The four-parameter logistic (4-PL) IRT model add to the 3-PL model by allowing the upper asymptote to be a value other than 1, and to vary across items. This parameter, which is represented by a $u_i$ in @eq-4pl, looks very similar to @eq-3pl. Notice the $u_i$ replaces a 1 in @eq-3pl. 
$$
P(Y_{ij} = 1 |\theta_j,a_i,b_i,c_i, u_i) = c_i + \frac{u_i - c_i}{1 + \text{exp}(-Da_i(\theta_j - b_i))}
$$ {#eq-4pl}

This parameter can be thought of as a ceiling parameter that prevents the probability of correctly answering the question from approaching one, no matter how high the examanee is on the latent trait. Examples of model that might use the 4-PL include personality traits for which the highest probability of endorsement may be thought not to realized.

To estimate this model simply pass "4PL" to the `itemtype` argument in the `mirt()` function.
```{r}
#| message: false
fourpl_mod <- "F = 1 - 16"
fourpl_fit <- mirt(data =SAPA, model = fourpl_mod,
                   itemtype = "4PL", SE = TRUE, verbose = FALSE)
fourpl_params <- coef(fourpl_fit, IRTpars = TRUE,
                      simplify = TRUE)
fourpl_items <- fourpl_params$items
fourpl_items
```

```{r}
#| label: fig-4pl
#| fig-cap: "Ploting items 13 and 15, with different ceiling parameters"
plot(fourpl_fit, type = "trace", which.items = c(13, 15))
```

## Ability Estimation in IRT Models

Up to this point, we have been considering the parameter estimates that describe the psychometric properties of the items. But IRT can also estimate characteristics of the persons completing the items. Specifically, when an examinee completes the items that comprise the instrument, IRT can obtain estimates of the latent trait underlying the instrument, and -- given the items difficulty, discrimination, guessing, and ceiling parameters -- can be used to predict each examinee's probability of getting the item correct ($P(\theta_j,a_i,b_i,c_i)$) or incorrect ($Q(\theta_j,a_i,b_i,c_i)$). Note that $Q(\theta_j,a_i,b_i,c_i)$ is simply 1 - $P(\theta_j,a_i,b_i,c_i)$.

With these we can calculate the probabilities of the items in the response pattern to obtain the joint likelihood function for the items with:

$$
L(\theta_j) = \prod^{N}_{i=1}P(\theta_j,a_i,b_i,c_i)^{x_i}Q(\theta_j,a_i,b_i,c_i)^{1-x_i}
$$ {#eq-joint_lik}
where $x_i$ is person $j$'s dichotomous (0,1) score on item $i$.
There are three methods to estimate the latent trait using the above joint likelihood:

**Maximum Likelihood Estimation (MLE)**: This method is used to find the latent trait that is most likely given the examinee's observed response pattern and the estimated item parameters.

**Maximum a Posteriori (MAP)**: This method is a Bayesian version of the MLE method, in which the MLE is multiplied by a prior population distribution. The MAP computes the mode value of the final estimated distribution.

**Expected a Posteriori (EAP)**: This method is very similar to MAP but uses the mean of the posterior distribution instead of the mode.

The last two have the advantage that they can be esimated for examinee's who get all items correct or incorrect, which cannot be done with MLE.
All these estimates can be calculated with the mirt package using the `fscores()` function as follows for the 2-PL model estimated above:

```{r}
latent_mle <- fscores(twopl_fit, method = "ML",
                      full.scores = TRUE, 
                      full.scores.SE = TRUE)
latent_map <- fscores(twopl_fit, method = "MAP",
                      full.scores = TRUE, 
                      full.scores.SE = TRUE)
latent_eap <- fscores(twopl_fit, method = "EAP",
                      full.scores = TRUE, 
                      full.scores.SE = TRUE)
```

Here the three sets of results are collected in a data frame and the first few and last few estimates with each method are displayed for comparison.

```{r}
latent <- data.frame(MLE = latent_mle[ ,1],
                     MAP = latent_map[ ,1],
                     EAP = latent_eap[ ,1])
rbind(head(latent), "...", tail(latent))
```
You can see that all three methods give similar results.
Below we can see that for examinees with all correct or incorrect the MLE estimates show as `Inf` and `-Inf` respectively, which reflects this methods inability to estimate theta for those individuals:

```{r}
latent[c(73, 89, 103, 105), ]
```
To understand the distribution of the estimated latent trait of the examinees we can calculate descriptive statistics and correlations. We will store these in an object we call `latent_est` and we will remove the examinees with either an `Inf` or `-Inf`.

```{r}
latent_est <- latent[is.finite(latent$MLE), ]
```
We can take this objecdt and get a summary of the three types of estimates with:

```{r}
apply(latent_est, 2, summary)
```
This function applies the `summary()` function to each column in the `latent_est` object (which is a data frame).
We can also get a sense of the dispersion of these latent trait estimates with:

```{r}
apply(latent_est, 2, sd)
```
To calculate the correlation between these estimate use:
```{r}
cor(latent_est)
```
All three estimates are extremely highly correlated with each other.
We can visualize these correlations with a scatterplot matrix.

```{r}
pairs(latent_est)
```
Finally, we can calculate the root mean squared deviation (RMSD) of the estimates as follows>

```{r}
rmsd(latent_est$MLE, latent_est$MAP)
```
```{r}
rmsd(latent_est$MLE, latent_est$EAP)
```
```{r}
rmsd(latent_est$MAP, latent_est$EAP)
```


## Model Diagnostics
Model diagnostics are important for evaluating how well the model fits the data, and can be examined at the level of items, persons, and the model.

### Item Fit

There are two ways commonly used to assess item fit in IRT, graphical analysis and item fit statistics. 

```{r}
rasch_mod <- "F = 1 - 16"
rasch_fit <- mirt(data = SAPA, model = rasch_mod,
                  itemtype = "Rasch", SE = TRUE)

itemfit(rasch_fit, empirical.plot = 1)
```

```{r}
itemfit(rasch_fit, fit_stats = c("Zh", "infit"), impute = 10, na.rm = TRUE)
```


### Person Fit

### Model Selection

Model comparison with models estimated with **mirt** can be done with the `anova()` function.

```{r}
anova(onepl_fit, twopl_fit)
```

```{r}
anova(twopl_fit, threepl_fit)
```

```{r}
anova(threepl_fit, fourpl_fit)
```

## References